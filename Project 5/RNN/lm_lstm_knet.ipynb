{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7S9cpFJqfXy"
   },
   "source": [
    "## Julia on Colaboratory ##\n",
    "\n",
    "[Colaboratory](https://colab.research.google.com) does not provide native support for the [Julia programming language](https://julialang.org). However, since Colaboratory gives you root access to the machine that runs your notebook (the *“runtime”* in Colaboratory terminology), we can install Julia support by uploading a specially crafted Julia notebook  – *this* notebook. We then install Julia and [IJulia](https://github.com/JuliaLang/IJulia.jl) ([Jupyter](https://jupyter.org)/Colaboratory notebook support) and reload the notebook so that Colaboratory detects and initiates what we installed.\n",
    "\n",
    "In brief:\n",
    "\n",
    "1. **Run the cell below**\n",
    "2. **Reload the page**\n",
    "3. **Edit the notebook name and start hacking Julia code below**\n",
    "\n",
    "**If your runtime resets**, either manually or if left idle for some time, **repeat steps 1 and 2**.\n",
    "\n",
    "### Acknowledgements ###\n",
    "\n",
    "This hack by Pontus Stenetorp is an adaptation of [James Bradbury’s original Colaboratory Julia hack](https://discourse.julialang.org/t/julia-on-google-colab-free-gpu-accelerated-shareable-notebooks/15319/27), that broke some time in September 2019 as Colaboratory increased their level of notebook runtime isolation. There also appears to be CUDA compilation support installed by default for each notebook runtime type in October 2019, which shaves off a good 15 minutes or so from the original hack’s installation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BrHjOFFsxf7W",
    "outputId": "9648d951-f8b6-4e03-a512-897366ffc483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-05 13:02:01--  https://julialang-s3.julialang.org/bin/linux/x64/1.2/julia-1.2.0-linux-x86_64.tar.gz\n",
      "Resolving julialang-s3.julialang.org (julialang-s3.julialang.org)... 151.101.2.49, 151.101.66.49, 151.101.130.49, ...\n",
      "Connecting to julialang-s3.julialang.org (julialang-s3.julialang.org)|151.101.2.49|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 gce internal redirect trigger\n",
      "Location: https://storage.googleapis.com/julialang2/bin/linux/x64/1.2/julia-1.2.0-linux-x86_64.tar.gz [following]\n",
      "--2020-01-05 13:02:01--  https://storage.googleapis.com/julialang2/bin/linux/x64/1.2/julia-1.2.0-linux-x86_64.tar.gz\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 2404:6800:4008:c00::80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 91990555 (88M) [application/x-tar]\n",
      "Saving to: ‘/tmp/julia.tar.gz’\n",
      "\n",
      "/tmp/julia.tar.gz   100%[===================>]  87.73M   257MB/s    in 0.3s    \n",
      "\n",
      "2020-01-05 13:02:02 (257 MB/s) - ‘/tmp/julia.tar.gz’ saved [91990555/91990555]\n",
      "\n",
      "   Cloning default registries into `~/.julia`\n",
      "   Cloning registry from \"https://github.com/JuliaRegistries/General.git\"\n",
      "\u001b[2K\u001b[?25h     Added registry `General` to `~/.julia/registries/General`\n",
      " Resolving package versions...\n",
      " Installed DataAPI ──────────── v1.1.0\n",
      " Installed Missings ─────────── v0.4.3\n",
      " Installed Showoff ──────────── v0.3.1\n",
      " Installed BinaryProvider ───── v0.5.8\n",
      " Installed Plots ────────────── v0.28.4\n",
      " Installed StatsBase ────────── v0.32.0\n",
      " Installed Contour ──────────── v0.5.1\n",
      " Installed Colors ───────────── v0.11.2\n",
      " Installed Reexport ─────────── v0.2.0\n",
      " Installed Measures ─────────── v0.3.1\n",
      " Installed Requires ─────────── v1.0.0\n",
      " Installed OrderedCollections ─ v1.1.0\n",
      " Installed RecipesBase ──────── v0.7.0\n",
      " Installed FixedPointNumbers ── v0.6.1\n",
      " Installed DataStructures ───── v0.17.7\n",
      " Installed Parsers ──────────── v0.3.10\n",
      " Installed JSON ─────────────── v0.21.0\n",
      " Installed NaNMath ──────────── v0.3.3\n",
      " Installed FFMPEG ───────────── v0.2.4\n",
      " Installed StaticArrays ─────── v0.12.1\n",
      " Installed SortingAlgorithms ── v0.3.1\n",
      " Installed GeometryTypes ────── v0.7.7\n",
      " Installed PlotUtils ────────── v0.6.1\n",
      " Installed PlotThemes ───────── v1.0.1\n",
      " Installed ColorTypes ───────── v0.9.0\n",
      " Installed GR ───────────────── v0.44.0\n",
      "  Updating `~/.julia/environments/v1.2/Project.toml`\n",
      "  [91a5bcdd] + Plots v0.28.4\n",
      "  Updating `~/.julia/environments/v1.2/Manifest.toml`\n",
      "  [b99e7846] + BinaryProvider v0.5.8\n",
      "  [3da002f7] + ColorTypes v0.9.0\n",
      "  [5ae59095] + Colors v0.11.2\n",
      "  [d38c429a] + Contour v0.5.1\n",
      "  [9a962f9c] + DataAPI v1.1.0\n",
      "  [864edb3b] + DataStructures v0.17.7\n",
      "  [c87230d0] + FFMPEG v0.2.4\n",
      "  [53c48c17] + FixedPointNumbers v0.6.1\n",
      "  [28b8d3ca] + GR v0.44.0\n",
      "  [4d00f742] + GeometryTypes v0.7.7\n",
      "  [682c06a0] + JSON v0.21.0\n",
      "  [442fdcdd] + Measures v0.3.1\n",
      "  [e1d29d7a] + Missings v0.4.3\n",
      "  [77ba4419] + NaNMath v0.3.3\n",
      "  [bac558e1] + OrderedCollections v1.1.0\n",
      "  [69de0a69] + Parsers v0.3.10\n",
      "  [ccf2f8ad] + PlotThemes v1.0.1\n",
      "  [995b91a9] + PlotUtils v0.6.1\n",
      "  [91a5bcdd] + Plots v0.28.4\n",
      "  [3cdcf5f2] + RecipesBase v0.7.0\n",
      "  [189a3867] + Reexport v0.2.0\n",
      "  [ae029012] + Requires v1.0.0\n",
      "  [992d4aef] + Showoff v0.3.1\n",
      "  [a2af1166] + SortingAlgorithms v0.3.1\n",
      "  [90137ffa] + StaticArrays v0.12.1\n",
      "  [2913bbd2] + StatsBase v0.32.0\n",
      "  [2a0f44e3] + Base64 \n",
      "  [ade2ca70] + Dates \n",
      "  [8bb1440f] + DelimitedFiles \n",
      "  [8ba89e20] + Distributed \n",
      "  [b77e0a4c] + InteractiveUtils \n",
      "  [76f85450] + LibGit2 \n",
      "  [8f399da3] + Libdl \n",
      "  [37e2e46d] + LinearAlgebra \n",
      "  [56ddb016] + Logging \n",
      "  [d6f4376e] + Markdown \n",
      "  [a63ad114] + Mmap \n",
      "  [44cfe95a] + Pkg \n",
      "  [de0858da] + Printf \n",
      "  [3fa0cd96] + REPL \n",
      "  [9a3f8284] + Random \n",
      "  [ea8e919c] + SHA \n",
      "  [9e88b42a] + Serialization \n",
      "  [6462fe0b] + Sockets \n",
      "  [2f01184e] + SparseArrays \n",
      "  [10745b16] + Statistics \n",
      "  [8dfed614] + Test \n",
      "  [cf7118a7] + UUIDs \n",
      "  [4ec0a83e] + Unicode \n",
      "  Building GR ────→ `~/.julia/packages/GR/oiZD3/deps/build.log`\n",
      "  Building FFMPEG → `~/.julia/packages/FFMPEG/guN1x/deps/build.log`\n",
      "  Building Plots ─→ `~/.julia/packages/Plots/qZHsp/deps/build.log`\n",
      " Resolving package versions...\n",
      " Installed VersionParsing ─ v1.2.0\n",
      " Installed Conda ────────── v1.3.0\n",
      " Installed LaTeXStrings ─── v1.0.3\n",
      " Installed MacroTools ───── v0.5.3\n",
      " Installed PyPlot ───────── v2.8.2\n",
      " Installed Compat ───────── v2.2.0\n",
      " Installed PyCall ───────── v1.91.2\n",
      "  Updating `~/.julia/environments/v1.2/Project.toml`\n",
      "  [d330b81b] + PyPlot v2.8.2\n",
      "  Updating `~/.julia/environments/v1.2/Manifest.toml`\n",
      "  [34da2185] + Compat v2.2.0\n",
      "  [8f4d0f93] + Conda v1.3.0\n",
      "  [b964fa9f] + LaTeXStrings v1.0.3\n",
      "  [1914dd2f] + MacroTools v0.5.3\n",
      "  [438e738f] + PyCall v1.91.2\n",
      "  [d330b81b] + PyPlot v2.8.2\n",
      "  [81def892] + VersionParsing v1.2.0\n",
      "  [1a1011a3] + SharedArrays \n",
      "  Building Conda ─→ `~/.julia/packages/Conda/kLXeC/deps/build.log`\n",
      "  Building PyCall → `~/.julia/packages/PyCall/ttONZ/deps/build.log`\n",
      " Resolving package versions...\n",
      " Installed SoftGlobalScope ─ v1.0.10\n",
      " Installed ZMQ ───────────── v1.0.0\n",
      " Installed IJulia ────────── v1.20.2\n",
      " Installed MbedTLS ───────── v0.7.0\n",
      "  Updating `~/.julia/environments/v1.2/Project.toml`\n",
      "  [7073ff75] + IJulia v1.20.2\n",
      "  Updating `~/.julia/environments/v1.2/Manifest.toml`\n",
      "  [7073ff75] + IJulia v1.20.2\n",
      "  [739be429] + MbedTLS v0.7.0\n",
      "  [b85f4697] + SoftGlobalScope v1.0.10\n",
      "  [c2297ded] + ZMQ v1.0.0\n",
      "  [7b1f6079] + FileWatching \n",
      "  Building ZMQ ────→ `~/.julia/packages/ZMQ/ABGOx/deps/build.log`\n",
      "  Building MbedTLS → `~/.julia/packages/MbedTLS/a1JFn/deps/build.log`\n",
      "  Building IJulia ─→ `~/.julia/packages/IJulia/F1GUo/deps/build.log`\n",
      " Resolving package versions...\n",
      " Installed URIParser ────────── v0.4.0\n",
      " Installed Adapt ────────────── v1.0.0\n",
      " Installed AbstractFFTs ─────── v0.5.0\n",
      " Installed JLD2 ─────────────── v0.1.11\n",
      " Installed CUDAdrv ──────────── v4.0.4\n",
      " Installed CUDAnative ───────── v2.6.0\n",
      " Installed CuArrays ─────────── v1.5.0\n",
      " Installed Requires ─────────── v0.5.2\n",
      " Installed TimerOutputs ─────── v0.5.3\n",
      " Installed Knet ─────────────── v1.3.2\n",
      " Installed NNlib ────────────── v0.6.2\n",
      " Installed CUDAapi ──────────── v2.1.0\n",
      " Installed GPUArrays ────────── v2.0.1\n",
      " Installed CodecZlib ────────── v0.6.0\n",
      " Installed TranscodingStreams ─ v0.9.5\n",
      " Installed CEnum ────────────── v0.2.0\n",
      " Installed LLVM ─────────────── v1.3.3\n",
      " Installed BinDeps ──────────── v1.0.0\n",
      " Installed AutoGrad ─────────── v1.2.0\n",
      " Installed SpecialFunctions ─── v0.8.0\n",
      " Installed FileIO ───────────── v1.2.1\n",
      "  Updating `~/.julia/environments/v1.2/Project.toml`\n",
      "  [1902f260] + Knet v1.3.2\n",
      "  Updating `~/.julia/environments/v1.2/Manifest.toml`\n",
      "  [621f4979] + AbstractFFTs v0.5.0\n",
      "  [79e6a3ab] + Adapt v1.0.0\n",
      "  [6710c13c] + AutoGrad v1.2.0\n",
      "  [9e28174c] + BinDeps v1.0.0\n",
      "  [fa961155] + CEnum v0.2.0\n",
      "  [3895d2a7] + CUDAapi v2.1.0\n",
      "  [c5f51814] + CUDAdrv v4.0.4\n",
      "  [be33ccc6] + CUDAnative v2.6.0\n",
      "  [944b1d66] + CodecZlib v0.6.0\n",
      "  [3a865a2d] + CuArrays v1.5.0\n",
      "  [5789e2e9] + FileIO v1.2.1\n",
      "  [0c68f7d7] + GPUArrays v2.0.1\n",
      "  [033835bb] + JLD2 v0.1.11\n",
      "  [1902f260] + Knet v1.3.2\n",
      "  [929cbde3] + LLVM v1.3.3\n",
      "  [872c559c] + NNlib v0.6.2\n",
      "  [ae029012] ↓ Requires v1.0.0 ⇒ v0.5.2\n",
      "  [276daf66] + SpecialFunctions v0.8.0\n",
      "  [a759f4b9] + TimerOutputs v0.5.3\n",
      "  [3bb67fe8] + TranscodingStreams v0.9.5\n",
      "  [30578b45] + URIParser v0.4.0\n",
      "  Building SpecialFunctions → `~/.julia/packages/SpecialFunctions/ne2iw/deps/build.log`\n",
      "  Building NNlib ───────────→ `~/.julia/packages/NNlib/udnNA/deps/build.log`\n",
      "  Building CodecZlib ───────→ `~/.julia/packages/CodecZlib/5t9zO/deps/build.log`\n",
      "  Building Knet ────────────→ `~/.julia/packages/Knet/LjPts/deps/build.log`\n",
      "Precompiling project...\n",
      "Precompiling Knet\n",
      "Precompiling IJulia\n",
      "Precompiling Plots\n",
      "Precompiling PyPlot\n",
      "  Building SpecialFunctions → `~/.julia/packages/SpecialFunctions/ne2iw/deps/build.log`\n",
      "  Building NNlib ───────────→ `~/.julia/packages/NNlib/udnNA/deps/build.log`\n",
      "  Building CodecZlib ───────→ `~/.julia/packages/CodecZlib/5t9zO/deps/build.log`\n",
      "  Building Knet ────────────→ `~/.julia/packages/Knet/LjPts/deps/build.log`\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installation cell\n",
    "%%shell\n",
    "if ! command -v julia 2>&1 > /dev/null\n",
    "then\n",
    "    wget 'https://julialang-s3.julialang.org/bin/linux/x64/1.2/julia-1.2.0-linux-x86_64.tar.gz' \\\n",
    "        -O /tmp/julia.tar.gz\n",
    "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
    "    rm /tmp/julia.tar.gz\n",
    "fi\n",
    "julia -e 'using Pkg; pkg\"add Plots; add PyPlot; add IJulia; add Knet; precompile\"'\n",
    "julia -e 'using Pkg; pkg\"build Knet;\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "1VdFDb9szt_o",
    "outputId": "91e07c54-1b07-4a07-ba7e-ba2e457036d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /root/.julia/compiled/v1.2/Knet/f4vSz.ji for Knet [1902f260-5fb4-5aff-8c31-6271790ab950]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet\n",
    "# Test if Knet is using gpu\n",
    "Knet.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "pWDXteAbeUmz",
    "outputId": "7dd19260-1dad-4dd9-fcb1-43397194bcfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 KnetArray{Float64,2}:\n",
       " 0.54592   0.554279  0.343429   0.838957\n",
       " 0.806902  0.768127  0.285473   0.488519\n",
       " 0.568271  0.857464  0.0606523  0.424611\n",
       " 0.53042   0.396642  0.694845   0.125767"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = KnetArray(randn(4,4))\n",
    "sigm.(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GmlApMZI1EDW",
    "outputId": "41edbea8-3beb-4687-e609-473a1ffd8e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m IterTools ─ v1.3.0\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [c8e1da08]\u001b[39m\u001b[92m + IterTools v1.3.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      " \u001b[90m [c8e1da08]\u001b[39m\u001b[92m + IterTools v1.3.0\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m StrTables ────── v1.0.1\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m WordTokenizers ─ v0.5.3\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m HTML_Entities ── v1.0.0\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [796a5d58]\u001b[39m\u001b[92m + WordTokenizers v0.5.3\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      " \u001b[90m [7693890a]\u001b[39m\u001b[92m + HTML_Entities v1.0.0\u001b[39m\n",
      " \u001b[90m [9700d1a9]\u001b[39m\u001b[92m + StrTables v1.0.1\u001b[39m\n",
      " \u001b[90m [796a5d58]\u001b[39m\u001b[92m + WordTokenizers v0.5.3\u001b[39m\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m HTML_Entities → `~/.julia/packages/HTML_Entities/g4t7p/deps/build.log`\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [8dfed614]\u001b[39m\u001b[92m + Test \u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [9a3f8284]\u001b[39m\u001b[92m + Random \u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [10745b16]\u001b[39m\u001b[92m + Statistics \u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [ade2ca70]\u001b[39m\u001b[92m + Dates \u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [37e2e46d]\u001b[39m\u001b[92m + LinearAlgebra \u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [3a865a2d]\u001b[39m\u001b[92m + CuArrays v1.5.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling IterTools [c8e1da08-722c-5040-9ed9-7db0dc04731e]\n",
      "└ @ Base loading.jl:1242\n",
      "┌ Info: Precompiling WordTokenizers [796a5d58-b03d-544a-977e-18100b691f6e]\n",
      "└ @ Base loading.jl:1242\n",
      "┌ Info: Recompiling stale cache file /root/.julia/compiled/v1.2/Knet/f4vSz.ji for Knet [1902f260-5fb4-5aff-8c31-6271790ab950]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m CUDAdrv ──── v5.0.1\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m CUDAnative ─ v2.7.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m CuArrays ─── v1.6.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Knet ─────── v1.2.7\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [3a865a2d]\u001b[39m\u001b[93m ↑ CuArrays v1.5.0 ⇒ v1.6.0\u001b[39m\n",
      " \u001b[90m [1902f260]\u001b[39m\u001b[95m ↓ Knet v1.3.2 ⇒ v1.2.7\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      " \u001b[90m [c5f51814]\u001b[39m\u001b[93m ↑ CUDAdrv v4.0.4 ⇒ v5.0.1\u001b[39m\n",
      " \u001b[90m [be33ccc6]\u001b[39m\u001b[93m ↑ CUDAnative v2.6.0 ⇒ v2.7.0\u001b[39m\n",
      " \u001b[90m [3a865a2d]\u001b[39m\u001b[93m ↑ CuArrays v1.5.0 ⇒ v1.6.0\u001b[39m\n",
      " \u001b[90m [1902f260]\u001b[39m\u001b[95m ↓ Knet v1.3.2 ⇒ v1.2.7\u001b[39m\n",
      " \u001b[90m [ae029012]\u001b[39m\u001b[93m ↑ Requires v0.5.2 ⇒ v1.0.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m Knet → `~/.julia/packages/Knet/IIjk8/deps/build.log`\n",
      "Package name: Statistics Version: 0.0.1\n",
      "Package name: Test Version: 0.0.1\n",
      "Package name: Random Version: 0.0.1\n",
      "Package name: WordTokenizers Version: 0.5.3\n",
      "Package name: IterTools Version: 1.3.0\n",
      "Package name: LinearAlgebra Version: 0.0.1\n",
      "Package name: CuArrays Version: 1.6.0\n",
      "Package name: IJulia Version: 1.20.2\n",
      "Package name: Plots Version: 0.28.4\n",
      "Package name: PyPlot Version: 2.8.2\n",
      "Package name: Dates Version: 0.0.1\n",
      "Package name: Knet Version: 1.2.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'nn4nlp-code'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RNN_model(Embed(P(KnetArray{Float32,2}(512,10000))), LSTM(input=512,hidden=512,bidirectional,dropout=0.2), Linear(P(KnetArray{Float32,2}(10000,1024)), P(KnetArray{Float32,1}(10000))), 0.2, Vocab(Dict(\"adviser\" => 1750,\"enjoy\" => 4607,\"advertisements\" => 7826,\"fight\" => 1441,\"nicholas\" => 3783,\"everywhere\" => 6278,\"surveyed\" => 3556,\"helping\" => 2081,\"whose\" => 621,\"manufacture\" => 5052…), [\"<s>\", \"<unk>\", \"the\", \"N\", \"of\", \"to\", \"a\", \"in\", \"and\", \"'s\"  …  \"cluett\", \"hydro-quebec\", \"memotec\", \"photography\", \"ipo\", \"ssangyong\", \"fromstein\", \"ferc\", \"gitano\", \"daewoo\"], 2, 1, split)), 9.214204f0)"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import Pkg\n",
    "using Pkg; for p in (\"Knet\",\"IterTools\",\"WordTokenizers\",\"Test\",\"Random\",\"Statistics\",\"Dates\",\"LinearAlgebra\",\"CuArrays\"); haskey(Pkg.installed(),p) || Pkg.add(p); end\n",
    "using Statistics, IterTools, WordTokenizers, Test, Knet, Random, Dates, Base.Iterators, LinearAlgebra\n",
    "# Update and list all packages\n",
    "Pkg.update()\n",
    "pkgs = Pkg.installed()\n",
    "\n",
    "for package in keys(pkgs)\n",
    "    if pkgs[package] == nothing\n",
    "        pkgs[package] = VersionNumber(\"0.0.1\")\n",
    "    end\n",
    "    println(\"Package name: \", package, \" Version: \", pkgs[package])\n",
    "end\n",
    "using CuArrays: CuArrays, usage_limit\n",
    "CuArrays.usage_limit[] = 8_000_000_000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "Knet.atype() = KnetArray{Float32} \n",
    "is_lstm_strategy_on = true # if true rnn type becomes lstm, otherwise we preferred to use relu\n",
    "gpu() # GPU test must result as 0\n",
    "# Vocabulary Structure\n",
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "function Vocab(file::String; tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "    vocab_freq = Dict{String,Int64}(unk => 1, eos => 1)\n",
    "    w2i = Dict{String, Int64}(unk => 2, eos => 1)\n",
    "    i2w = Vector{String}()\n",
    "\n",
    "    push!(i2w, eos)\n",
    "    push!(i2w, unk)\n",
    "\n",
    "    open(file) do f\n",
    "        for line in eachline(f)\n",
    "            sentence = strip(lowercase(line))\n",
    "            sentence = tokenizer(line, [' '], keepempty = false)\n",
    "\n",
    "            for word in sentence\n",
    "                word == unk && continue\n",
    "                word == eos && continue # They are default ones to be added later\n",
    "                vocab_freq[word] = get!(vocab_freq, word, 0) + 1\n",
    "            end\n",
    "        end\n",
    "        close(f)\n",
    "    end\n",
    "\n",
    "\n",
    "    # End of vanilla implementation of the vocaulary\n",
    "    # From here we must add the mincount and vocabsize properties\n",
    "    # We must change the first two property of the vocab wrt those paramaters\n",
    "    vocab_freq = sort!(\n",
    "        collect(vocab_freq),\n",
    "        by = tuple -> last(tuple),\n",
    "        rev = true,\n",
    "    )\n",
    "\n",
    "    if length(vocab_freq)>vocabsize - 2 # eos and unk ones\n",
    "        vocab_freq = vocab_freq[1:vocabsize-2] # trim to fit the size\n",
    "    end\n",
    "\n",
    "    #vocab_freq = reverse(vocab_freq)\n",
    "\n",
    "    while true\n",
    "        length(vocab_freq)==0 && break\n",
    "        word,freq = vocab_freq[end]\n",
    "        freq>=mincount && break # since it is already ordered\n",
    "        vocab_freq = vocab_freq[1:(end - 1)]\n",
    "    end\n",
    "    #pushfirst!(vocab_freq,unk=>1,eos=>1) # freq does not matter, just adding the\n",
    "    for i in 1:length(vocab_freq)\n",
    "        word, freq = vocab_freq[i]\n",
    "        ind = (get!(w2i, word, 1+length(w2i)))\n",
    "        (length(i2w) < ind) && push!(i2w, word)\n",
    "    end\n",
    "\n",
    "    return Vocab(w2i, i2w, 2, 1, tokenizer)\n",
    "end\n",
    "# Special reader for the task\n",
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "word2ind(dict,x) = get(dict, x, 2)\n",
    "\n",
    "#Implementing the iterate function\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    if s == nothing\n",
    "        state = open(r.file)\n",
    "        Base.iterate(r,state)\n",
    "    else\n",
    "        if eof(s) == true\n",
    "            close(s)\n",
    "            return nothing\n",
    "        else\n",
    "            line = readline(s)\n",
    "            line = strip(lowercase(line))\n",
    "            sent = r.vocab.tokenizer(line, [' '], keepempty = false)\n",
    "            sent_ind = Int[]\n",
    "            for word in sent\n",
    "                ind = word2ind(r.vocab.w2i,word)\n",
    "                push!(sent_ind,ind)\n",
    "            end\n",
    "            push!(sent_ind,r.vocab.eos)\n",
    "            return (sent_ind, s)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "# File \n",
    "const datadir = \"nn4nlp-code/data/ptb\"\n",
    "isdir(datadir) || run(`git clone https://github.com/neubig/nn4nlp-code.git`)\n",
    "\n",
    "if !isdefined(Main, :vocab)\n",
    "    vocab = Vocab(\"$datadir/train.txt\", mincount=1)\n",
    "\n",
    "    train = TextReader(\"$datadir/train.txt\", vocab)\n",
    "    test = TextReader(\"$datadir/valid.txt\", vocab)\n",
    "\n",
    "end\n",
    "#Embed\n",
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    Embed(param(embedsize,vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    l.w[:,x]\n",
    "end\n",
    "\n",
    "#Linear\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    Linear(param(outputsize,inputsize), param0(outputsize))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    l.w * mat(x,dims=1) .+ l.b\n",
    "end\n",
    "# Mask!\n",
    "function mask!(a,pad)\n",
    "    matr = a\n",
    "    for j in 1:size(matr)[1]\n",
    "        i=0\n",
    "        while i<(length(matr[j,:])-1)\n",
    "            matr[j,length(matr[j,:])-i-1]!=pad && break\n",
    "\n",
    "            if matr[j,length(matr[j,:])-i]== pad\n",
    "                matr[j,length(matr[j,:])-i]= 0\n",
    "            end\n",
    "            i+=1\n",
    "        end\n",
    "    end\n",
    "    matr\n",
    "end\n",
    "# Minibatching\n",
    "struct LMData\n",
    "    src::TextReader\n",
    "    batchsize::Int\n",
    "    maxlength::Int\n",
    "    bucketwidth::Int\n",
    "    buckets\n",
    "end\n",
    "\n",
    "function LMData(src::TextReader; batchsize = 64, maxlength = typemax(Int), bucketwidth = 10)\n",
    "    numbuckets = min(128, maxlength ÷ bucketwidth)\n",
    "    buckets = [ [] for i in 1:numbuckets ]\n",
    "    LMData(src, batchsize, maxlength, bucketwidth, buckets)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{LMData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{LMData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{LMData}) = Matrix{Int}\n",
    "\n",
    "function Base.iterate(d::LMData, state=nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets; empty!(b); end\n",
    "    end\n",
    "    bucket,ibucket = nothing,nothing\n",
    "    while true\n",
    "        iter = (state === nothing ? iterate(d.src) : iterate(d.src, state))\n",
    "        if iter === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent, state = iter\n",
    "            if length(sent) > d.maxlength || length(sent) == 0; continue; end\n",
    "            ibucket = min(1 + (length(sent)-1) ÷ d.bucketwidth, length(d.buckets))\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, sent)\n",
    "            if length(bucket) === d.batchsize; break; end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing; return nothing; end\n",
    "    batchsize = length(bucket)\n",
    "    maxlen = maximum(length.(bucket))\n",
    "    batch = fill(d.src.vocab.eos, batchsize, maxlen + 1)\n",
    "    for i in 1:batchsize\n",
    "        batch[i, 1:length(bucket[i])] = bucket[i]\n",
    "    end\n",
    "    empty!(bucket)\n",
    "    return batch, state\n",
    "end\n",
    "struct RNN_model\n",
    "    embed::Embed        # language embedding\n",
    "    rnn::RNN            # RNN (can be bidirectional)\n",
    "    projection::Linear  # converts output to vocab scores\n",
    "    dropout::Real       # dropout probability to prevent overfitting\n",
    "    vocab::Vocab        # language vocabulary  \n",
    "end\n",
    "\n",
    "function RNN_model(hidden::Int,      # hidden size for both the encoder and decoder RNN\n",
    "                embsz::Int,          # embedding size\n",
    "                vocab::Vocab;     # vocabulary for source language\n",
    "                layers=1,            # number of layers\n",
    "                bidirectional=false, # whether encoder RNN is bidirectional\n",
    "                dropout=0)           # dropout probability\n",
    "\n",
    "    embed = Embed(length(vocab.i2w),embsz)\n",
    "\n",
    "    rnn = RNN(embsz,hidden;rnnType=is_lstm_strategy_on ? :lstm : :relu, numLayers=layers,bidirectional=bidirectional ,dropout= dropout)\n",
    "    \n",
    "    layerMultiplier = bidirectional ? 2 : 1\n",
    "    \n",
    "    projection = Linear(layerMultiplier*hidden,length(vocab.i2w))\n",
    "\n",
    "    RNN_model(embed,rnn,projection,dropout,vocab)\n",
    "\n",
    "end\n",
    "function calc_scores(rm::RNN_model, data; average=true)\n",
    "    B, Tx = size(data)\n",
    "    \n",
    "    project = rm.projection\n",
    "    emb = rm.embed(data)\n",
    "    \n",
    "#     rm.rnn.h = 0\n",
    "#     rm.rnn.c = 0\n",
    "\n",
    "    y = rm.rnn(emb)\n",
    "\n",
    "    return project(reshape(y,:,B*Tx))\n",
    "    \n",
    "\n",
    "end\n",
    "function loss_f(model, batch; average = true)  \n",
    "    verify = deepcopy(batch[:,2:end])\n",
    "    mask!(verify,vocab.eos)\n",
    "        \n",
    "    scores = calc_scores(model,batch[:,1:end-1]) # trim one end\n",
    "   \n",
    "    return nll(scores,verify;average=average)\n",
    "\n",
    "end\n",
    "function maploss(lossfn, model, data; average = true)\n",
    "    total_words = 0\n",
    "    total_loss = 0\n",
    "    for part in collect(data)\n",
    "        curr_loss, curr_word = lossfn(model,part, average = false)\n",
    "        total_loss += curr_loss\n",
    "        total_words += curr_word\n",
    "    end\n",
    "\n",
    "    average && return total_loss/total_words\n",
    "    return total_loss, total_words\n",
    "end\n",
    "model = RNN_model(512, 512, vocab; bidirectional=true, dropout=0.2)\n",
    "train_batches = collect(LMData(train))\n",
    "test_batches = collect(LMData(test))\n",
    "train_batches50 = train_batches[1:50] # Small sample for quick loss calculation\n",
    "epoch = adam(loss_f, ((model, batch) for batch in train_batches))\n",
    "bestmodel, bestloss = deepcopy(model), maploss(loss_f, model, test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mbj3Fg3D1mWf",
    "outputId": "e9ee4514-4b91-411d-fd85-33b0ea70c266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time=1.578230789136887e9\n",
      "train-nll= 7.245623\n",
      "train-nll= 6.812294\n",
      "train-nll= 6.5757356\n",
      "train-nll= 6.4693737\n",
      "train-nll= 6.3861575\n",
      "train-nll= 6.3389435\n",
      "train-nll= 6.3085117\n",
      "train-nll= 6.219439\n",
      "train-nll= 6.0866265\n",
      "train-nll= 5.8965154\n",
      "nll=5.954507    ppl=385.48676    words=70390    time=168.54808592796326    word_per_sec=39238.653845209636\n",
      "train-nll= 5.748297\n",
      "train-nll= 5.638169\n",
      "train-nll= 5.5402837\n",
      "train-nll= 5.481519\n",
      "train-nll= 5.3810406\n",
      "train-nll= 5.302901\n",
      "train-nll= 5.214938\n",
      "train-nll= 5.1277294\n",
      "train-nll= 5.0314837\n",
      "train-nll= 4.9339175\n",
      "nll=4.8852177    ppl=132.31926    words=70390    time=326.67675709724426    word_per_sec=40490.177867360675\n",
      "train-nll= 4.8225665\n",
      "train-nll= 4.72732\n",
      "train-nll= 4.6216307\n",
      "train-nll= 4.5201325\n",
      "train-nll= 4.410288\n",
      "train-nll= 4.320653\n",
      "train-nll= 4.2322564\n",
      "train-nll= 4.1465745\n",
      "train-nll= 4.0589604\n",
      "train-nll= 3.9847374\n",
      "nll=3.8957253    ppl=49.19172    words=70390    time=483.9358160495758    word_per_sec=40998.82534415979\n",
      "train-nll= 3.908325\n",
      "train-nll= 3.8422122\n",
      "train-nll= 3.781679\n",
      "train-nll= 3.7234533\n",
      "train-nll= 3.6634562\n",
      "train-nll= 3.618987\n",
      "train-nll= 3.5521395\n",
      "train-nll= 3.493374\n",
      "train-nll= 3.443836\n",
      "train-nll= 3.3975394\n",
      "nll=3.2694597    ppl=26.297127    words=70390    time=642.0458879470825    word_per_sec=41203.28546077438\n",
      "train-nll= 3.3439698\n",
      "train-nll= 3.3056087\n",
      "train-nll= 3.2576706\n",
      "train-nll= 3.2066574\n",
      "train-nll= 3.1581905\n",
      "train-nll= 3.110758\n",
      "train-nll= 3.0715206\n",
      "train-nll= 3.0685062\n",
      "train-nll= 2.9987154\n",
      "train-nll= 2.9480124\n",
      "nll=2.8153472    ppl=16.698973    words=70390    time=801.0410010814667    word_per_sec=41281.282675113594\n",
      "train-nll= 2.915185\n",
      "train-nll= 2.8796723\n",
      "train-nll= 2.8314095\n",
      "train-nll= 2.7989604\n",
      "train-nll= 2.7564914\n",
      "train-nll= 2.7291276\n",
      "train-nll= 2.6929572\n",
      "train-nll= 2.6505878\n",
      "train-nll= 2.61621\n",
      "train-nll= 2.588494\n",
      "nll=2.4289498    ppl=11.34696    words=70390    time=959.657487154007    word_per_sec=41349.7529391253\n",
      "train-nll= 2.5615544\n",
      "train-nll= 2.513555\n",
      "train-nll= 2.48383\n",
      "train-nll= 2.4532473\n",
      "train-nll= 2.414994\n",
      "train-nll= 2.3952022\n",
      "train-nll= 2.337931\n",
      "train-nll= 2.259759\n",
      "train-nll= 2.1357317\n",
      "train-nll= 2.0317426\n",
      "nll=2.1511    ppl=8.594306    words=70390    time=1118.390867471695    word_per_sec=41394.472493018344\n",
      "train-nll= 1.9721851\n",
      "train-nll= 1.9098845\n",
      "train-nll= 1.902846\n",
      "train-nll= 1.9031646\n",
      "train-nll= 1.9150175\n",
      "train-nll= 1.8801477\n",
      "train-nll= 1.8498174\n",
      "train-nll= 1.8295045\n",
      "train-nll= 1.8310492\n",
      "train-nll= 1.7892011\n",
      "nll=1.8967456    ppl=6.664171    words=70390    time=1275.991245508194    word_per_sec=41464.861288235414\n",
      "train-nll= 1.7931856\n",
      "train-nll= 1.7451277\n",
      "train-nll= 1.7454453\n",
      "train-nll= 1.7396318\n",
      "train-nll= 1.6960491\n",
      "train-nll= 1.6873034\n",
      "train-nll= 1.6564264\n",
      "train-nll= 1.6511611\n",
      "train-nll= 1.6312048\n",
      "train-nll= 1.6079848\n",
      "nll=1.6691248    ppl=5.307521    words=70390    time=1433.7840883731842    word_per_sec=41514.200417397544\n",
      "train-nll= 1.5785975\n",
      "train-nll= 1.5628647\n",
      "train-nll= 1.5491979\n",
      "train-nll= 1.5421793\n",
      "train-nll= 1.518401\n",
      "train-nll= 1.4900395\n",
      "train-nll= 1.4764276\n",
      "train-nll= 1.4726375\n",
      "train-nll= 1.4532349\n",
      "train-nll= 1.4441582\n",
      "nll=1.4976249    ppl=4.471057    words=70390    time=1590.8740363121033    word_per_sec=41572.116013228595\n",
      "train-nll= 1.4275852\n",
      "train-nll= 1.427231\n",
      "train-nll= 1.3983401\n",
      "train-nll= 1.396516\n",
      "train-nll= 1.377738\n",
      "train-nll= 1.3606348\n",
      "train-nll= 1.3472595\n",
      "train-nll= 1.3397539\n",
      "train-nll= 1.3343434\n",
      "train-nll= 1.3146343\n",
      "nll=1.3495369    ppl=3.8556395    words=70390    time=1749.037918329239    word_per_sec=41594.066793871316\n",
      "train-nll= 1.2970953\n",
      "train-nll= 1.2841187\n",
      "train-nll= 1.2661523\n",
      "train-nll= 1.277059\n",
      "train-nll= 1.2387779\n",
      "train-nll= 1.2265257\n",
      "train-nll= 1.2306781\n",
      "train-nll= 1.2279724\n",
      "train-nll= 1.2100718\n",
      "train-nll= 1.1933665\n",
      "nll=1.2214756    ppl=3.3921895    words=70390    time=1907.6233112812042    word_per_sec=41603.1821013436\n",
      "train-nll= 1.1805487\n",
      "train-nll= 1.1745707\n",
      "train-nll= 1.1613148\n",
      "train-nll= 1.1493694\n",
      "train-nll= 1.1437981\n",
      "train-nll= 1.1317588\n",
      "train-nll= 1.1143105\n",
      "train-nll= 1.100891\n",
      "train-nll= 1.1032169\n",
      "train-nll= 1.0934814\n",
      "nll=1.1067202    ppl=3.0244226    words=70390    time=2064.8444213867188    word_per_sec=41638.3913042026\n",
      "train-nll= 1.0815921\n",
      "train-nll= 1.0543816\n",
      "train-nll= 1.037886\n",
      "train-nll= 0.989767\n",
      "train-nll= 0.8856362\n",
      "train-nll= 0.8189107\n",
      "train-nll= 0.7959985\n",
      "train-nll= 0.7744768\n",
      "train-nll= 0.7825512\n",
      "train-nll= 0.7969833\n",
      "nll=1.036848    ppl=2.8203132    words=70390    time=2222.5741744041443    word_per_sec=41659.08209782146\n",
      "train-nll= 0.81885916\n",
      "train-nll= 0.83460855\n",
      "train-nll= 0.8011777\n",
      "train-nll= 0.7860183\n",
      "train-nll= 0.792305\n",
      "train-nll= 0.7871876\n",
      "train-nll= 0.82848334\n",
      "train-nll= 0.76221657\n",
      "train-nll= 0.76853985\n",
      "train-nll= 0.77383894\n",
      "nll=0.96089244    ppl=2.6140282    words=70390    time=2381.295644760132    word_per_sec=41659.673891518345\n",
      "train-nll= 0.76810944\n",
      "train-nll= 0.7552404\n",
      "train-nll= 0.75407904\n",
      "train-nll= 0.7423434\n",
      "train-nll= 0.73209846\n",
      "train-nll= 0.73751044\n",
      "train-nll= 0.7251544\n",
      "train-nll= 0.7094584\n",
      "train-nll= 0.7041293\n",
      "train-nll= 0.7129759\n",
      "nll=0.8857941    ppl=2.4249094    words=70390    time=2539.963397502899    word_per_sec=41661.0727949985\n",
      "train-nll= 0.70953244\n",
      "train-nll= 0.687817\n",
      "train-nll= 0.6750457\n",
      "train-nll= 0.6825929\n",
      "train-nll= 0.6797019\n",
      "train-nll= 0.6952554\n",
      "train-nll= 0.6784198\n",
      "train-nll= 0.6880731\n",
      "train-nll= 0.65889955\n",
      "train-nll= 0.680488\n",
      "nll=0.83855677    ppl=2.3130264    words=70390    time=2698.5797884464264    word_per_sec=41663.100154146894\n",
      "train-nll= 0.65833735\n",
      "train-nll= 0.65605843\n",
      "train-nll= 0.6476482\n",
      "train-nll= 0.6377732\n",
      "train-nll= 0.65081054\n",
      "train-nll= 0.63728523\n",
      "train-nll= 0.6335599\n",
      "train-nll= 0.61795944\n",
      "train-nll= 0.6046447\n",
      "train-nll= 0.61846083\n",
      "nll=0.7736207    ppl=2.1676004    words=70390    time=2859.238737821579    word_per_sec=41635.13820140072\n",
      "train-nll= 0.6103133\n",
      "train-nll= 0.5922574\n",
      "train-nll= 0.59926516\n",
      "train-nll= 0.6077524\n",
      "train-nll= 0.59779096\n",
      "train-nll= 0.5951513\n",
      "train-nll= 0.58376426\n",
      "train-nll= 0.5848849\n",
      "train-nll= 0.57902277\n",
      "train-nll= 0.5744501\n",
      "nll=0.7160026    ppl=2.0462372    words=70390    time=3016.952065706253    word_per_sec=41650.777759567754\n",
      "train-nll= 0.5607346\n",
      "train-nll= 0.55996794\n",
      "train-nll= 0.548658\n",
      "train-nll= 0.5415587\n",
      "train-nll= 0.5364882\n",
      "train-nll= 0.5515245\n",
      "train-nll= 0.54328775\n",
      "train-nll= 0.5237638\n",
      "train-nll= 0.5067732\n",
      "train-nll= 0.43367252\n",
      "nll=0.6678437    ppl=1.950028    words=70390    time=3175.0916888713837    word_per_sec=41659.269388537665\n",
      "train-nll= 0.394651\n",
      "train-nll= 0.3504895\n",
      "train-nll= 0.332991\n",
      "train-nll= 0.32205668\n",
      "train-nll= 0.33205798\n",
      "train-nll= 0.3535021\n",
      "train-nll= 0.35961157\n",
      "train-nll= 0.40901697\n",
      "train-nll= 0.36834785\n",
      "train-nll= 0.36015707\n",
      "nll=0.64150286    ppl=1.8993331    words=70390    time=3333.1684877872467    word_per_sec=41667.74062243713\n",
      "train-nll= 0.35544398\n",
      "train-nll= 0.35198766\n",
      "train-nll= 0.37991685\n",
      "train-nll= 0.3613544\n",
      "train-nll= 0.34571305\n",
      "train-nll= 0.37669817\n",
      "train-nll= 0.36190376\n",
      "train-nll= 0.35708892\n",
      "train-nll= 0.35149038\n",
      "train-nll= 0.3483065\n",
      "nll=0.6019635    ppl=1.8257    words=70390    time=3491.7105836868286    word_per_sec=41669.891164453344\n",
      "train-nll= 0.35316923\n",
      "train-nll= 0.35308728\n",
      "train-nll= 0.33926797\n",
      "train-nll= 0.33602202\n",
      "train-nll= 0.32217297\n",
      "train-nll= 0.32652733\n",
      "train-nll= 0.34379748\n",
      "train-nll= 0.33514115\n",
      "train-nll= 0.31974456\n",
      "train-nll= 0.3218373\n",
      "nll=0.55951905    ppl=1.7498307    words=70390    time=3650.56050491333    word_per_sec=41668.34101099535\n",
      "train-nll= 0.32346314\n",
      "train-nll= 0.32995844\n",
      "train-nll= 0.33698654\n",
      "train-nll= 0.31762567\n",
      "train-nll= 0.3215684\n",
      "train-nll= 0.31023887\n",
      "train-nll= 0.31923816\n",
      "train-nll= 0.30561268\n",
      "train-nll= 0.30687428\n",
      "train-nll= 0.29502416\n",
      "nll=0.51763886    ppl=1.6780608    words=70390    time=3810.067786693573    word_per_sec=41659.73124004307\n",
      "train-nll= 0.31280825\n",
      "train-nll= 0.3052999\n",
      "train-nll= 0.29964802\n",
      "train-nll= 0.30792853\n",
      "train-nll= 0.29592922\n",
      "train-nll= 0.28855655\n",
      "train-nll= 0.29346955\n",
      "train-nll= 0.28874868\n",
      "train-nll= 0.28299183\n",
      "train-nll= 0.28528154\n",
      "nll=0.49421325    ppl=1.6392081    words=70390    time=3969.304705619812    word_per_sec=41654.65043938519\n",
      "train-nll= 0.28781927\n",
      "train-nll= 0.28867143\n",
      "train-nll= 0.28901404\n",
      "train-nll= 0.27993453\n",
      "train-nll= 0.2732995\n",
      "train-nll= 0.2728318\n",
      "train-nll= 0.28202498\n",
      "train-nll= 0.26158866\n",
      "train-nll= 0.26509258\n",
      "train-nll= 0.25915524\n",
      "nll=0.46633327    ppl=1.5941381    words=70390    time=4127.485280752182    word_per_sec=41660.62100860203\n",
      "train-nll= 0.25717354\n",
      "train-nll= 0.25073558\n",
      "train-nll= 0.2640036\n",
      "train-nll= 0.25923523\n",
      "train-nll= 0.23040617\n",
      "train-nll= 0.19636916\n",
      "train-nll= 0.16522817\n",
      "train-nll= 0.13412933\n",
      "train-nll= 0.11647634\n",
      "train-nll= 0.113777556\n",
      "nll=0.47746262    ppl=1.611979    words=70390    time=4286.948201656342    word_per_sec=41653.68733193634\n",
      "train-nll= 0.11933681\n",
      "train-nll= 0.13584776\n",
      "train-nll= 0.151624\n",
      "train-nll= 0.17483987\n",
      "train-nll= 0.1830839\n",
      "train-nll= 0.1598472\n",
      "train-nll= 0.16099039\n",
      "train-nll= 0.1590736\n",
      "train-nll= 0.16275676\n",
      "train-nll= 0.17814584\n",
      "nll=0.45691416    ppl=1.5791934    words=70390    time=4446.503533840179    word_per_sec=41646.3854331114\n",
      "train-nll= 0.15248288\n",
      "train-nll= 0.15594435\n",
      "train-nll= 0.15952763\n",
      "train-nll= 0.16006157\n",
      "train-nll= 0.15679915\n",
      "train-nll= 0.15260936\n",
      "train-nll= 0.15437222\n",
      "train-nll= 0.15553999\n",
      "train-nll= 0.15678579\n",
      "train-nll= 0.14937869\n",
      "nll=0.41724107    ppl=1.5177684    words=70390    time=4605.474268198013    word_per_sec=41644.87495335491\n",
      "train-nll= 0.15128055\n",
      "train-nll= 0.1462846\n",
      "train-nll= 0.15514079\n",
      "train-nll= 0.15393046\n",
      "train-nll= 0.1461789\n",
      "train-nll= 0.13935639\n",
      "train-nll= 0.14501613\n",
      "train-nll= 0.149225\n",
      "train-nll= 0.15017271\n",
      "train-nll= 0.1497093\n",
      "nll=0.39769375    ppl=1.4883882    words=70390    time=4763.712932109833    word_per_sec=41649.86489060451\n",
      "train-nll= 0.14395553\n",
      "train-nll= 0.14632049\n",
      "train-nll= 0.14504217\n",
      "train-nll= 0.13650796\n",
      "train-nll= 0.13242777\n",
      "train-nll= 0.13115087\n",
      "train-nll= 0.13275564\n",
      "train-nll= 0.14929369\n",
      "train-nll= 0.13846736\n",
      "train-nll= 0.13174222\n",
      "nll=0.37333244    ppl=1.4525671    words=70390    time=4924.078771829605    word_per_sec=41636.53944224405\n",
      "train-nll= 0.12819824\n",
      "train-nll= 0.1292049\n",
      "train-nll= 0.13013746\n",
      "train-nll= 0.13376965\n",
      "train-nll= 0.12722437\n",
      "train-nll= 0.125758\n",
      "train-nll= 0.12856552\n",
      "train-nll= 0.12881897\n",
      "train-nll= 0.13635913\n",
      "train-nll= 0.13204634\n",
      "nll=0.3613787    ppl=1.4353069    words=70390    time=5083.1835289001465    word_per_sec=41634.381052102544\n",
      "train-nll= 0.12676409\n",
      "train-nll= 0.12354159\n",
      "train-nll= 0.12142542\n",
      "train-nll= 0.121584296\n",
      "train-nll= 0.11937056\n",
      "train-nll= 0.118262626\n",
      "train-nll= 0.1157863\n",
      "train-nll= 0.11172115\n",
      "train-nll= 0.1094742\n",
      "train-nll= 0.111046836\n",
      "nll=0.3483817    ppl=1.416773    words=70390    time=5243.283413887024    word_per_sec=41624.452231965995\n",
      "train-nll= 0.109188944\n",
      "train-nll= 0.08901132\n",
      "train-nll= 0.073085\n",
      "train-nll= 0.056441993\n",
      "train-nll= 0.043771368\n",
      "train-nll= 0.041449092\n",
      "train-nll= 0.04276762\n",
      "train-nll= 0.048392087\n",
      "train-nll= 0.055026025\n",
      "train-nll= 0.06509367\n",
      "nll=0.3476126    ppl=1.4156837    words=70390    time=5401.518267869949    word_per_sec=41629.48061058264\n",
      "train-nll= 0.08486011\n",
      "train-nll= 0.078855254\n",
      "train-nll= 0.07227243\n",
      "train-nll= 0.07091373\n",
      "train-nll= 0.06950131\n",
      "train-nll= 0.08204411\n",
      "train-nll= 0.07743384\n",
      "train-nll= 0.07557221\n",
      "train-nll= 0.07557719\n",
      "train-nll= 0.074083045\n",
      "nll=0.33826017    ppl=1.4025054    words=70390    time=5558.811393022537    word_per_sec=41641.27609915862\n",
      "train-nll= 0.07383316\n",
      "train-nll= 0.07198587\n",
      "train-nll= 0.07063682\n",
      "train-nll= 0.071342126\n",
      "train-nll= 0.0716218\n",
      "train-nll= 0.07162488\n",
      "train-nll= 0.07027107\n",
      "train-nll= 0.06703557\n",
      "train-nll= 0.068449385\n",
      "train-nll= 0.07280491\n",
      "nll=0.3270002    ppl=1.3868017    words=70390    time=5717.4272339344025    word_per_sec=41642.78621455415\n",
      "train-nll= 0.071074106\n",
      "train-nll= 0.06674683\n",
      "train-nll= 0.06532112\n",
      "train-nll= 0.066754974\n",
      "train-nll= 0.0679097\n",
      "train-nll= 0.06885448\n",
      "train-nll= 0.06603795\n",
      "train-nll= 0.064794876\n",
      "train-nll= 0.06782129\n",
      "train-nll= 0.0683443\n",
      "nll=0.314789    ppl=1.3699702    words=70390    time=5875.994184970856    word_per_sec=41644.56129413506\n",
      "train-nll= 0.064721785\n",
      "train-nll= 0.06225825\n",
      "train-nll= 0.059679918\n",
      "train-nll= 0.064455286\n",
      "train-nll= 0.07261386\n",
      "train-nll= 0.0645972\n",
      "train-nll= 0.05923338\n",
      "train-nll= 0.058826268\n",
      "train-nll= 0.060535047\n",
      "train-nll= 0.06029337\n",
      "nll=0.30066383    ppl=1.3507552    words=70390    time=6036.185019016266    word_per_sec=41635.03921901947\n",
      "train-nll= 0.06085661\n",
      "train-nll= 0.057224907\n",
      "train-nll= 0.05621774\n",
      "train-nll= 0.057027005\n",
      "train-nll= 0.05926033\n",
      "train-nll= 0.060723625\n",
      "train-nll= 0.058772862\n",
      "train-nll= 0.057031885\n",
      "train-nll= 0.05561887\n",
      "train-nll= 0.055225357\n",
      "nll=0.29127395    ppl=1.3381312    words=70390    time=6192.818542003632    word_per_sec=41649.92050882035\n",
      "train-nll= 0.053600047\n",
      "train-nll= 0.053854544\n",
      "train-nll= 0.054070346\n",
      "train-nll= 0.052415233\n",
      "train-nll= 0.04925187\n",
      "train-nll= 0.047131795\n",
      "train-nll= 0.04534094\n",
      "train-nll= 0.039955042\n",
      "train-nll= 0.032876115\n",
      "train-nll= 0.026551211\n",
      "nll=0.28918785    ppl=1.3353425    words=70390    time=6350.5605318546295    word_per_sec=41656.79528177682\n",
      "train-nll= 0.01979294\n",
      "train-nll= 0.016439686\n",
      "train-nll= 0.016335864\n",
      "train-nll= 0.018183615\n",
      "train-nll= 0.020875927\n",
      "train-nll= 0.022963278\n",
      "train-nll= 0.029910233\n",
      "train-nll= 0.035392728\n",
      "train-nll= 0.033406172\n",
      "train-nll= 0.031534847\n",
      "nll=0.28830358    ppl=1.3341622    words=70390    time=6508.276814699173    word_per_sec=41663.50137221898\n",
      "train-nll= 0.030788958\n",
      "train-nll= 0.032586522\n",
      "train-nll= 0.034868788\n",
      "train-nll= 0.033715207\n",
      "train-nll= 0.03570696\n",
      "train-nll= 0.034998797\n",
      "train-nll= 0.033771276\n",
      "train-nll= 0.033446953\n",
      "train-nll= 0.03293628\n",
      "train-nll= 0.033125862\n",
      "nll=0.2829516    ppl=1.3270409    words=70390    time=6665.8128707408905    word_per_sec=41671.01678165267\n",
      "train-nll= 0.03390562\n",
      "train-nll= 0.033531755\n",
      "train-nll= 0.03339224\n",
      "train-nll= 0.03125078\n",
      "train-nll= 0.030715965\n",
      "train-nll= 0.032075837\n",
      "train-nll= 0.033002146\n",
      "train-nll= 0.033414133\n",
      "train-nll= 0.032056913\n",
      "train-nll= 0.03200662\n",
      "nll=0.27286556    ppl=1.3137236    words=70390    time=6823.35893702507    word_per_sec=41678.124018489565\n",
      "train-nll= 0.03419332\n",
      "train-nll= 0.03562111\n",
      "train-nll= 0.03419475\n",
      "train-nll= 0.03222773\n",
      "train-nll= 0.032162167\n",
      "train-nll= 0.03297994\n",
      "train-nll= 0.031667344\n",
      "train-nll= 0.030238694\n",
      "train-nll= 0.029266167\n",
      "train-nll= 0.029703593\n",
      "nll=0.26686507    ppl=1.3058642    words=70390    time=6982.608219861984    word_per_sec=41674.742565715904\n",
      "train-nll= 0.033169083\n",
      "train-nll= 0.033636753\n",
      "train-nll= 0.030381083\n",
      "train-nll= 0.02820953\n",
      "train-nll= 0.027731914\n",
      "train-nll= 0.027880128\n",
      "train-nll= 0.029489826\n",
      "train-nll= 0.027682872\n",
      "train-nll= 0.026692979\n",
      "train-nll= 0.02686174\n",
      "nll=0.25711992    ppl=1.2932003    words=70390    time=7140.6911108493805    word_per_sec=41678.318720132855\n",
      "train-nll= 0.02678125\n",
      "train-nll= 0.026717205\n",
      "train-nll= 0.027292268\n",
      "train-nll= 0.02756607\n",
      "train-nll= 0.02736229\n",
      "train-nll= 0.026306158\n",
      "train-nll= 0.025938086\n",
      "train-nll= 0.024995275\n",
      "train-nll= 0.025086531\n",
      "train-nll= 0.024474762\n",
      "nll=0.24916004    ppl=1.2829473    words=70390    time=7298.047322750092    word_per_sec=41685.890286247144\n",
      "train-nll= 0.023401253\n",
      "train-nll= 0.021620333\n",
      "train-nll= 0.020607337\n",
      "train-nll= 0.017979503\n",
      "train-nll= 0.015336782\n",
      "train-nll= 0.012401151\n",
      "train-nll= 0.010541174\n",
      "train-nll= 0.008271806\n",
      "train-nll= 0.0077116946\n",
      "train-nll= 0.0082442565\n",
      "nll=0.25542703    ppl=1.2910128    words=70390    time=7455.592351913452    word_per_sec=41692.08633304961\n",
      "train-nll= 0.009069242\n",
      "train-nll= 0.010147273\n",
      "train-nll= 0.01193864\n",
      "train-nll= 0.013504857\n",
      "train-nll= 0.014680806\n",
      "train-nll= 0.014831725\n",
      "train-nll= 0.014575037\n",
      "train-nll= 0.014965245\n",
      "train-nll= 0.015602382\n",
      "train-nll= 0.016410496\n",
      "nll=0.25922674    ppl=1.2959276    words=70390    time=7613.294469833374    word_per_sec=41697.165564508614\n",
      "train-nll= 0.016950918\n",
      "train-nll= 0.017244184\n",
      "train-nll= 0.017276552\n",
      "train-nll= 0.017167898\n",
      "train-nll= 0.01693825\n",
      "train-nll= 0.01680201\n",
      "train-nll= 0.01697108\n",
      "train-nll= 0.016994303\n",
      "train-nll= 0.016889328\n",
      "train-nll= 0.016742181\n",
      "nll=0.25042036    ppl=1.2845652    words=70390    time=7770.638499498367    word_per_sec=41703.96036579492\n",
      "train-nll= 0.0160805\n",
      "train-nll= 0.015727628\n",
      "train-nll= 0.0157075\n",
      "train-nll= 0.015949085\n",
      "train-nll= 0.01599908\n",
      "train-nll= 0.01580093\n",
      "train-nll= 0.016149508\n",
      "train-nll= 0.017175032\n",
      "train-nll= 0.017205989\n",
      "train-nll= 0.016328368\n",
      "nll=0.24636382    ppl=1.279365    words=70390    time=7928.439081430435    word_per_sec=41708.08359674491\n",
      "train-nll= 0.015797783\n",
      "train-nll= 0.016254073\n",
      "train-nll= 0.01640035\n",
      "train-nll= 0.01574417\n",
      "train-nll= 0.014938284\n",
      "train-nll= 0.014949086\n",
      "train-nll= 0.015701191\n",
      "train-nll= 0.016043494\n",
      "train-nll= 0.01576246\n",
      "train-nll= 0.015426551\n",
      "nll=0.24089354    ppl=1.2723856    words=70390    time=8088.458527565002    word_per_sec=41700.603254689726\n",
      "train-nll= 0.014587841\n",
      "train-nll= 0.014162554\n",
      "train-nll= 0.014265227\n",
      "train-nll= 0.014082868\n",
      "train-nll= 0.013710149\n",
      "train-nll= 0.013523768\n",
      "train-nll= 0.013384889\n",
      "train-nll= 0.013293932\n",
      "train-nll= 0.013224564\n",
      "train-nll= 0.013181896\n",
      "nll=0.23277058    ppl=1.2620919    words=70390    time=8246.462809562683    word_per_sec=41703.60164617509\n",
      "train-nll= 0.013039602\n",
      "train-nll= 0.012610579\n",
      "train-nll= 0.012335458\n",
      "train-nll= 0.012057937\n",
      "train-nll= 0.011898361\n",
      "train-nll= 0.011657138\n",
      "train-nll= 0.011174063\n",
      "train-nll= 0.010306809\n",
      "train-nll= 0.009610475\n",
      "train-nll= 0.008350819\n",
      "nll=0.2274104    ppl=1.255345    words=70390    time=8403.82805776596    word_per_sec=41709.65869251506\n",
      "train-nll= 0.0071985475\n",
      "train-nll= 0.006275868\n",
      "train-nll= 0.005346585\n",
      "train-nll= 0.0045174677\n",
      "train-nll= 0.0041200556\n",
      "train-nll= 0.0042586057\n",
      "train-nll= 0.004552353\n",
      "train-nll= 0.0050049466\n",
      "train-nll= 0.005690046\n",
      "train-nll= 0.0065070023\n",
      "nll=0.23360951    ppl=1.2631512    words=70390    time=8561.132365703583    word_per_sec=41715.79000818889\n",
      "train-nll= 0.007098469\n",
      "train-nll= 0.0076181106\n",
      "train-nll= 0.007714895\n",
      "train-nll= 0.00773257\n",
      "train-nll= 0.008030432\n",
      "train-nll= 0.008394337\n",
      "train-nll= 0.008890752\n",
      "train-nll= 0.009025229\n",
      "train-nll= 0.009003502\n",
      "train-nll= 0.008752136\n",
      "nll=0.23196317    ppl=1.2610734    words=70390    time=8718.749314785004    word_per_sec=41720.20399567707\n",
      "train-nll= 0.008664352\n",
      "train-nll= 0.00882371\n",
      "train-nll= 0.00915998\n",
      "train-nll= 0.009757198\n",
      "train-nll= 0.00993311\n",
      "train-nll= 0.009857471\n",
      "train-nll= 0.009504743\n",
      "train-nll= 0.0093212575\n",
      "train-nll= 0.009309441\n",
      "train-nll= 0.009299033\n",
      "nll=0.22676407    ppl=1.2545339    words=70390    time=8876.411328792572    word_per_sec=41724.24939329384\n",
      "train-nll= 0.009248646\n",
      "train-nll= 0.009254371\n",
      "train-nll= 0.00928836\n",
      "train-nll= 0.009434022\n",
      "train-nll= 0.009645175\n",
      "train-nll= 0.0095027825\n",
      "train-nll= 0.008994074\n",
      "train-nll= 0.009051781\n",
      "train-nll= 0.0092920065\n",
      "train-nll= 0.008986497\n",
      "nll=0.2254434    ppl=1.2528781    words=70390    time=9035.099509716034    word_per_sec=41723.414290524845\n",
      "train-nll= 0.008727705\n",
      "train-nll= 0.0086857155\n",
      "train-nll= 0.008860189\n",
      "train-nll= 0.008782376\n",
      "train-nll= 0.008576151\n",
      "train-nll= 0.008503878\n",
      "train-nll= 0.008423655\n",
      "train-nll= 0.008165413\n",
      "train-nll= 0.007988837\n",
      "train-nll= 0.007875026\n",
      "nll=0.21772318    ppl=1.2432429    words=70390    time=9195.377797842026    word_per_sec=41715.39315002596\n",
      "train-nll= 0.0076942346\n",
      "train-nll= 0.0076937033\n",
      "train-nll= 0.007547207\n",
      "train-nll= 0.0075327074\n",
      "train-nll= 0.0074991756\n",
      "train-nll= 0.0074461764\n",
      "train-nll= 0.007227248\n",
      "train-nll= 0.0070762364\n",
      "train-nll= 0.0069666086\n",
      "train-nll= 0.0067622894\n",
      "nll=0.21523778    ppl=1.2401568    words=70390    time=9352.413836956024    word_per_sec=41722.105843746656\n",
      "train-nll= 0.0066648345\n",
      "train-nll= 0.006529288\n",
      "train-nll= 0.0062575885\n",
      "train-nll= 0.0058932253\n",
      "train-nll= 0.005419746\n",
      "train-nll= 0.0048028496\n",
      "train-nll= 0.004227747\n",
      "train-nll= 0.003862734\n",
      "train-nll= 0.0034033146\n",
      "train-nll= 0.0030213848\n",
      "nll=0.21536887    ppl=1.2403194    words=70390    time=9509.52718782425    word_per_sec=41728.25758446465\n",
      "train-nll= 0.0027623882\n",
      "train-nll= 0.0027665799\n",
      "train-nll= 0.0028785001\n",
      "train-nll= 0.0030626259\n",
      "train-nll= 0.0033603988\n",
      "train-nll= 0.0036641546\n",
      "train-nll= 0.0039578914\n",
      "train-nll= 0.004244868\n",
      "train-nll= 0.0044643306\n",
      "train-nll= 0.004601717\n",
      "nll=0.21727502    ppl=1.2426858    words=70390    time=9667.502025842667    word_per_sec=41730.49035020348\n",
      "train-nll= 0.004785512\n",
      "train-nll= 0.0049381345\n",
      "train-nll= 0.0051530604\n",
      "train-nll= 0.0053142095\n",
      "train-nll= 0.005470522\n",
      "train-nll= 0.005464492\n",
      "train-nll= 0.005397313\n",
      "train-nll= 0.0054571056\n",
      "train-nll= 0.005615481\n",
      "train-nll= 0.0060702763\n",
      "nll=0.21833405    ppl=1.2440026    words=70390    time=9824.471104860306    word_per_sec=41736.923608757505\n",
      "train-nll= 0.0063695903\n",
      "train-nll= 0.006455137\n",
      "train-nll= 0.0063537834\n",
      "train-nll= 0.006195442\n",
      "train-nll= 0.0062525887\n",
      "train-nll= 0.006230549\n",
      "train-nll= 0.0060968436\n",
      "train-nll= 0.0060684416\n",
      "train-nll= 0.0061981063\n",
      "train-nll= 0.0063071726\n",
      "nll=0.21517323    ppl=1.2400767    words=70390    time=9982.702803850174    word_per_sec=41737.874820765166\n",
      "train-nll= 0.0063325795\n",
      "train-nll= 0.006206703\n",
      "train-nll= 0.0060739485\n",
      "train-nll= 0.0058566765\n",
      "train-nll= 0.0058332058\n",
      "train-nll= 0.005756404\n",
      "train-nll= 0.0056815194\n",
      "train-nll= 0.005699371\n",
      "train-nll= 0.005624844\n",
      "train-nll= 0.0055849613\n",
      "nll=0.21279947    ppl=1.2371366    words=70390    time=10142.444231748581    word_per_sec=41732.583421563184\n",
      "train-nll= 0.0055010314\n",
      "train-nll= 0.0054686423\n",
      "train-nll= 0.0053372\n",
      "train-nll= 0.0051995185\n",
      "train-nll= 0.005163678\n",
      "train-nll= 0.005094419\n",
      "train-nll= 0.004969444\n",
      "train-nll= 0.004875597\n",
      "train-nll= 0.004819892\n",
      "train-nll= 0.0047565377\n",
      "nll=0.20516764    ppl=1.2277309    words=70390    time=10300.642487764359    word_per_sec=41733.70743724371\n",
      "train-nll= 0.0047606146\n",
      "train-nll= 0.004798402\n",
      "train-nll= 0.0047622486\n",
      "train-nll= 0.0046393205\n",
      "train-nll= 0.0045422665\n",
      "train-nll= 0.004400711\n",
      "train-nll= 0.004277289\n",
      "train-nll= 0.0041924673\n",
      "train-nll= 0.0040672338\n",
      "train-nll= 0.0038014338\n",
      "nll=0.20513864    ppl=1.2276952    words=70390    time=10457.808250665665    word_per_sec=41738.91790110188\n",
      "train-nll= 0.0035146037\n",
      "train-nll= 0.0033571818\n",
      "train-nll= 0.0025628938\n",
      "train-nll= 0.002419496\n",
      "train-nll= 0.0021834462\n",
      "train-nll= 0.0019545692\n",
      "train-nll= 0.0018021711\n",
      "train-nll= 0.0017754798\n",
      "train-nll= 0.0018166791\n",
      "train-nll= 0.0019185835\n",
      "nll=0.2062124    ppl=1.2290142    words=70390    time=10615.206726551056    word_per_sec=41743.058935600166\n",
      "train-nll= 0.0020771169\n",
      "train-nll= 0.0022694818\n",
      "train-nll= 0.0024174748\n",
      "train-nll= 0.0025521654\n",
      "train-nll= 0.0026606747\n",
      "train-nll= 0.0027836096\n",
      "train-nll= 0.0030405822\n",
      "train-nll= 0.0031322069\n",
      "train-nll= 0.0033502397\n",
      "train-nll= 0.0034189138\n",
      "nll=0.2094177    ppl=1.2329599    words=70390    time=10772.961522340775    word_per_sec=41745.69815991348\n",
      "train-nll= 0.0034427776\n",
      "train-nll= 0.003392717\n",
      "train-nll= 0.0033974966\n",
      "train-nll= 0.0034019572\n",
      "train-nll= 0.0035208468\n",
      "train-nll= 0.0037905565\n",
      "train-nll= 0.004039268\n",
      "train-nll= 0.0041445713\n",
      "train-nll= 0.004208739\n",
      "train-nll= 0.0041363914\n",
      "nll=0.20724662    ppl=1.2302859    words=70390    time=10930.956746339798    word_per_sec=41747.34294441369\n",
      "train-nll= 0.004101559\n",
      "train-nll= 0.004118376\n",
      "train-nll= 0.004058212\n",
      "train-nll= 0.00399969\n",
      "train-nll= 0.004078996\n",
      "train-nll= 0.00418461\n",
      "train-nll= 0.0042878794\n",
      "train-nll= 0.0043424424\n",
      "train-nll= 0.004219595\n",
      "train-nll= 0.004150339\n",
      "nll=0.20489095    ppl=1.2273912    words=70390    time=11089.163581371307    word_per_sec=41748.144177232025\n",
      "train-nll= 0.0040785726\n",
      "train-nll= 0.00401387\n",
      "train-nll= 0.0038636534\n",
      "train-nll= 0.003807393\n",
      "train-nll= 0.0038007216\n",
      "train-nll= 0.0038123499\n",
      "train-nll= 0.003700019\n",
      "train-nll= 0.0036132196\n",
      "train-nll= 0.003528624\n",
      "train-nll= 0.0034548794\n",
      "nll=0.20082156    ppl=1.2224066    words=70390    time=11248.585235357285    word_per_sec=41744.414090763246\n",
      "train-nll= 0.0033922202\n",
      "train-nll= 0.0032897345\n",
      "train-nll= 0.0032396382\n",
      "train-nll= 0.0032083092\n",
      "train-nll= 0.0032233133\n",
      "train-nll= 0.0031750868\n",
      "train-nll= 0.0031350409\n",
      "train-nll= 0.0030912857\n",
      "train-nll= 0.0030839655\n",
      "train-nll= 0.0030480279\n",
      "nll=0.19761698    ppl=1.2184956    words=70390    time=11405.625354528427    word_per_sec=41749.50388063907\n",
      "train-nll= 0.003010612\n",
      "train-nll= 0.003111428\n",
      "train-nll= 0.0030359232\n",
      "train-nll= 0.0029770571\n",
      "train-nll= 0.0027657414\n",
      "train-nll= 0.0025529074\n",
      "train-nll= 0.0023824438\n",
      "train-nll= 0.0022358655\n",
      "train-nll= 0.0016861547\n",
      "train-nll= 0.0015000481\n",
      "nll=0.19736671    ppl=1.2181907    words=70390    time=11563.457284450531    word_per_sec=41751.59626777151\n",
      "train-nll= 0.0013282485\n",
      "train-nll= 0.0011711083\n",
      "train-nll= 0.0010533828\n",
      "train-nll= 0.0010208491\n",
      "train-nll= 0.0010505894\n",
      "train-nll= 0.0011080919\n",
      "train-nll= 0.001187802\n",
      "train-nll= 0.0013232694\n",
      "train-nll= 0.0014183477\n",
      "train-nll= 0.0014961489\n",
      "nll=0.20040086    ppl=1.2218925    words=70390    time=11720.226195573807    word_per_sec=41757.41933929794\n",
      "train-nll= 0.0015762208\n",
      "train-nll= 0.0016619806\n",
      "train-nll= 0.001762167\n",
      "train-nll= 0.0018673793\n",
      "train-nll= 0.002012256\n",
      "train-nll= 0.002094792\n",
      "train-nll= 0.0021486743\n",
      "train-nll= 0.0021796296\n",
      "train-nll= 0.0021791777\n",
      "train-nll= 0.0021519912\n",
      "nll=0.20129104    ppl=1.2229806    words=70390    time=11878.83573460579    word_per_sec=41756.61749029657\n",
      "train-nll= 0.0021686477\n",
      "train-nll= 0.0022586472\n",
      "train-nll= 0.0023711151\n",
      "train-nll= 0.0024787725\n",
      "train-nll= 0.00253175\n",
      "train-nll= 0.0025575901\n",
      "train-nll= 0.0025674605\n",
      "train-nll= 0.0027608532\n",
      "train-nll= 0.0031077864\n",
      "train-nll= 0.0030002154\n",
      "nll=0.19993955    ppl=1.221329    words=70390    time=12036.436363697052    word_per_sec=41759.336801379766\n",
      "train-nll= 0.0030078806\n",
      "train-nll= 0.0029966293\n",
      "train-nll= 0.0031098085\n",
      "train-nll= 0.0030853641\n",
      "train-nll= 0.0030419913\n",
      "train-nll= 0.0029346596\n",
      "train-nll= 0.0028506373\n",
      "train-nll= 0.002767612\n",
      "train-nll= 0.0026912463\n",
      "train-nll= 0.0026347432\n",
      "nll=0.19767171    ppl=1.2185622    words=70390    time=12194.993634700775    word_per_sec=41758.70978324584\n",
      "train-nll= 0.0026100131\n",
      "train-nll= 0.0026372217\n",
      "train-nll= 0.0025648477\n",
      "train-nll= 0.0025208732\n",
      "train-nll= 0.0024986938\n",
      "train-nll= 0.002438293\n",
      "train-nll= 0.0023458304\n",
      "train-nll= 0.00228649\n",
      "train-nll= 0.0022525645\n",
      "train-nll= 0.0022455836\n",
      "nll=0.1924598    ppl=1.2122278    words=70390    time=12353.8392765522    word_per_sec=41757.12411761036\n",
      "train-nll= 0.0022355332\n",
      "train-nll= 0.0021783223\n",
      "train-nll= 0.0022111803\n",
      "train-nll= 0.0022189973\n",
      "train-nll= 0.002199927\n",
      "train-nll= 0.002141503\n",
      "train-nll= 0.0020759322\n",
      "train-nll= 0.002058337\n",
      "train-nll= 0.0019794707\n",
      "train-nll= 0.00189648\n",
      "nll=0.19225311    ppl=1.2119772    words=70390    time=12511.118861675262    word_per_sec=41760.805390513226\n",
      "train-nll= 0.0018079911\n",
      "train-nll= 0.0017351821\n",
      "train-nll= 0.0016906584\n",
      "train-nll= 0.0015658448\n",
      "train-nll= 0.0013780391\n",
      "train-nll= 0.0012058555\n",
      "train-nll= 0.0010250638\n",
      "train-nll= 0.00088025996\n",
      "train-nll= 0.0008519669\n",
      "train-nll= 0.0007205354\n",
      "nll=0.19128291    ppl=1.210802    words=70390    time=12668.660198688507    word_per_sec=41763.532346914835\n",
      "train-nll= 0.0007302039\n",
      "train-nll= 0.0007614787\n",
      "train-nll= 0.00080212374\n",
      "train-nll= 0.0008747598\n",
      "train-nll= 0.00095527124\n",
      "train-nll= 0.0010478137\n",
      "train-nll= 0.0010929799\n",
      "train-nll= 0.0011447229\n",
      "train-nll= 0.0012263522\n",
      "train-nll= 0.0012613147\n",
      "nll=0.19078207    ppl=1.2101957    words=70390    time=12826.997371673584    word_per_sec=41763.60097983751\n",
      "train-nll= 0.0013533272\n",
      "train-nll= 0.0013851129\n",
      "train-nll= 0.0014508599\n",
      "train-nll= 0.0014530683\n",
      "train-nll= 0.0014422684\n",
      "train-nll= 0.0014774218\n",
      "train-nll= 0.0015283083\n",
      "train-nll= 0.0015852569\n",
      "train-nll= 0.0016608612\n",
      "train-nll= 0.0017603306\n",
      "nll=0.19162384    ppl=1.2112148    words=70390    time=12985.051552772522    word_per_sec=41764.578122464736\n",
      "train-nll= 0.0018764427\n",
      "train-nll= 0.001902243\n",
      "train-nll= 0.001903304\n",
      "train-nll= 0.0020165117\n",
      "train-nll= 0.0021815044\n",
      "train-nll= 0.0023069591\n",
      "train-nll= 0.0022269036\n",
      "train-nll= 0.0022199552\n",
      "train-nll= 0.0020898862\n",
      "train-nll= 0.00212383\n",
      "nll=0.19095656    ppl=1.2104069    words=70390    time=13142.846274852753    word_per_sec=41766.35627628917\n",
      "train-nll= 0.0020936988\n",
      "train-nll= 0.002019198\n",
      "train-nll= 0.0019641207\n",
      "train-nll= 0.0019390602\n",
      "train-nll= 0.0018751029\n",
      "train-nll= 0.0018073886\n",
      "train-nll= 0.00178163\n",
      "train-nll= 0.0017593368\n",
      "train-nll= 0.0017169818\n",
      "train-nll= 0.0016964346\n",
      "nll=0.18848719    ppl=1.2074217    words=70390    time=13302.87398481369    word_per_sec=41761.081149396494\n",
      "train-nll= 0.0016760605\n",
      "train-nll= 0.0016330313\n",
      "train-nll= 0.001572444\n",
      "train-nll= 0.0015115899\n",
      "train-nll= 0.0014384714\n",
      "train-nll= 0.00141429\n",
      "train-nll= 0.0016117209\n",
      "train-nll= 0.0014968732\n",
      "train-nll= 0.0014266028\n",
      "train-nll= 0.0014967277\n",
      "nll=0.18422608    ppl=1.2022877    words=70390    time=13460.95963382721    word_per_sec=41761.95570688063\n",
      "train-nll= 0.001547548\n",
      "train-nll= 0.0014949725\n",
      "train-nll= 0.0014528036\n",
      "train-nll= 0.0013953825\n",
      "train-nll= 0.0013501487\n",
      "train-nll= 0.0012629199\n",
      "train-nll= 0.0011909803\n",
      "train-nll= 0.0010957617\n",
      "train-nll= 0.0010069396\n",
      "train-nll= 0.0009524325\n",
      "nll=0.18343394    ppl=1.2013355    words=70390    time=13618.25455379486    word_per_sec=41765.23487303347\n",
      "train-nll= 0.0008640958\n",
      "train-nll= 0.0007553462\n",
      "train-nll= 0.0006619717\n",
      "train-nll= 0.0005685333\n",
      "train-nll= 0.0005013318\n",
      "train-nll= 0.00046674718\n",
      "train-nll= 0.0004943133\n",
      "train-nll= 0.00059778907\n",
      "train-nll= 0.0006354599\n",
      "train-nll= 0.0007422993\n",
      "nll=0.18917589    ppl=1.2082535    words=70390    time=13775.7969019413    word_per_sec=41767.68894719378\n",
      "train-nll= 0.0008481409\n",
      "train-nll= 0.00086441013\n",
      "train-nll= 0.00090175145\n",
      "train-nll= 0.00093028706\n",
      "train-nll= 0.00093833724\n",
      "train-nll= 0.0009596454\n",
      "train-nll= 0.0009852634\n",
      "train-nll= 0.0010991553\n",
      "train-nll= 0.0011178267\n",
      "train-nll= 0.0011816298\n",
      "nll=0.18840395    ppl=1.2073212    words=70390    time=13933.17792224884    word_per_sec=41770.57116816496\n",
      "train-nll= 0.0011698921\n",
      "train-nll= 0.0011742545\n",
      "train-nll= 0.0012060951\n",
      "train-nll= 0.0012431813\n",
      "train-nll= 0.0012934533\n",
      "train-nll= 0.0013932418\n",
      "train-nll= 0.001416859\n",
      "train-nll= 0.0014384008\n",
      "train-nll= 0.0014181992\n",
      "train-nll= 0.0014098668\n",
      "nll=0.18726672    ppl=1.2059488    words=70390    time=14090.962846040726    word_per_sec=41772.19161183067\n",
      "train-nll= 0.0013548437\n",
      "train-nll= 0.0013842289\n",
      "train-nll= 0.0013603147\n",
      "train-nll= 0.0013681012\n",
      "train-nll= 0.0014191562\n",
      "train-nll= 0.0014643655\n",
      "train-nll= 0.0014840288\n",
      "train-nll= 0.0014900173\n",
      "train-nll= 0.0014681607\n",
      "train-nll= 0.0014528804\n",
      "nll=0.18415375    ppl=1.2022007    words=70390    time=14249.138371944427    word_per_sec=41772.631050587246\n",
      "train-nll= 0.0013972387\n",
      "train-nll= 0.0013449683\n",
      "train-nll= 0.0013111267\n",
      "train-nll= 0.0012712297\n",
      "train-nll= 0.0012525666\n",
      "train-nll= 0.0012101297\n",
      "train-nll= 0.0011780048\n",
      "train-nll= 0.001146975\n",
      "train-nll= 0.0011138666\n",
      "train-nll= 0.0010721853\n",
      "nll=0.179846    ppl=1.197033    words=70390    time=14407.377636909485    word_per_sec=41772.87603388591\n",
      "train-nll= 0.0015931013\n",
      "train-nll= 0.0013953261\n",
      "train-nll= 0.001276917\n",
      "train-nll= 0.0011424446\n",
      "train-nll= 0.0010775705\n",
      "train-nll= 0.0011274597\n",
      "train-nll= 0.0010851087\n",
      "train-nll= 0.0012708893\n",
      "train-nll= 0.0012563266\n",
      "train-nll= 0.0011933063\n",
      "nll=0.18333834    ppl=1.2012208    words=70390    time=14566.602139949799    word_per_sec=41770.290295173596\n",
      "train-nll= 0.0013983331\n",
      "train-nll= 0.0013182386\n",
      "train-nll= 0.0011457185\n",
      "train-nll= 0.0010434756\n",
      "train-nll= 0.0008958415\n",
      "train-nll= 0.0007887015\n",
      "train-nll= 0.0007606384\n",
      "train-nll= 0.0006462445\n",
      "train-nll= 0.00053522794\n",
      "train-nll= 0.00044348286\n",
      "nll=0.17957729    ppl=1.1967114    words=70390    time=14724.313082695007    word_per_sec=41772.05391828194\n",
      "train-nll= 0.00038048887\n",
      "train-nll= 0.00034395725\n",
      "train-nll= 0.00035911443\n",
      "train-nll= 0.0004184918\n",
      "train-nll= 0.00049528154\n",
      "train-nll= 0.0005402242\n",
      "train-nll= 0.000580745\n",
      "train-nll= 0.00059905084\n",
      "train-nll= 0.000677588\n",
      "train-nll= 0.0006876536\n",
      "nll=0.18409015    ppl=1.2021242    words=70390    time=14882.047885894775    word_per_sec=41773.713185617926\n",
      "train-nll= 0.0007684864\n",
      "train-nll= 0.0007703101\n",
      "train-nll= 0.00086781057\n",
      "train-nll= 0.0013274194\n",
      "train-nll= 0.0014337915\n",
      "train-nll= 0.0015795337\n",
      "train-nll= 0.0016441451\n",
      "train-nll= 0.0014625073\n",
      "train-nll= 0.0014551635\n",
      "train-nll= 0.0013082135\n",
      "nll=0.18465205    ppl=1.2027998    words=70390    time=15039.272901773453    word_per_sec=41776.753710341334\n",
      "train-nll= 0.0013357\n",
      "train-nll= 0.0012973616\n",
      "train-nll= 0.0013125482\n",
      "train-nll= 0.0014020981\n",
      "train-nll= 0.0015324807\n",
      "train-nll= 0.0015697582\n",
      "train-nll= 0.0015111025\n",
      "train-nll= 0.0019980152\n",
      "train-nll= 0.0019261126\n",
      "train-nll= 0.002076859\n",
      "nll=0.18679507    ppl=1.2053802    words=70390    time=15197.300994634628    word_per_sec=41777.52353685381\n",
      "train-nll= 0.0024632036\n",
      "train-nll= 0.0025392205\n",
      "train-nll= 0.0026846584\n",
      "train-nll= 0.0024580667\n",
      "train-nll= 0.00271798\n",
      "train-nll= 0.002707955\n",
      "train-nll= 0.002289412\n",
      "train-nll= 0.0019691742\n",
      "train-nll= 0.0018577779\n",
      "train-nll= 0.0025679045\n",
      "nll=0.19643609    ppl=1.2170576    words=70390    time=15356.893162727356    word_per_sec=41774.022466798706\n",
      "train-nll= 0.0028085331\n",
      "train-nll= 0.0030015088\n",
      "train-nll= 0.002440675\n",
      "train-nll= 0.0024802762\n",
      "train-nll= 0.0026513832\n",
      "train-nll= 0.0029905867\n",
      "train-nll= 0.0036022004\n",
      "train-nll= 0.0041815504\n",
      "train-nll= 0.004381204\n",
      "train-nll= 0.0038319663\n",
      "nll=0.19548878    ppl=1.2159052    words=70390    time=15515.545926809311    word_per_sec=41773.122457785474\n",
      "train-nll= 0.0038604615\n",
      "train-nll= 0.0032232138\n",
      "train-nll= 0.003345048\n",
      "train-nll= 0.0037378194\n",
      "train-nll= 0.0038070416\n",
      "train-nll= 0.0035332083\n",
      "train-nll= 0.00530456\n",
      "train-nll= 0.0047927243\n",
      "train-nll= 0.0055852863\n",
      "train-nll= 0.0072945114\n",
      "nll=0.20936847    ppl=1.2328992    words=70390    time=15672.618773698807    word_per_sec=41776.45162267142\n",
      "train-nll= 0.005986819\n",
      "train-nll= 0.0070523336\n",
      "train-nll= 0.008017674\n",
      "train-nll= 0.007145628\n",
      "train-nll= 0.0075296527\n",
      "train-nll= 0.005846364\n",
      "train-nll= 0.0072879246\n",
      "train-nll= 0.0085312305\n",
      "train-nll= 0.011154518\n",
      "train-nll= 0.012592546\n",
      "nll=0.25126278    ppl=1.2856479    words=70390    time=15829.997237682343    word_per_sec=41778.908111599216\n",
      "train-nll= 0.01547262\n",
      "train-nll= 0.018625641\n",
      "train-nll= 0.020776654\n",
      "train-nll= 0.022890745\n",
      "train-nll= 0.021306193\n",
      "train-nll= 0.019519014\n",
      "train-nll= 0.019546028\n",
      "train-nll= 0.020148117\n",
      "train-nll= 0.018859966\n",
      "train-nll= 0.019971222\n",
      "nll=0.25745136    ppl=1.2936289    words=70390    time=15988.04938173294    word_per_sec=41779.55572011115\n",
      "train-nll= 0.018164251\n",
      "train-nll= 0.01876556\n",
      "train-nll= 0.01981895\n",
      "train-nll= 0.017262524\n",
      "train-nll= 0.017207181\n",
      "train-nll= 0.017790414\n",
      "train-nll= 0.018311882\n",
      "train-nll= 0.016000237\n",
      "train-nll= 0.015184998\n",
      "train-nll= 0.013912732\n",
      "nll=0.23487987    ppl=1.2647568    words=70390    time=16145.990547657013    word_per_sec=41780.47782258185\n",
      "train-nll= 0.014413606\n",
      "train-nll= 0.015888458\n",
      "train-nll= 0.015057103\n",
      "train-nll= 0.01411114\n",
      "train-nll= 0.0140183745\n",
      "train-nll= 0.013216605\n",
      "train-nll= 0.015126718\n",
      "train-nll= 0.016318388\n",
      "train-nll= 0.0149757825\n",
      "train-nll= 0.012745746\n",
      "nll=0.2243071    ppl=1.2514553    words=70390    time=16303.375666618347    word_per_sec=41782.8070658262\n",
      "train-nll= 0.012482085\n",
      "train-nll= 0.012032099\n",
      "train-nll= 0.011612884\n",
      "train-nll= 0.011504476\n",
      "train-nll= 0.011307328\n",
      "train-nll= 0.0114508895\n",
      "train-nll= 0.011659839\n",
      "train-nll= 0.010684227\n",
      "train-nll= 0.009847026\n",
      "train-nll= 0.010032969\n",
      "nll=0.20657234    ppl=1.2294567    words=70390    time=16463.945868730545    word_per_sec=41777.00810510707\n",
      "train-nll= 0.008901614\n",
      "train-nll= 0.008197687\n",
      "train-nll= 0.008094201\n",
      "train-nll= 0.007825889\n",
      "train-nll= 0.007370708\n",
      "train-nll= 0.007091399\n",
      "train-nll= 0.0064196037\n",
      "train-nll= 0.0068307933\n",
      "train-nll= 0.0066239587\n",
      "train-nll= 0.0062324004\n",
      "nll=0.19256355    ppl=1.2123536    words=70390    time=16622.043623685837    word_per_sec=41777.53444290473\n",
      "train-nll= 0.005553765\n",
      "train-nll= 0.0056403405\n",
      "train-nll= 0.005545292\n",
      "train-nll= 0.004970844\n",
      "train-nll= 0.0051599354\n",
      "train-nll= 0.00495454\n",
      "train-nll= 0.0051371763\n",
      "train-nll= 0.005150791\n",
      "train-nll= 0.005632066\n",
      "train-nll= 0.004096994\n",
      "nll=0.18585049    ppl=1.2042422    words=70390    time=16779.717171669006    word_per_sec=41779.10705096053\n",
      "train-nll= 0.0028625915\n",
      "train-nll= 0.00196513\n",
      "train-nll= 0.0013373855\n",
      "train-nll= 0.0008944353\n",
      "train-nll= 0.0008020438\n",
      "train-nll= 0.00085535174\n",
      "train-nll= 0.0011611085\n",
      "train-nll= 0.0012480839\n",
      "train-nll= 0.001475205\n",
      "train-nll= 0.0017623625\n",
      "nll=0.1860914    ppl=1.2045324    words=70390    time=16937.262694835663    word_per_sec=41780.96618975928\n",
      "train-nll= 0.0018136699\n",
      "train-nll= 0.0018703907\n",
      "train-nll= 0.0019895541\n",
      "train-nll= 0.0021116333\n",
      "train-nll= 0.0023036615\n",
      "train-nll= 0.0023805168\n",
      "train-nll= 0.0029350098\n",
      "train-nll= 0.0029360189\n",
      "train-nll= 0.0024412463\n",
      "train-nll= 0.002333733\n",
      "nll=0.18419205    ppl=1.2022467    words=70390    time=17094.685206651688    word_per_sec=41783.09172502761\n",
      "train-nll= 0.0022625255\n",
      "train-nll= 0.0019617253\n",
      "train-nll= 0.002037019\n",
      "train-nll= 0.0020784223\n",
      "train-nll= 0.0020634762\n",
      "train-nll= 0.0019494692\n",
      "train-nll= 0.0019253372\n",
      "train-nll= 0.0019685694\n",
      "train-nll= 0.0018817079\n",
      "train-nll= 0.0017901523\n",
      "nll=0.1753937    ppl=1.1917152    words=70390    time=17252.118832588196    word_per_sec=41785.15155125742\n",
      "train-nll= 0.0018366912\n",
      "train-nll= 0.0016263118\n",
      "train-nll= 0.0016377452\n",
      "train-nll= 0.0017177344\n",
      "train-nll= 0.0017295823\n",
      "train-nll= 0.0017473167\n",
      "train-nll= 0.001559463\n",
      "train-nll= 0.0016039766\n",
      "train-nll= 0.0015682434\n",
      "train-nll= 0.0015345862\n",
      "nll=0.17087512    ppl=1.1863426    words=70390    time=17411.693506479263    word_per_sec=41782.03571808125\n",
      "train-nll= 0.0014434351\n",
      "train-nll= 0.001474471\n",
      "train-nll= 0.0015143246\n",
      "train-nll= 0.0016442579\n",
      "train-nll= 0.0015874613\n",
      "train-nll= 0.0015177021\n",
      "train-nll= 0.0015414008\n",
      "train-nll= 0.0014642017\n",
      "train-nll= 0.0015637116\n",
      "train-nll= 0.0016666445\n",
      "nll=0.16820194    ppl=1.1831756    words=70390    time=17571.199160575867    word_per_sec=41779.140586324145\n",
      "train-nll= 0.0015883069\n",
      "train-nll= 0.0014775879\n",
      "train-nll= 0.0013521805\n",
      "train-nll= 0.0012969121\n",
      "train-nll= 0.0012972696\n",
      "train-nll= 0.0021896455\n",
      "train-nll= 0.0021221777\n",
      "train-nll= 0.0017583282\n",
      "train-nll= 0.0014179078\n",
      "train-nll= 0.0011824049\n",
      "nll=0.16323581    ppl=1.1773143    words=70390    time=17728.21520972252    word_per_sec=41782.16426399044\n",
      "train-nll= 0.0010831868\n",
      "train-nll= 0.0010279613\n",
      "train-nll= 0.0011211423\n",
      "train-nll= 0.0011296648\n",
      "train-nll= 0.0010552203\n",
      "train-nll= 0.0008335005\n",
      "train-nll= 0.0005515392\n",
      "train-nll= 0.0004608892\n",
      "train-nll= 0.0003484222\n",
      "train-nll= 0.0002839111\n",
      "nll=0.15925133    ppl=1.1726327    words=70390    time=17885.847997665405    word_per_sec=41783.694018731905\n",
      "train-nll= 0.00024321504\n",
      "train-nll= 0.00023584731\n",
      "train-nll= 0.00024675354\n",
      "train-nll= 0.00025542293\n",
      "train-nll= 0.00025291627\n",
      "train-nll= 0.0002416359\n",
      "train-nll= 0.00027053506\n",
      "train-nll= 0.00030271086\n",
      "train-nll= 0.00029914465\n",
      "train-nll= 0.00031350873\n",
      "nll=0.15819284    ppl=1.1713921    words=70390    time=18043.113199710846    word_per_sec=41786.04831964822\n",
      "train-nll= 0.00036510205\n",
      "train-nll= 0.0003903244\n",
      "train-nll= 0.00043640615\n",
      "train-nll= 0.0005574285\n",
      "train-nll= 0.000555955\n",
      "train-nll= 0.0005383726\n",
      "train-nll= 0.00052698184\n",
      "train-nll= 0.00046793994\n",
      "train-nll= 0.0004588239\n",
      "train-nll= 0.00045879016\n",
      "nll=0.15985623    ppl=1.1733422    words=70390    time=18201.15139389038    word_per_sec=41786.58720762579\n",
      "train-nll= 0.00042622516\n",
      "train-nll= 0.0004176626\n",
      "train-nll= 0.0004196494\n",
      "train-nll= 0.00043189584\n",
      "train-nll= 0.0004410773\n",
      "train-nll= 0.00043466216\n",
      "train-nll= 0.0004233827\n",
      "train-nll= 0.00040598708\n",
      "train-nll= 0.00038453072\n",
      "train-nll= 0.00037856124\n",
      "nll=0.15490855    ppl=1.1675512    words=70390    time=18358.335399866104    word_per_sec=41789.06111529019\n",
      "train-nll= 0.0003720985\n",
      "train-nll= 0.00037353206\n",
      "train-nll= 0.00038935395\n",
      "train-nll= 0.00039558113\n",
      "train-nll= 0.0003850778\n",
      "train-nll= 0.0003901933\n",
      "train-nll= 0.0004756196\n",
      "train-nll= 0.0005197742\n",
      "train-nll= 0.00048337036\n",
      "train-nll= 0.00043334454\n",
      "nll=0.15556544    ppl=1.1683184    words=70390    time=18516.923050880432    word_per_sec=41788.325083697324\n",
      "train-nll= 0.00042323497\n",
      "train-nll= 0.00040978513\n",
      "train-nll= 0.0004022486\n",
      "train-nll= 0.00038646074\n",
      "train-nll= 0.00039833388\n",
      "train-nll= 0.00037882518\n",
      "train-nll= 0.00037632827\n",
      "train-nll= 0.00038311808\n",
      "train-nll= 0.00037640924\n",
      "train-nll= 0.0003645902\n",
      "nll=0.1515698    ppl=1.1636596    words=70390    time=18675.067853927612    word_per_sec=41788.592475495105\n",
      "train-nll= 0.0003592505\n",
      "train-nll= 0.00033952884\n",
      "train-nll= 0.00032004906\n",
      "train-nll= 0.00031392093\n",
      "train-nll= 0.00030765645\n",
      "train-nll= 0.00029366612\n",
      "train-nll= 0.00033827507\n",
      "train-nll= 0.0003352721\n",
      "train-nll= 0.00030669363\n",
      "train-nll= 0.0002875844\n",
      "nll=0.15001875    ppl=1.161856    words=70390    time=18832.052803993225    word_per_sec=41791.42912307029\n",
      "train-nll= 0.00028071253\n",
      "train-nll= 0.00025286627\n",
      "train-nll= 0.00021724544\n",
      "train-nll= 0.0001808767\n",
      "train-nll= 0.0001491825\n",
      "train-nll= 0.00013258067\n",
      "train-nll= 0.000120235716\n",
      "train-nll= 0.00011527939\n",
      "train-nll= 0.00011403048\n",
      "train-nll= 0.00011422302\n",
      "nll=0.14559197    ppl=1.1567241    words=70390    time=18989.542985916138    word_per_sec=41793.10690039294\n",
      "train-nll= 0.00011482088\n",
      "train-nll= 0.000113294554\n",
      "train-nll= 0.00011297599\n",
      "train-nll= 0.00011378081\n",
      "train-nll= 0.00011360772\n",
      "train-nll= 0.00011266689\n",
      "train-nll= 0.00011248423\n",
      "train-nll= 0.00011294096\n",
      "train-nll= 0.000116196556\n",
      "train-nll= 0.00011906916\n",
      "nll=0.1452659    ppl=1.156347    words=70390    time=19147.500069856644    word_per_sec=41793.73793343412\n",
      "train-nll= 0.000119543554\n",
      "train-nll= 0.00012734535\n",
      "train-nll= 0.00012854322\n",
      "train-nll= 0.00012636602\n",
      "train-nll= 0.00012694734\n",
      "train-nll= 0.00012987758\n",
      "train-nll= 0.00012958869\n",
      "train-nll= 0.00012829741\n",
      "train-nll= 0.00012753424\n",
      "train-nll= 0.00012742839\n",
      "nll=0.14517573    ppl=1.1562427    words=70390    time=19305.3455388546    word_per_sec=41794.60027669992\n",
      "train-nll= 0.0001353301\n",
      "train-nll= 0.00014307027\n",
      "train-nll= 0.00014772729\n",
      "train-nll= 0.00014348119\n",
      "train-nll= 0.00014166052\n",
      "train-nll= 0.00014450181\n",
      "train-nll= 0.00014772627\n",
      "train-nll= 0.00014275614\n",
      "train-nll= 0.00021610841\n",
      "train-nll= 0.00023628473\n",
      "nll=0.14878501    ppl=1.1604235    words=70390    time=19463.729494810104    word_per_sec=41794.292312627345\n",
      "train-nll= 0.00020741308\n",
      "train-nll= 0.00019783882\n",
      "train-nll= 0.00017838765\n",
      "train-nll= 0.00021541757\n",
      "train-nll= 0.00018385965\n",
      "train-nll= 0.00019704671\n",
      "train-nll= 0.00020381845\n",
      "train-nll= 0.00021411652\n",
      "train-nll= 0.00020667889\n",
      "train-nll= 0.00021726152\n",
      "nll=0.14653498    ppl=1.1578155    words=70390    time=19623.51544380188    word_per_sec=41791.00336780002\n",
      "train-nll= 0.00020583672\n",
      "train-nll= 0.0001911132\n",
      "train-nll= 0.00018480912\n",
      "train-nll= 0.00022843867\n",
      "train-nll= 0.00019730451\n",
      "train-nll= 0.0001983595\n",
      "train-nll= 0.00019129485\n",
      "train-nll= 0.00018573001\n",
      "train-nll= 0.00018116583\n",
      "train-nll= 0.00019543157\n",
      "nll=0.1431001    ppl=1.1538453    words=70390    time=19780.528060674667    word_per_sec=41793.62641200405\n",
      "train-nll= 0.00017494093\n",
      "train-nll= 0.00016590882\n",
      "train-nll= 0.00015870317\n",
      "train-nll= 0.00015476673\n",
      "train-nll= 0.00015172295\n",
      "train-nll= 0.00014927422\n",
      "train-nll= 0.00014642414\n",
      "train-nll= 0.0001436987\n",
      "train-nll= 0.00013630674\n",
      "train-nll= 0.00012272947\n",
      "nll=0.14166003    ppl=1.1521848    words=70390    time=19938.058626651764    word_per_sec=41795.122363923954\n",
      "train-nll= 0.00011401488\n",
      "train-nll= 8.769293e-5\n",
      "train-nll= 8.6344844e-5\n",
      "train-nll= 8.61309e-5\n",
      "train-nll= 8.478955e-5\n",
      "train-nll= 8.344057e-5\n",
      "train-nll= 8.295788e-5\n",
      "train-nll= 8.314731e-5\n",
      "train-nll= 8.198564e-5\n",
      "train-nll= 8.195955e-5\n",
      "nll=0.14060271    ppl=1.1509672    words=70390    time=20095.925731658936    word_per_sec=41795.89491002081\n",
      "train-nll= 8.1502716e-5\n",
      "train-nll= 8.07638e-5\n",
      "train-nll= 8.024444e-5\n",
      "train-nll= 8.012313e-5\n",
      "train-nll= 8.0075835e-5\n",
      "train-nll= 8.035903e-5\n",
      "train-nll= 8.0970785e-5\n",
      "train-nll= 8.10762e-5\n",
      "train-nll= 8.060228e-5\n",
      "train-nll= 8.0520724e-5\n",
      "nll=0.14035952    ppl=1.1506875    words=70390    time=20253.09931755066    word_per_sec=41798.086639826826\n",
      "train-nll= 8.073746e-5\n",
      "train-nll= 0.000100726815\n",
      "train-nll= 0.0001235807\n",
      "train-nll= 0.000112792404\n",
      "train-nll= 0.00010540432\n",
      "train-nll= 0.000101378224\n",
      "train-nll= 9.932797e-5\n",
      "train-nll= 9.811002e-5\n",
      "train-nll= 9.664407e-5\n",
      "train-nll= 9.54488e-5\n",
      "nll=0.14112669    ppl=1.1515706    words=70390    time=20410.776275634766    word_per_sec=41799.21373291655\n",
      "train-nll= 9.465518e-5\n",
      "train-nll= 9.4973126e-5\n",
      "train-nll= 9.6698735e-5\n",
      "train-nll= 9.8642e-5\n",
      "train-nll= 9.484549e-5\n",
      "train-nll= 9.395783e-5\n",
      "train-nll= 9.825562e-5\n",
      "train-nll= 9.9980905e-5\n",
      "train-nll= 9.979157e-5\n",
      "train-nll= 9.912525e-5\n",
      "nll=0.14181705    ppl=1.1523658    words=70390    time=20570.5289747715    word_per_sec=41796.105537901\n",
      "train-nll= 9.879978e-5\n",
      "train-nll= 9.806115e-5\n",
      "train-nll= 9.72953e-5\n",
      "train-nll= 9.337391e-5\n",
      "train-nll= 9.2246155e-5\n",
      "train-nll= 9.1424285e-5\n",
      "train-nll= 9.059952e-5\n",
      "train-nll= 9.00406e-5\n",
      "train-nll= 8.8902205e-5\n",
      "train-nll= 8.8262954e-5\n",
      "nll=0.13989332    ppl=1.1501511    words=70390    time=20729.50787973404    word_per_sec=41794.60530498208\n",
      "train-nll= 8.756017e-5\n",
      "train-nll= 8.7040506e-5\n",
      "train-nll= 8.6706554e-5\n",
      "train-nll= 8.607317e-5\n",
      "train-nll= 8.527602e-5\n",
      "train-nll= 8.650626e-5\n",
      "train-nll= 9.014689e-5\n",
      "train-nll= 8.814067e-5\n",
      "train-nll= 8.541506e-5\n",
      "train-nll= 8.441894e-5\n",
      "nll=0.13910487    ppl=1.1492447    words=70390    time=20886.74596476555    word_per_sec=41796.6111845608\n",
      "train-nll= 8.282724e-5\n",
      "train-nll= 8.112701e-5\n",
      "train-nll= 7.969812e-5\n",
      "train-nll= 7.858697e-5\n",
      "train-nll= 7.592864e-5\n",
      "train-nll= 7.1933995e-5\n",
      "train-nll= 6.7170535e-5\n",
      "train-nll= 6.141044e-5\n",
      "train-nll= 6.0185102e-5\n",
      "train-nll= 6.1075545e-5\n",
      "nll=0.13824321    ppl=1.1482548    words=70390    time=21044.458253860474    word_per_sec=41797.645222757936\n",
      "train-nll= 5.9642043e-5\n",
      "train-nll= 5.785805e-5\n",
      "train-nll= 5.659987e-5\n",
      "train-nll= 5.6222107e-5\n",
      "train-nll= 5.608527e-5\n",
      "train-nll= 5.6156678e-5\n",
      "train-nll= 5.6208173e-5\n",
      "train-nll= 5.622163e-5\n",
      "train-nll= 5.6253535e-5\n",
      "train-nll= 5.6256675e-5\n",
      "nll=0.13767226    ppl=1.1475993    words=70390    time=21202.282819747925    word_per_sec=41798.44253254501\n",
      "train-nll= 5.6440716e-5\n",
      "train-nll= 5.6744153e-5\n",
      "train-nll= 5.7494883e-5\n",
      "train-nll= 5.7849942e-5\n",
      "train-nll= 5.7881272e-5\n",
      "train-nll= 5.7862828e-5\n",
      "train-nll= 5.7886158e-5\n",
      "train-nll= 5.8032554e-5\n",
      "train-nll= 5.821631e-5\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] pointer(::KnetArray{Int32,1}) at /root/.julia/packages/Knet/LjPts/src/karray.jl:125",
      " [2] unsafe_convert(::Type{Ptr{Int64}}, ::KnetArray{Int32,1}) at /root/.julia/packages/Knet/LjPts/src/karray.jl:124",
      " [3] getindex(::KnetArray{Float32,2}, ::SubArray{Int64,1,Array{Int64,1},Tuple{UnitRange{Int64}},true}) at /root/.julia/packages/Knet/LjPts/src/karray.jl:544",
      " [4] #nll#754(::Int64, ::Bool, ::typeof(nll), ::KnetArray{Float32,2}, ::Array{Int64,2}) at /root/.julia/packages/Knet/LjPts/src/loss.jl:245",
      " [5] (::getfield(Knet, Symbol(\"#kw##nll\")))(::NamedTuple{(:average,),Tuple{Bool}}, ::typeof(nll), ::KnetArray{Float32,2}, ::Array{Int64,2}) at ./none:0",
      " [6] #loss_f#15(::Bool, ::typeof(loss_f), ::RNN_model, ::Array{Int64,2}) at ./In[1]:263",
      " [7] (::getfield(Main, Symbol(\"#kw##loss_f\")))(::NamedTuple{(:average,),Tuple{Bool}}, ::typeof(loss_f), ::RNN_model, ::Array{Int64,2}) at ./none:0",
      " [8] #maploss#16(::Bool, ::typeof(maploss), ::Function, ::RNN_model, ::Array{Array{Int64,2},1}) at ./In[1]:270",
      " [9] (::getfield(Main, Symbol(\"#kw##maploss\")))(::NamedTuple{(:average,),Tuple{Bool}}, ::typeof(maploss), ::Function, ::RNN_model, ::Array{Array{Int64,2},1}) at ./none:0",
      " [10] top-level scope at In[2]:13"
     ]
    }
   ],
   "source": [
    "#progress(ncycle(epoch, 100), seconds=5) do x\n",
    "#for ep =1:100 \n",
    "j =0\n",
    "start = time()\n",
    "dev_time = 0\n",
    "println(\"Start Time=\",start)\n",
    "all_tagged = 0\n",
    "for i in ncycle(epoch,100) \n",
    "    j += 1\n",
    "    global bestmodel, bestloss\n",
    "    ## Report gradient norm for the first batch\n",
    "    f = @diff loss_f(model,train_batches[1])\n",
    "    gnorm = sqrt(sum(norm(grad(f,x))^2 for x in params(model)))\n",
    "    ## Report training and validation loss\n",
    "    trnloss,this_words = maploss(loss_f,model, train_batches50,average = false)\n",
    "    trnloss= trnloss/this_words\n",
    "    all_tagged+=this_words\n",
    "    if(j%10 == 0)\n",
    "      println(\"train-nll= \",trnloss)\n",
    "      end\n",
    "    if (j%100==0)\n",
    "      dev_start = time()\n",
    "      devloss,words  = maploss(loss_f,model, test_batches,average = false )\n",
    "      ## Save model that does best on validation data\n",
    "      if devloss < bestloss\n",
    "          bestmodel, bestloss = deepcopy(model), devloss\n",
    "      end\n",
    "      dev_time += time() - dev_start\n",
    "\n",
    "      train_time = time() - start - dev_time\n",
    "\n",
    "      println(\"nll=\",devloss/words,\"    ppl=\", exp(devloss/words),\"    words=\",words, \"    time=\",train_time,\"    word_per_sec=\",all_tagged/train_time)\n",
    "    end\n",
    "    \n",
    "end\n",
    "Knet.save(\"lm-lstm.jld2\", \"model\", bestmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "Binding \\texttt{accuracy} does not exist.\n",
       "\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "Binding `accuracy` does not exist.\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  Binding \u001b[36maccuracy\u001b[39m does not exist."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JuliaOnColab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
