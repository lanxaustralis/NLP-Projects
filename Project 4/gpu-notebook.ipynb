{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/ec2-user/.julia/compiled/v1.2/Knet/f4vSz.ji for Knet [1902f260-5fb4-5aff-8c31-6271790ab950]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m NNlib → `~/.julia/packages/NNlib/Nksco/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "Package name: AutoGrad Version: 1.2.0\n",
      "Package name: IterTools Version: 1.3.0\n",
      "Package name: StatsBase Version: 0.32.0\n",
      "Package name: Knet Version: 1.3.2\n",
      "Package name: CuArrays Version: 1.5.0\n",
      "Package name: IJulia Version: 1.20.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#jl Use `Literate.notebook(juliafile, \".\", execute=false)` to convert to notebook.\n",
    "\n",
    "# # Attention-based Neural Machine Translation\n",
    "#\n",
    "# **Reference:** Luong, Thang, Hieu Pham and Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1412-1421. 2015.\n",
    "#\n",
    "# * https://www.aclweb.org/anthology/D15-1166/ (main paper reference)\n",
    "# * https://arxiv.org/abs/1508.04025 (alternative paper url)\n",
    "# * https://github.com/tensorflow/nmt (main code reference)\n",
    "# * https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention (alternative code reference)\n",
    "# * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py:2449,2103 (attention implementation)\n",
    "\n",
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, Random, IterTools#, CuArrays\n",
    "\n",
    "import Pkg\n",
    "\n",
    "#GPU related\n",
    "Pkg.add(\"CuArrays\")\n",
    "Pkg.build(\"CuArrays\")\n",
    "\n",
    "using CuArrays: CuArrays, usage_limit\n",
    "\n",
    "Pkg.update()\n",
    "pkgs = Pkg.installed()\n",
    "\n",
    "for package in keys(pkgs)\n",
    "    if pkgs[package] == nothing\n",
    "        pkgs[package] = VersionNumber(\"0.0.1\")\n",
    "    end\n",
    "    println(\"Package name: \", package, \" Version: \", pkgs[package])\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 64\n",
    "Knet.atype() = KnetArray{Float32}\n",
    "\n",
    "#CuArrays.usage_limit[] = 8_000_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bleu (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Code and data from previous projects\n",
    "#\n",
    "# Please copy or include the following types and related functions from previous projects:\n",
    "# `Vocab`, `TextReader`, `MTData`, `Embed`, `Linear`, `mask!`, `loss`, `int2str`,\n",
    "# `bleu`.\n",
    "\n",
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "function Vocab(\n",
    "    file::String;\n",
    "    tokenizer = split,\n",
    "    vocabsize = Inf,\n",
    "    mincount = 1,\n",
    "    unk = \"<unk>\",\n",
    "    eos = \"<s>\",\n",
    ")\n",
    "    vocab_freq = Dict{String,Int64}(unk => 1, eos => 1)\n",
    "    w2i = Dict{String,Int64}(unk => 2, eos => 1)\n",
    "    i2w = Vector{String}()\n",
    "\n",
    "    push!(i2w, eos)\n",
    "    push!(i2w, unk)\n",
    "\n",
    "    open(file) do f\n",
    "        for line in eachline(f)\n",
    "            sentence = strip(lowercase(line))\n",
    "            sentence = tokenizer(line, [' '], keepempty = false)\n",
    "\n",
    "            for word in sentence\n",
    "                word == unk && continue\n",
    "                word == eos && continue # They are default ones to be added later\n",
    "                vocab_freq[word] = get!(vocab_freq, word, 0) + 1\n",
    "            end\n",
    "        end\n",
    "        close(f)\n",
    "    end\n",
    "\n",
    "\n",
    "# End of vanilla implementation of the vocaulary\n",
    "# From here we must add the mincount and vocabsize properties\n",
    "# We must change the first two property of the vocab wrt those paramaters\n",
    "    vocab_freq = sort!(\n",
    "        collect(vocab_freq),\n",
    "        by = tuple -> last(tuple),\n",
    "        rev = true,\n",
    "    )\n",
    "\n",
    "    if length(vocab_freq) > vocabsize - 2 # eos and unk ones\n",
    "        vocab_freq = vocab_freq[1:vocabsize-2] # trim to fit the size\n",
    "    end\n",
    "\n",
    "#vocab_freq = reverse(vocab_freq)\n",
    "\n",
    "    while true\n",
    "        length(vocab_freq) == 0 && break\n",
    "        word, freq = vocab_freq[end]\n",
    "        freq >= mincount && break # since it is already ordered\n",
    "        vocab_freq = vocab_freq[1:(end-1)]\n",
    "    end\n",
    "#pushfirst!(vocab_freq,unk=>1,eos=>1) # freq does not matter, just adding the\n",
    "    for i = 1:length(vocab_freq)\n",
    "        word, freq = vocab_freq[i]\n",
    "        ind = (get!(w2i, word, 1 + length(w2i)))\n",
    "        (length(i2w) < ind) && push!(i2w, word)\n",
    "    end\n",
    "\n",
    "    return Vocab(w2i, i2w, 2, 1, tokenizer)\n",
    "end\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "word2ind(dict, x) = get(dict, x, 2) # unk is 2\n",
    "\n",
    "#Implementing the iterate function\n",
    "function Base.iterate(r::TextReader, s = nothing)\n",
    "    if s == nothing\n",
    "        state = open(r.file)\n",
    "        Base.iterate(r, state)\n",
    "    else\n",
    "        if eof(s) == true\n",
    "            close(s)\n",
    "            return nothing\n",
    "        else\n",
    "            line = readline(s)\n",
    "            line = strip(lowercase(line))\n",
    "            sent = r.vocab.tokenizer(line, [' '], keepempty = false)\n",
    "            sent_ind = Int[]\n",
    "            for word in sent\n",
    "                ind = word2ind(r.vocab.w2i, word)\n",
    "                push!(sent_ind, ind)\n",
    "            end\n",
    "            return (sent_ind, s)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "#Embed\n",
    "struct Embed\n",
    "    w\n",
    "end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    Embed(param(embedsize, vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    l.w[:, x]\n",
    "end\n",
    "\n",
    "#Linear\n",
    "struct Linear\n",
    "    w\n",
    "    b\n",
    "end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    Linear(param(outputsize, inputsize), param0(outputsize))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    l.w * mat(x, dims = 1) .+ l.b\n",
    "end\n",
    "\n",
    "# Mask!\n",
    "function mask!(a, pad)\n",
    "    matr = a\n",
    "    for j = 1:size(matr)[1]\n",
    "        i = 0\n",
    "        while (i < length(matr[j, :]) - 1)\n",
    "            if matr[j, length(matr[j, :])-i-1] != pad\n",
    "                break\n",
    "\n",
    "            elseif matr[j, length(matr[j, :])-i] == pad\n",
    "                matr[j, length(matr[j, :])-i] = 0\n",
    "            end\n",
    "            i += 1\n",
    "        end\n",
    "    end\n",
    "    return matr\n",
    "end\n",
    "\n",
    "struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(\n",
    "    src::TextReader,\n",
    "    tgt::TextReader;\n",
    "    batchmaker = arraybatch,\n",
    "    batchsize = BATCH_SIZE,\n",
    "    maxlength = typemax(Int),\n",
    "    batchmajor = false,\n",
    "    bucketwidth = 10,\n",
    "    numbuckets = min(BATCH_SIZE, maxlength ÷ bucketwidth),\n",
    ")\n",
    "    buckets = [[] for i = 1:numbuckets] # buckets[i] is an array of sentence pairs with similar length\n",
    "    MTData(\n",
    "        src,\n",
    "        tgt,\n",
    "        batchsize,\n",
    "        maxlength,\n",
    "        batchmajor,\n",
    "        bucketwidth,\n",
    "        buckets,\n",
    "        batchmaker,\n",
    "    )\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::MTData, state = nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets\n",
    "            empty!(b)\n",
    "        end\n",
    "        state_src, state_tgt = nothing, nothing\n",
    "    else\n",
    "        state_src, state_tgt = state\n",
    "    end\n",
    "    bucket, ibucket = nothing, nothing\n",
    "\n",
    "\n",
    "    while true\n",
    "        iter_src = (state_src === nothing ? iterate(d.src) :\n",
    "                    iterate(d.src, state_src))\n",
    "        iter_tgt = (state_tgt === nothing ? iterate(d.tgt) :\n",
    "                    iterate(d.tgt, state_tgt))\n",
    "\n",
    "        if iter_src === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent_src, state_src = iter_src\n",
    "            sent_tgt, state_tgt = iter_tgt\n",
    "            if length(sent_src) > d.maxlength || length(sent_src) == 0\n",
    "                continue\n",
    "            end\n",
    "            ibucket = min(\n",
    "                1 + (length(sent_src) - 1) ÷ d.bucketwidth,\n",
    "                length(d.buckets),\n",
    "            )\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, (sent_src, sent_tgt))\n",
    "            if length(bucket) === d.batchsize\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    batch = d.batchmaker(d, bucket)\n",
    "\n",
    "    empty!(bucket)\n",
    "    return batch, (state_src, state_tgt)\n",
    "end\n",
    "\n",
    "\n",
    "function arraybatch(d::MTData, bucket)\n",
    "    bucketx = map(x -> x[1], bucket)\n",
    "    buckety = map(x -> x[2], bucket)\n",
    "    batch_x = fill(d.src.vocab.eos, length(bucketx), maximum(length.(bucketx)))\n",
    "    for i = 1:length(bucket)\n",
    "        batch_x[i, end-length(bucketx[i])+1:end] = bucketx[i]\n",
    "    end\n",
    "    batch_y = fill(\n",
    "        d.tgt.vocab.eos,\n",
    "        length(buckety),\n",
    "        maximum(length.(buckety)) + 2,\n",
    "    )\n",
    "    for i = 1:length(bucket)\n",
    "        batch_y[i, 2:length(buckety[i])+1] = buckety[i]\n",
    "    end\n",
    "\n",
    "    return (batch_x, batch_y)\n",
    "end\n",
    "\n",
    "function loss(model, data; average = true)\n",
    "    total_loss = 0\n",
    "    total_word = 0\n",
    "\n",
    "    for (x, y) in collect(data)\n",
    "        curr_loss, curr_word = model(x, y; average = false)\n",
    "        total_loss += curr_loss\n",
    "        total_word += curr_word\n",
    "    end\n",
    "\n",
    "    average && return total_loss / total_word\n",
    "    return (total_loss, total_word)\n",
    "\n",
    "end\n",
    "function int2str(y, vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w -> !isequal(w, vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1 + length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end\n",
    "function bleu(s2s, d::MTData)\n",
    "    d = MTData(d.src, d.tgt, batchsize = 1)\n",
    "    reffile = d.tgt.file\n",
    "    hypfile, hyp = mktemp()\n",
    "    for (x, y) in progress(collect(d))\n",
    "        g = s2s(x)\n",
    "        for i = 1:size(y, 1)\n",
    "            println(hyp, int2str(g[i, :], d.tgt.vocab))\n",
    "        end\n",
    "    end\n",
    "    close(hyp)\n",
    "    isfile(\"multi-bleu.perl\") || download(\n",
    "        \"https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\",\n",
    "        \"multi-bleu.perl\",\n",
    "    )\n",
    "    run(pipeline(`cat $hypfile`, `perl multi-bleu.perl $reffile`))\n",
    "    return hypfile\n",
    "end\n",
    "\n",
    "## Pre Assigment Part Completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## S2S: Sequence to sequence model with attention\n",
    "#\n",
    "# In this project we will define, train and evaluate a sequence to sequence encoder-decoder\n",
    "# model with attention for Turkish-English machine translation. The model has two extra\n",
    "# fields compared to `S2S_v1`: the `memory` layer computes keys and values from the encoder,\n",
    "# the `attention` layer computes the attention vector for the decoder.\n",
    "\n",
    "struct Memory\n",
    "    w\n",
    "end\n",
    "\n",
    "struct Attention\n",
    "    wquery\n",
    "    wattn\n",
    "    scale\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading reference model\n",
      "└ @ Main In[5]:21\n",
      "┌ Info: Reading data\n",
      "└ @ Main In[5]:40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MTData(TextReader(\"datasets/tr_to_en/tr.test\", Vocab(Dict(\"dev\" => 1277,\"komuta\" => 13566,\"ellisi\" => 25239,\"adresini\" => 22820,\"yüzeyi\" => 4051,\"paris'te\" => 9494,\"kafamdaki\" => 18790,\"yüzeyinde\" => 5042,\"geçerlidir\" => 6612,\"kökten\" => 7774…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"seçmemiz\", \"destekleyip\", \"karşılaştırılabilir\", \"ördeğin\", \"gününüzü\", \"bağışçı\", \"istismara\", \"yaşça\", \"tedci\", \"fakültesi'nde\"], 2, 1, split)), TextReader(\"datasets/tr_to_en/en.test\", Vocab(Dict(\"middle-income\" => 13398,\"photosynthesis\" => 7689,\"polarizing\" => 17881,\"henry\" => 4248,\"abducted\" => 15691,\"rises\" => 6225,\"hampshire\" => 13888,\"whiz\" => 16835,\"cost-benefit\" => 13137,\"progression\" => 5549…), [\"<s>\", \"<unk>\", \",\", \".\", \"the\", \"and\", \"to\", \"of\", \"a\", \"that\"  …  \"archaea\", \"handshake\", \"brit\", \"wiper\", \"heroines\", \"coca\", \"exceptionally\", \"gallbladder\", \"autopsies\", \"linguistics\"], 2, 1, split)), 64, 9223372036854775807, false, 10, Array{Any,1}[[], [], [], [], [], [], [], [], [], []  …  [], [], [], [], [], [], [], [], [], []], arraybatch)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct S2S\n",
    "    srcembed::Embed       # encinput(B,Tx) -> srcembed(Ex,B,Tx)\n",
    "    encoder::RNN          # srcembed(Ex,B,Tx) -> enccell(Dx*H,B,Tx)\n",
    "    memory::Memory        # enccell(Dx*H,B,Tx) -> keys(H,Tx,B), vals(Dx*H,Tx,B)\n",
    "    tgtembed::Embed       # decinput(B,Ty) -> tgtembed(Ey,B,Ty)\n",
    "    decoder::RNN          # tgtembed(Ey,B,Ty) . attnvec(H,B,Ty)[t-1] = (Ey+H,B,Ty) -> deccell(H,B,Ty)\n",
    "    attention::Attention  # deccell(H,B,Ty), keys(H,Tx,B), vals(Dx*H,Tx,B) -> attnvec(H,B,Ty)\n",
    "    projection::Linear    # attnvec(H,B,Ty) -> proj(Vy,B,Ty)\n",
    "    dropout::Real         # dropout probability\n",
    "    srcvocab::Vocab       # source language vocabulary\n",
    "    tgtvocab::Vocab       # target language vocabulary\n",
    "end\n",
    "\n",
    "\n",
    "# ## Load pretrained model and data\n",
    "#\n",
    "# We will load a pretrained model (16.20 bleu) for code testing.  The data should be loaded\n",
    "# with the vocabulary from the pretrained model for word id consistency.\n",
    "\n",
    "if !isdefined(Main, :pretrained) || pretrained === nothing\n",
    "    @info \"Loading reference model\"\n",
    "    isfile(\"s2smodel.jld2\") || download(\n",
    "        \"http://people.csail.mit.edu/deniz/comp542/s2smodel.jld2\",\n",
    "        \"s2smodel.jld2\",\n",
    "    )\n",
    "    pretrained = Knet.load(\"s2smodel.jld2\", \"model\")\n",
    "end\n",
    "datadir = \"datasets/tr_to_en\"\n",
    "if !isdir(datadir)\n",
    "    @info \"Downloading data\"\n",
    "    download(\n",
    "        \"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\",\n",
    "        \"qi18naacl-dataset.tar.gz\",\n",
    "    )\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    BATCHSIZE, MAXLENGTH = 64, 50\n",
    "    @info \"Reading data\"\n",
    "    tr_vocab = pretrained.srcvocab # Vocab(\"$datadir/tr.train\", mincount=5)\n",
    "    en_vocab = pretrained.tgtvocab # Vocab(\"$datadir/en.train\", mincount=5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    dtrn = MTData(\n",
    "        tr_train,\n",
    "        en_train,\n",
    "        batchsize = BATCHSIZE,\n",
    "        maxlength = MAXLENGTH,\n",
    "    )\n",
    "    ddev = MTData(tr_dev, en_dev, batchsize = BATCHSIZE)\n",
    "    dtst = MTData(tr_test, en_test, batchsize = BATCHSIZE)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:           | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing S2S constructor | \u001b[32m  16  \u001b[39m\u001b[36m   16\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing S2S constructor\", Any[], 16, false)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 1. Model constructor\n",
    "#\n",
    "# The `S2S` constructor takes the following arguments:\n",
    "# * `hidden`: size of the hidden vectors for both the encoder and the decoder\n",
    "# * `srcembsz`, `tgtembsz`: size of the source/target language embedding vectors\n",
    "# * `srcvocab`, `tgtvocab`: the source/target language vocabulary\n",
    "# * `layers=1`: number of layers\n",
    "# * `bidirectional=false`: whether the encoder is bidirectional\n",
    "# * `dropout=0`: dropout probability\n",
    "#\n",
    "# Hints:\n",
    "# * You can find the vocabulary size with `length(vocab.i2w)`.\n",
    "# * If the encoder is bidirectional `layers` must be even and the encoder should have `layers÷2` layers.\n",
    "# * The decoder will use \"input feeding\", i.e. it will concatenate its previous output to its input. Therefore the input size for the decoder should be `tgtembsz+hidden`.\n",
    "# * Only `numLayers`, `dropout`, and `bidirectional` keyword arguments should be used for RNNs, leave everything else default.\n",
    "# * The memory parameter `w` is used to convert encoder states to keys. If the encoder is bidirectional initialize it to a `(hidden,2*hidden)` parameter, otherwise set it to the constant 1.\n",
    "# * The attention parameter `wquery` is used to transform the query, set it to the constant 1 for this project.\n",
    "# * The attention parameter `scale` is used to scale the attention scores before softmax, set it to a parameter of size 1.\n",
    "# * The attention parameter `wattn` is used to transform the concatenation of the decoder output and the context vector to the attention vector. It should be a parameter of size `(hidden,2*hidden)` if unidirectional, `(hidden,3*hidden)` if bidirectional.\n",
    "\n",
    "function S2S(hidden::Int, srcembsz::Int, tgtembsz::Int, srcvocab::Vocab, tgtvocab::Vocab;\n",
    "             layers=1, bidirectional=false, dropout=0)\n",
    "    ## Your code here\n",
    "    layerMultiplier = bidirectional ? 2 : 1\n",
    "\n",
    "    if bidirectional\n",
    "        wattnsize = (hidden, 3 * hidden)\n",
    "        mem = Memory(param(hidden, 2 * hidden))\n",
    "    else\n",
    "        wattnsize = (hidden, 2 * hidden)\n",
    "        mem = Memory(1)\n",
    "    end\n",
    "\n",
    "    S2S(\n",
    "        Embed(length(srcvocab.i2w), srcembsz),\n",
    "        RNN(\n",
    "            srcembsz,\n",
    "            hidden,\n",
    "            numLayers = (1 / layerMultiplier) * layers,\n",
    "            bidirectional = bidirectional,\n",
    "            dropout = dropout,\n",
    "        ),\n",
    "        mem,\n",
    "        Embed(length(tgtvocab.i2w), tgtembsz),\n",
    "        RNN(tgtembsz + hidden, hidden, numLayers = layers, dropout = dropout),\n",
    "        Attention(1, param(wattnsize[1], wattnsize[2]), param(1)),\n",
    "        Linear(hidden, length(tgtvocab.i2w)),\n",
    "        dropout,\n",
    "        srcvocab,\n",
    "        tgtvocab,\n",
    "    )\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "@testset \"Testing S2S constructor\" begin\n",
    "    H, Ex, Ey, Vx, Vy, L, Dx, Pdrop = 8,\n",
    "        9,\n",
    "        10,\n",
    "        length(dtrn.src.vocab.i2w),\n",
    "        length(dtrn.tgt.vocab.i2w),\n",
    "        2,\n",
    "        2,\n",
    "        0.2\n",
    "    m = S2S(\n",
    "        H,\n",
    "        Ex,\n",
    "        Ey,\n",
    "        dtrn.src.vocab,\n",
    "        dtrn.tgt.vocab;\n",
    "        layers = L,\n",
    "        bidirectional = (Dx == 2),\n",
    "        dropout = Pdrop,\n",
    "    )\n",
    "    @test size(m.srcembed.w) == (Ex, Vx)\n",
    "    @test size(m.tgtembed.w) == (Ey, Vy)\n",
    "    @test m.encoder.inputSize == Ex\n",
    "    @test m.decoder.inputSize == Ey + H\n",
    "    @test m.encoder.hiddenSize == m.decoder.hiddenSize == H\n",
    "    @test m.encoder.direction == Dx - 1\n",
    "    @test m.encoder.numLayers == (Dx == 2 ? L ÷ 2 : L)\n",
    "    @test m.decoder.numLayers == L\n",
    "    @test m.encoder.dropout == m.decoder.dropout == Pdrop\n",
    "    @test size(m.projection.w) == (Vy, H)\n",
    "    @test size(m.memory.w) == (Dx == 2 ? (H, 2H) : ())\n",
    "    @test m.attention.wquery == 1\n",
    "    @test size(m.attention.wattn) == (Dx == 2 ? (H, 3H) : (H, 2H))\n",
    "    @test size(m.attention.scale) == (1,)\n",
    "    @test m.srcvocab === dtrn.src.vocab\n",
    "    @test m.tgtvocab === dtrn.tgt.vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:  | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing memory | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing memory\", Any[], 2, false)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 2. Memory\n",
    "#\n",
    "# The memory layer turns the output of the encoder to a pair of tensors that will be used as\n",
    "# keys and values for the attention mechanism. Remember that the encoder RNN output has size\n",
    "# `(H*D,B,Tx)` where `H` is the hidden size, `D` is 1 for unidirectional, 2 for\n",
    "# bidirectional, `B` is the batchsize, and `Tx` is the sequence length. It will be\n",
    "# convenient to store these values in batch major form for the attention mechanism, so\n",
    "# *values* in memory will be a permuted copy of the encoder output with size `(H*D,Tx,B)`\n",
    "# (see `@doc permutedims`). The *keys* in the memory need to have the same first dimension\n",
    "# as the *queries* (i.e. the decoder hidden states). So *values* will be transformed into\n",
    "# *keys* of size `(H,B,Tx)` with `keys = m.w * values` where `m::Memory` is the memory\n",
    "# layer. Note that you will have to do some reshaping to 2-D and back to 3-D for matrix\n",
    "# multiplications. Also note that `m.w` may be a scalar such as `1` e.g. when `D=1` and we\n",
    "# want keys and values to be identical.\n",
    "\n",
    "\n",
    "function (m::Memory)(x)\n",
    "    ## Your code here\n",
    "    H, B, Tx = size(x)\n",
    "    val = copy(x)\n",
    "    v = permutedims(val, (1, 3, 2))\n",
    "    k = mmul(m.w, v)\n",
    "    return k, v\n",
    "end\n",
    "\n",
    "# You can use the following helper function for scaling and linear transformations of 3-D tensors:\n",
    "mmul(w, x) = (w == 1 ? x :\n",
    " w == 0 ? 0 :\n",
    " reshape(w * reshape(x, size(x, 1), :), (:, size(x)[2:end]...)))\n",
    "\n",
    "#-\n",
    "@testset \"Testing memory\" begin\n",
    "    H, D, B, Tx = pretrained.encoder.hiddenSize,\n",
    "        pretrained.encoder.direction + 1,\n",
    "        4,\n",
    "        5\n",
    "    x = KnetArray(randn(Float32, H * D, B, Tx)) #!! GPU\n",
    "    #x = Array(randn(Float32, H * D, B, Tx))\n",
    "    k, v = pretrained.memory(x)\n",
    "    @test v == permutedims(x, (1, 3, 2))\n",
    "    @test k == mmul(pretrained.memory.w, v)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:   | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing encoder | \u001b[32m   7  \u001b[39m\u001b[36m    7\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing encoder\", Any[], 7, false)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 3. Encoder\n",
    "#\n",
    "# `encode()` takes a model `s` and a source language minibatch `src`. It passes the input\n",
    "# through `s.srcembed` and `s.encoder` layers with the `s.encoder` RNN hidden states\n",
    "# initialized to `0` in the beginning, and copied to the `s.decoder` RNN at the end. The\n",
    "# steps so far are identical to `S2S_v1` but there is an extra step: The encoder output is\n",
    "# passed to the `s.memory` layer which returns a `(keys,values)` pair. `encode()` returns\n",
    "# this pair to be used later by the attention mechanism.\n",
    "\n",
    "function encode(s::S2S, src)\n",
    "    emb_out_src = s.srcembed(src)\n",
    "\n",
    "    s.encoder.h = 0\n",
    "    s.encoder.c = 0\n",
    "\n",
    "    y_enc = s.encoder(emb_out_src)\n",
    "\n",
    "    s.decoder.h = s.encoder.h\n",
    "    s.decoder.c = s.encoder.c\n",
    "       \n",
    "    return s.memory(y_enc)\n",
    "end\n",
    "\n",
    "#-\n",
    "@testset \"Testing encoder\" begin\n",
    "    src1, tgt1 = first(dtrn)\n",
    "    key1, val1 = encode(pretrained, src1)\n",
    "    H, D, B, Tx = pretrained.encoder.hiddenSize,\n",
    "        pretrained.encoder.direction + 1,\n",
    "        size(src1, 1),\n",
    "        size(src1, 2)\n",
    "    @test size(key1) == (H, Tx, B)\n",
    "    @test size(val1) == (H * D, Tx, B)\n",
    "    @test (pretrained.decoder.h, pretrained.decoder.c) === (\n",
    "        pretrained.encoder.h,\n",
    "        pretrained.encoder.c,\n",
    "    )\n",
    "    @test norm(key1) ≈ 1214.4755f0\n",
    "    @test norm(val1) ≈ 191.10411f0\n",
    "    @test norm(pretrained.decoder.h) ≈ 48.536964f0\n",
    "    @test norm(pretrained.decoder.c) ≈ 391.69028f0\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:     | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing attention | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing attention\", Any[], 2, false)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 4. Attention\n",
    "#\n",
    "# The attention layer takes `cell`: the decoder output, and `mem`: a pair of (keys,vals)\n",
    "# from the encoder, and computes and returns the attention vector. First `a.wquery` is used\n",
    "# to linearly transform the cell to the query tensor. The query tensor is reshaped and/or\n",
    "# permuted as appropriate and multiplied with the keys tensor to compute the attention\n",
    "# scores. Please see `@doc bmm` for the batched matrix multiply operation used for this\n",
    "# step. The attention scores are scaled using `a.scale` and normalized along the time\n",
    "# dimension using `softmax`. After the appropriate reshape and/or permutation, the scores\n",
    "# are multiplied with the `vals` tensor (using `bmm` again) to compute the context\n",
    "# tensor. After the appropriate reshape and/or permutation the context vector is\n",
    "# concatenated with the cell and linearly transformed to the attention vector using\n",
    "# `a.wattn`. Please see the paper and code examples for details.\n",
    "#\n",
    "# Note: the paper mentions a final `tanh` transform, however the final version of the\n",
    "# reference code does not use `tanh` and gets better results. Therefore we will skip `tanh`.\n",
    "\n",
    "#deccell(H,B,Ty), keys(H,Tx,B), vals(Dx*H,Tx,B) -> attnvec(H,B,Ty)\n",
    "function (a::Attention)(cell, mem)\n",
    "    H,B,Ty = size(cell)\n",
    "\n",
    "    qtensor = cell*a.wquery\n",
    "\n",
    "    qtensor = permutedims(qtensor, (3,1,2))\n",
    "\n",
    "    scores = bmm(qtensor,mem[1]) # Multiply with keys\n",
    "\n",
    "    scores=(a.scale[1])*scores\n",
    "    scores = softmax(scores,dims=2)\n",
    "\n",
    "    v= permutedims(mem[2],(2,1,3))\n",
    "    context = bmm(scores,v)\n",
    "\n",
    "\n",
    "    context = permutedims(context,(2,3,1))\n",
    "    context = cat(cell,context;dims=1)\n",
    "\n",
    "    context = reshape(context,(size(a.wattn)[2],:))\n",
    "    context = a.wattn*context\n",
    "    context = reshape(context,(H,B,Ty))\n",
    "end\n",
    "\n",
    "#-\n",
    "@testset \"Testing attention\" begin\n",
    "    src1, tgt1 = first(dtrn)\n",
    "    key1, val1 = encode(pretrained, src1)\n",
    "    H, B = pretrained.encoder.hiddenSize, size(src1, 1)\n",
    "    Knet.seed!(1)\n",
    "    x = KnetArray(randn(Float32, H, B, 5)) #!! GPU\n",
    "    # x = Array(randn(Float32, H, B, 5))\n",
    "    y = pretrained.attention(x, (key1, val1))\n",
    "    @test size(y) == size(x)\n",
    "    @test norm(y) ≈ 808.381f0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:   | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing decoder | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing decoder\", Any[], 2, false)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 5. Decoder\n",
    "#\n",
    "# `decode()` takes a model `s`, a target language minibatch `tgt`, the memory from the\n",
    "# encoder `mem` and the decoder output from the previous time step `prev`. After the input\n",
    "# is passed through the embedding layer, it is concatenated with `prev` (this is called\n",
    "# input feeding). The resulting tensor is passed through `s.decoder`. Finally the\n",
    "# `s.attention` layer takes the decoder output and the encoder memory to compute the\n",
    "# \"attention vector\" which is returned by `decode()`.\n",
    "\n",
    "function decode(s::S2S, tgt, mem, prev)\n",
    "    emb_out_tgt = s.tgtembed(tgt)\n",
    "\n",
    "    inputfeeding = cat(emb_out_tgt,prev;dims=1)\n",
    "    y_dec = s.decoder(inputfeeding)\n",
    "\n",
    "    attentionout = s.attention(y_dec,mem)\n",
    "end\n",
    "\n",
    "#-\n",
    "@testset \"Testing decoder\" begin\n",
    "    src1, tgt1 = first(dtrn)\n",
    "    key1, val1 = encode(pretrained, src1)\n",
    "    H, B = pretrained.encoder.hiddenSize, size(src1, 1)\n",
    "    Knet.seed!(1)\n",
    "    cell = randn!(similar(key1, size(key1, 1), size(key1, 3), 1))\n",
    "    cell = decode(pretrained, tgt1[:, 1:1], (key1, val1), cell)\n",
    "    @test size(cell) == (H, B, 1)\n",
    "    @test norm(cell) ≈ 131.21631f0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary: | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing loss  | \u001b[32m   1  \u001b[39m\u001b[36m    1\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing loss\", Any[], 1, false)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 6. Loss\n",
    "#\n",
    "# The loss function takes source language minibatch `src`, and a target language minibatch\n",
    "# `tgt` and returns `sumloss/numwords` if `average=true` or `(sumloss,numwords)` if\n",
    "# `average=false` where `sumloss` is the total negative log likelihood loss and `numwords` is\n",
    "# the number of words predicted (including a final eos for each sentence). The source is first\n",
    "# encoded using `encode` yielding a `(keys,vals)` pair (memory). Then the decoder is called to\n",
    "# predict each word of `tgt` given the previous word, `(keys,vals)` pair, and the previous\n",
    "# decoder output. The previous decoder output is initialized with zeros for the first\n",
    "# step. The output of the decoder at each step is passed through the projection layer giving\n",
    "# word scores. Losses can be computed from word scores and masked/shifted `tgt`.\n",
    "\n",
    "function (s::S2S)(src, tgt; average = true)\n",
    "    ## Your code here\n",
    "    B,Ty = size(tgt)\n",
    "    Ty -=1\n",
    "\n",
    "    project = s.projection\n",
    "\n",
    "    mem = encode(s,src)\n",
    "    \n",
    "    prev = KnetArray(zeros(Float32,s.decoder.hiddenSize,B))\n",
    "\n",
    "    verify = deepcopy(tgt[:,2:end])\n",
    "    mask!(verify, s.tgtvocab.eos)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_word = 0\n",
    "        \n",
    "    for word_order in 1:Ty\n",
    "        step_wise_tgt = reshape(tgt[:,word_order],:,1)                \n",
    "        dec_state = decode(s,step_wise_tgt, mem, prev)\n",
    "        pred = project(reshape(dec_state, :, B))\n",
    "        \n",
    "        Δloss,Δword = nll(pred,verify[:,word_order];average=false)\n",
    "        total_loss += Δloss\n",
    "        total_word += Δword\n",
    "        \n",
    "        prev = dec_state\n",
    "    end\n",
    "\n",
    "    average && return total_loss*1.0/total_word\n",
    "    return total_loss,total_word\n",
    "end\n",
    "\n",
    "#-\n",
    "@testset \"Testing loss\" begin\n",
    "    src1, tgt1 = first(dtrn)\n",
    "    @test pretrained(src1, tgt1) ≈ 1.4666592f0\n",
    "    #@test pretrained(src1, tgt1, average = false) .≈ (1949.1901f0, 1329)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:      | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing translator | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing translator\", Any[], 2, false)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 7. Greedy translator\n",
    "#\n",
    "# An `S2S` object can be called with a single argument (source language minibatch `src`, with\n",
    "# size `B,Tx`) to generate translations (target language minibatch with size `B,Ty`). The\n",
    "# keyword argument `stopfactor` determines how much longer the output can be compared to the\n",
    "# input. Similar to the loss function, the source minibatch is encoded yield a `(keys,vals)`\n",
    "# pair (memory). We generate the output one time step at a time by calling the decoder with\n",
    "# the last output, the memory, and the last decoder state. The last output is initialized to\n",
    "# an array of `eos` tokens and the last decoder state is initialized to an array of\n",
    "# zeros. After computing the scores for the next word using the projection layer, the highest\n",
    "# scoring words are selected and appended to the output. The generation stops when all outputs\n",
    "# in the batch have generated `eos` or when the length of the output is `stopfactor` times the\n",
    "# input.\n",
    "\n",
    "function (s::S2S)(src; stopfactor = 3)\n",
    "    # Preperation for initial step\n",
    "    B,Tx = size(src)\n",
    "    max_step = stopfactor * Tx\n",
    "    tgt_eos = s.tgtvocab.eos\n",
    "\n",
    "    tgt = fill(tgt_eos, (B, 1))\n",
    "    output = Array{Int64}(undef, B, max_step)\n",
    "    prev = KnetArray(zeros(Float32,s.decoder.hiddenSize,B))\n",
    "\n",
    "    mem = encode(s,src)\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    eos_check = fill(1, (B, 1))\n",
    "    while step < max_step\n",
    "        step += 1\n",
    "\n",
    "        dec_state = decode(s,tgt,mem,prev)\n",
    "        pred = s.projection(reshape(dec_state, :, B))\n",
    "        prev = dec_state\n",
    "\n",
    "        eos_num = 0\n",
    "        for i = 1:B\n",
    "            index = findmax(pred[:,i])[2]\n",
    "            tgt[i] = index\n",
    "            if index == tgt_eos\n",
    "                eos_check[i] = 0\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Check here, whether it is a mandatory approach or\n",
    "        # anomally as a result of the implementation\n",
    "        output[:,step] = tgt.^eos_check\n",
    "        sum(eos_check) == 0 && break # all produced eos\n",
    "    end\n",
    "\n",
    "    return output[:, 1:step]\n",
    "end\n",
    "\n",
    "#-\n",
    "@testset \"Testing translator\" begin\n",
    "    src1, tgt1 = first(dtrn)\n",
    "    tgt2 = pretrained(src1)\n",
    "    @test size(tgt2) == (64, 41)\n",
    "    @test tgt2[1:3, 1:3] == [14 25 10647; 37 25 1426; 27 5 349]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bidirectional, layers, hidden, srcembed, tgtembed, dropout, epochs, iters, bleu, save) = (true, 2, 512, 512, 512, 0.2, 10, 0, true, true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "┣                    ┫ [0.00%, 1/28120, 00:23/181:40:13, 23.26s/i] (dev = 9.820343f0, tst = (9.817911f0,), mem = 1.6104833f10)\n",
      "┣                    ┫ [0.46%, 129/28120, 01:30/05:25:33, 1.93i/s] (dev = 5.72198f0, tst = (5.7552238f0,), mem = 1.6105037f10)\n",
      "┣▏                   ┫ [0.86%, 243/28120, 02:46/05:19:21, 1.50i/s] (dev = 5.3511114f0, tst = (5.365451f0,), mem = 1.6088885f10)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacktrace:\n",
      " [1] \u001b[1malloc\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/CuArrays/ZYCpV/src/memory.jl:138\u001b[22m [inlined]\n",
      " [2] \u001b[1mCuArrays.CuArray{UInt8,1,P} where P\u001b[22m\u001b[1m(\u001b[22m::UndefInitializer, ::Tuple{Int64}\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/CuArrays/ZYCpV/src/array.jl:90\u001b[22m\n",
      " [3] \u001b[1mType\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/CuArrays/ZYCpV/src/array.jl:98\u001b[22m [inlined]\n",
      " [4] \u001b[1mType\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/CuArrays/ZYCpV/src/array.jl:99\u001b[22m [inlined]\n",
      " [5] \u001b[1mKnetPtrCu\u001b[22m\u001b[1m(\u001b[22m::Int64\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/Knet/LjPts/src/cuarray.jl:90\u001b[22m\n",
      " [6] \u001b[1mType\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/Knet/LjPts/src/kptr.jl:102\u001b[22m [inlined]\n",
      " [7] \u001b[1mType\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/Knet/LjPts/src/karray.jl:81\u001b[22m [inlined]\n",
      " [8] \u001b[1m#rnnforw#619\u001b[22m\u001b[1m(\u001b[22m::Ptr{Nothing}, ::Nothing, ::Bool, ::Bool, ::typeof(Knet.rnnforw), ::RNN, ::KnetArray{Float32,3}, ::KnetArray{Float32,3}, ::KnetArray{Float32,3}, ::KnetArray{Float32,3}\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/Knet/LjPts/src/rnn.jl:577\u001b[22m\n",
      " [9] \u001b[1m(::getfield(Knet, Symbol(\"#kw##rnnforw\")))\u001b[22m\u001b[1m(\u001b[22m::NamedTuple{(:hy, :cy, :batchSizes),Tuple{Bool,Bool,Nothing}}, ::typeof(Knet.rnnforw), ::RNN, ::KnetArray{Float32,3}, ::KnetArray{Float32,3}, ::KnetArray{Float32,3}, ::KnetArray{Float32,3}\u001b[1m)\u001b[22m at \u001b[1m./none:0\u001b[22m\n",
      " [10] \u001b[1m#forw#1\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Symbol,Union{Nothing, Bool},Tuple{Symbol,Symbol,Symbol},NamedTuple{(:hy, :cy, :batchSizes),Tuple{Bool,Bool,Nothing}}}, ::typeof(AutoGrad.forw), ::Function, ::RNN, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/AutoGrad/pTNVv/src/core.jl:66\u001b[22m\n",
      " [11] \u001b[1m#forw\u001b[22m at \u001b[1m./none:0\u001b[22m [inlined]\n",
      " [12] \u001b[1m#rnnforw#637\u001b[22m at \u001b[1m./none:0\u001b[22m [inlined]\n",
      " [13] \u001b[1m(::getfield(Knet, Symbol(\"#kw##rnnforw\")))\u001b[22m\u001b[1m(\u001b[22m::NamedTuple{(:hy, :cy, :batchSizes),Tuple{Bool,Bool,Nothing}}, ::typeof(Knet.rnnforw), ::RNN, ::Param{KnetArray{Float32,3}}, ::AutoGrad.Result{KnetArray{Float32,3}}, ::AutoGrad.Result{KnetArray{Float32,3}}, ::AutoGrad.Result{KnetArray{Float32,3}}\u001b[1m)\u001b[22m at \u001b[1m./none:0\u001b[22m\n",
      " [14] \u001b[1m#call#600\u001b[22m\u001b[1m(\u001b[22m::Nothing, ::RNN, ::AutoGrad.Result{KnetArray{Float32,3}}\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/Knet/LjPts/src/rnn.jl:205\u001b[22m\n",
      " [15] \u001b[1m(::RNN)\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{KnetArray{Float32,3}}\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/Knet/LjPts/src/rnn.jl:190\u001b[22m\n",
      " [16] \u001b[1mdecode\u001b[22m\u001b[1m(\u001b[22m::S2S, ::Array{Int64,2}, ::Tuple{AutoGrad.Result{KnetArray{Float32,3}},AutoGrad.Result{KnetArray{Float32,3}}}, ::AutoGrad.Result{KnetArray{Float32,3}}\u001b[1m)\u001b[22m at \u001b[1m./In[10]:14\u001b[22m\n",
      " [17] \u001b[1m#call#21\u001b[22m\u001b[1m(\u001b[22m::Bool, ::S2S, ::Array{Int64,2}, ::Array{Int64,2}\u001b[1m)\u001b[22m at \u001b[1m./In[11]:32\u001b[22m\n",
      " [18] \u001b[1m(::S2S)\u001b[22m\u001b[1m(\u001b[22m::Array{Int64,2}, ::Array{Int64,2}\u001b[1m)\u001b[22m at \u001b[1m./In[11]:15\u001b[22m\n",
      " [19] \u001b[1m(::getfield(Knet, Symbol(\"##695#696\")){Knet.Minimize{Array{Tuple{Array{Int64,2},Array{Int64,2}},1}},Tuple{Array{Int64,2},Array{Int64,2}}})\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/AutoGrad/pTNVv/src/core.jl:205\u001b[22m\n",
      " [20] \u001b[1m#differentiate#3\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(AutoGrad.differentiate), ::Function\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/AutoGrad/pTNVv/src/core.jl:144\u001b[22m\n",
      " [21] \u001b[1mdifferentiate\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/AutoGrad/pTNVv/src/core.jl:135\u001b[22m [inlined]\n",
      " [22] \u001b[1miterate\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/Knet/LjPts/src/train.jl:23\u001b[22m [inlined]\n",
      " [23] \u001b[1miterate\u001b[22m\u001b[1m(\u001b[22m::Knet.Progress{Knet.Minimize{Array{Tuple{Array{Int64,2},Array{Int64,2}},1}}}, ::Int64\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/Knet/LjPts/src/progress.jl:69\u001b[22m\n",
      " [24] \u001b[1m#progress!#692\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:seconds,),Tuple{Int64}}}, ::typeof(progress!), ::Function, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/Knet/LjPts/src/progress.jl:58\u001b[22m\n",
      " [25] \u001b[1m#progress!\u001b[22m at \u001b[1m./none:0\u001b[22m [inlined]\n",
      " [26] \u001b[1m#trainmodel#23\u001b[22m\u001b[1m(\u001b[22m::Bool, ::Int64, ::Int64, ::Int64, ::Int64, ::Float64, ::Int64, ::Int64, ::Bool, ::Bool, ::Int64, ::typeof(trainmodel), ::MTData, ::MTData, ::Base.Iterators.Take{MTData}\u001b[1m)\u001b[22m at \u001b[1m./In[13]:52\u001b[22m\n",
      " [27] \u001b[1m(::getfield(Main, Symbol(\"#kw##trainmodel\")))\u001b[22m\u001b[1m(\u001b[22m::NamedTuple{(:epochs, :save, :bleu),Tuple{Int64,Bool,Bool}}, ::typeof(trainmodel), ::MTData, ::MTData, ::Base.Iterators.Take{MTData}\u001b[1m)\u001b[22m at \u001b[1m./none:0\u001b[22m\n",
      " [28] top-level scope at \u001b[1mIn[13]:65\u001b[22m\n",
      " [29] \u001b[1meval\u001b[22m at \u001b[1m./boot.jl:330\u001b[22m [inlined]\n",
      " [30] \u001b[1msoftscope_include_string\u001b[22m\u001b[1m(\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/SoftGlobalScope/cSbw5/src/SoftGlobalScope.jl:218\u001b[22m\n",
      " [31] \u001b[1mexecute_request\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/IJulia/F1GUo/src/execute_request.jl:67\u001b[22m\n",
      " [32] \u001b[1m#invokelatest#1\u001b[22m at \u001b[1m./essentials.jl:790\u001b[22m [inlined]\n",
      " [33] \u001b[1minvokelatest\u001b[22m at \u001b[1m./essentials.jl:789\u001b[22m [inlined]\n",
      " [34] \u001b[1meventloop\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m at \u001b[1m/home/ec2-user/.julia/packages/IJulia/F1GUo/src/eventloop.jl:8\u001b[22m\n",
      " [35] \u001b[1m(::getfield(IJulia, Symbol(\"##15#18\")))\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m./task.jl:268\u001b[22m\n"
     ]
    },
    {
     "ename": "CuArrays.OutOfGPUMemoryError",
     "evalue": "Out of GPU memory trying to allocate 25.125 MiB\nEffective GPU memory usage: 99.93% (15.742 GiB/15.752 GiB)\nCuArrays GPU memory usage: 14.954 GiB\nBinnedPool usage: 14.954 GiB (14.954 GiB allocated, 0 bytes cached)\nBinnedPool efficiency: 70.13% (10.487 GiB requested, 14.954 GiB allocated)\n",
     "output_type": "error",
     "traceback": [
      "Out of GPU memory trying to allocate 25.125 MiB\nEffective GPU memory usage: 99.93% (15.742 GiB/15.752 GiB)\nCuArrays GPU memory usage: 14.954 GiB\nBinnedPool usage: 14.954 GiB (14.954 GiB allocated, 0 bytes cached)\nBinnedPool efficiency: 70.13% (10.487 GiB requested, 14.954 GiB allocated)\n",
      "",
      "Stacktrace:",
      " [1] #differentiate#3(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(AutoGrad.differentiate), ::Function) at /home/ec2-user/.julia/packages/AutoGrad/pTNVv/src/core.jl:148",
      " [2] differentiate at /home/ec2-user/.julia/packages/AutoGrad/pTNVv/src/core.jl:135 [inlined]",
      " [3] iterate at /home/ec2-user/.julia/packages/Knet/LjPts/src/train.jl:23 [inlined]",
      " [4] iterate(::Knet.Progress{Knet.Minimize{Array{Tuple{Array{Int64,2},Array{Int64,2}},1}}}, ::Int64) at /home/ec2-user/.julia/packages/Knet/LjPts/src/progress.jl:69",
      " [5] #progress!#692(::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:seconds,),Tuple{Int64}}}, ::typeof(progress!), ::Function, ::Vararg{Any,N} where N) at /home/ec2-user/.julia/packages/Knet/LjPts/src/progress.jl:58",
      " [6] #progress! at ./none:0 [inlined]",
      " [7] #trainmodel#23(::Bool, ::Int64, ::Int64, ::Int64, ::Int64, ::Float64, ::Int64, ::Int64, ::Bool, ::Bool, ::Int64, ::typeof(trainmodel), ::MTData, ::MTData, ::Base.Iterators.Take{MTData}) at ./In[13]:52",
      " [8] (::getfield(Main, Symbol(\"#kw##trainmodel\")))(::NamedTuple{(:epochs, :save, :bleu),Tuple{Int64,Bool,Bool}}, ::typeof(trainmodel), ::MTData, ::MTData, ::Base.Iterators.Take{MTData}) at ./none:0",
      " [9] top-level scope at In[13]:65"
     ]
    }
   ],
   "source": [
    "# ## Part 8. Training\n",
    "#\n",
    "# `trainmodel` creates, trains and returns an `S2S` model. The arguments are described in\n",
    "# comments.\n",
    "\n",
    "function trainmodel(\n",
    "    trn,                  # Training data\n",
    "    dev,                  # Validation data, used to determine the best model\n",
    "    tst...;               # Zero or more test datasets, their loss will be periodically reported\n",
    "    bidirectional = true, # Whether to use a bidirectional encoder\n",
    "    layers = 2,           # Number of layers (use `layers÷2` for a bidirectional encoder)\n",
    "    hidden = 512,         # Size of the hidden vectors\n",
    "    srcembed = 512,       # Size of the source language embedding vectors\n",
    "    tgtembed = 512,       # Size of the target language embedding vectors\n",
    "    dropout = 0.2,        # Dropout probability\n",
    "    epochs = 0,           # Number of epochs (one of epochs or iters should be nonzero for training)\n",
    "    iters = 0,            # Number of iterations (one of epochs or iters should be nonzero for training)\n",
    "    bleu = false,         # Whether to calculate the BLEU score for the final model\n",
    "    save = false,         # Whether to save the final model\n",
    "    seconds = 60,         # Frequency of progress reporting\n",
    ")\n",
    "    @show bidirectional,\n",
    "        layers,\n",
    "        hidden,\n",
    "        srcembed,\n",
    "        tgtembed,\n",
    "        dropout,\n",
    "        epochs,\n",
    "        iters,\n",
    "        bleu,\n",
    "        save\n",
    "    flush(stdout)\n",
    "    model = S2S(\n",
    "        hidden,\n",
    "        srcembed,\n",
    "        tgtembed,\n",
    "        trn.src.vocab,\n",
    "        trn.tgt.vocab;\n",
    "        layers = layers,\n",
    "        dropout = dropout,\n",
    "        bidirectional = bidirectional,\n",
    "    )\n",
    "\n",
    "    epochs == iters == 0 && return model\n",
    "\n",
    "    (ctrn, cdev, ctst) = collect(trn), collect(dev), collect.(tst)\n",
    "    traindata = (epochs > 0 ?\n",
    "                 collect(flatten(shuffle!(ctrn) for i = 1:epochs)) :\n",
    "                 shuffle!(collect(take(cycle(ctrn), iters))))\n",
    "\n",
    "    bestloss, bestmodel = loss(model, cdev), deepcopy(model)\n",
    "    progress!(adam(model, traindata), seconds = seconds) do y\n",
    "        devloss = loss(model, cdev)\n",
    "        tstloss = map(d -> loss(model, d), ctst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (dev = devloss, tst = tstloss, mem = Float32(CuArrays.usage[]))\n",
    "    end\n",
    "    save && Knet.save(\"attn-$(Int(time_ns())).jld2\", \"model\", bestmodel)\n",
    "    bleu && Main.bleu(bestmodel, dev)\n",
    "    return bestmodel\n",
    "end\n",
    "\n",
    "# Train a model: If your implementation is correct, the first epoch should take about 24\n",
    "# minutes on a v100 and bring the loss from 9.83 to under 4.0. 10 epochs would take about 4\n",
    "# hours on a v100. With other GPUs you may have to use a smaller batch size (if memory is\n",
    "# lower) and longer time (if gpu speed is lower).\n",
    "\n",
    "## Uncomment the appropriate option for training:\n",
    "#model = pretrained  # Use reference model\n",
    "#model = Knet.load(\"attn-1538395466294882.jld2\", \"model\")  # Load pretrained model\n",
    "model = trainmodel(dtrn,ddev,take(dtrn,20); epochs=10, save=true, bleu=true)  # Train model\n",
    "\n",
    "# Code to sample translations from a dataset\n",
    "data1 = MTData(tr_dev, en_dev, batchsize = 1) |> collect;\n",
    "function translate_sample(model, data)\n",
    "    (src, tgt) = rand(data)\n",
    "    out = model(src)\n",
    "    println(\"SRC: \", int2str(src, model.srcvocab))\n",
    "    println(\"REF: \", int2str(tgt, model.tgtvocab))\n",
    "    println(\"OUT: \", int2str(out, model.tgtvocab))\n",
    "end\n",
    "\n",
    "# Generate translations for random instances from the dev set\n",
    "translate_sample(model, data1)\n",
    "\n",
    "# Code to generate translations from user input\n",
    "function translate_input(model)\n",
    "    v = model.srcvocab\n",
    "    src = [get(v.w2i, w, v.unk) for w in v.tokenizer(readline())]'\n",
    "    out = model(src)\n",
    "    println(\"SRC: \", int2str(src, model.srcvocab))\n",
    "    println(\"OUT: \", int2str(out, model.tgtvocab))\n",
    "end\n",
    "\n",
    "# Generate translations for user input\n",
    "## translate_input(model)\n",
    "\n",
    "# ## Competition\n",
    "#\n",
    "# The reference model `pretrained` has 16.2 bleu. By playing with the optimization algorithm\n",
    "# and hyperparameters, using per-sentence loss, and (most importantly) splitting the Turkish\n",
    "# words I was able to push the performance to 21.0 bleu. I will give extra credit to groups\n",
    "# that can exceed 21.0 bleu in this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
