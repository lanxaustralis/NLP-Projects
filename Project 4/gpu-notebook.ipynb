{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/ec2-user/.julia/compiled/v1.2/Knet/f4vSz.ji for Knet [1902f260-5fb4-5aff-8c31-6271790ab950]\n",
      "└ @ Base loading.jl:1240\n",
      "┌ Info: Recompiling stale cache file /home/ec2-user/.julia/compiled/v1.2/IterTools/hhnii.ji for IterTools [c8e1da08-722c-5040-9ed9-7db0dc04731e]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m NNlib → `~/.julia/packages/NNlib/Nksco/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "Package name: AutoGrad Version: 1.2.0\n",
      "Package name: IterTools Version: 1.3.0\n",
      "Package name: StatsBase Version: 0.32.0\n",
      "Package name: Knet Version: 1.3.2\n",
      "Package name: CuArrays Version: 1.5.0\n",
      "Package name: IJulia Version: 1.20.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#jl Use `Literate.notebook(juliafile, \".\", execute=false)` to convert to notebook.\n",
    "\n",
    "# # Attention-based Neural Machine Translation\n",
    "#\n",
    "# **Reference:** Luong, Thang, Hieu Pham and Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1412-1421. 2015.\n",
    "#\n",
    "# * https://www.aclweb.org/anthology/D15-1166/ (main paper reference)\n",
    "# * https://arxiv.org/abs/1508.04025 (alternative paper url)\n",
    "# * https://github.com/tensorflow/nmt (main code reference)\n",
    "# * https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention (alternative code reference)\n",
    "# * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py:2449,2103 (attention implementation)\n",
    "\n",
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, Random, IterTools#, CuArrays\n",
    "\n",
    "import Pkg\n",
    "\n",
    "#GPU related\n",
    "Pkg.add(\"CuArrays\")\n",
    "Pkg.build(\"CuArrays\")\n",
    "\n",
    "using CuArrays: CuArrays, usage_limit\n",
    "\n",
    "Pkg.update()\n",
    "pkgs = Pkg.installed()\n",
    "\n",
    "for package in keys(pkgs)\n",
    "    if pkgs[package] == nothing\n",
    "        pkgs[package] = VersionNumber(\"0.0.1\")\n",
    "    end\n",
    "    println(\"Package name: \", package, \" Version: \", pkgs[package])\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCHSIZE = 32\n",
    "Knet.atype() = KnetArray{Float32}\n",
    "\n",
    "#CuArrays.usage_limit[] = 8_000_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bleu (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Code and data from previous projects\n",
    "#\n",
    "# Please copy or include the following types and related functions from previous projects:\n",
    "# `Vocab`, `TextReader`, `MTData`, `Embed`, `Linear`, `mask!`, `loss`, `int2str`,\n",
    "# `bleu`.\n",
    "\n",
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "function Vocab(\n",
    "    file::String;\n",
    "    tokenizer = split,\n",
    "    vocabsize = Inf,\n",
    "    mincount = 1,\n",
    "    unk = \"<unk>\",\n",
    "    eos = \"<s>\",\n",
    ")\n",
    "    vocab_freq = Dict{String,Int64}(unk => 1, eos => 1)\n",
    "    w2i = Dict{String,Int64}(unk => 2, eos => 1)\n",
    "    i2w = Vector{String}()\n",
    "\n",
    "    push!(i2w, eos)\n",
    "    push!(i2w, unk)\n",
    "\n",
    "    open(file) do f\n",
    "        for line in eachline(f)\n",
    "            sentence = strip(lowercase(line))\n",
    "            sentence = tokenizer(line, [' '], keepempty = false)\n",
    "\n",
    "            for word in sentence\n",
    "                word == unk && continue\n",
    "                word == eos && continue # They are default ones to be added later\n",
    "                vocab_freq[word] = get!(vocab_freq, word, 0) + 1\n",
    "            end\n",
    "        end\n",
    "        close(f)\n",
    "    end\n",
    "\n",
    "\n",
    "# End of vanilla implementation of the vocaulary\n",
    "# From here we must add the mincount and vocabsize properties\n",
    "# We must change the first two property of the vocab wrt those paramaters\n",
    "    vocab_freq = sort!(\n",
    "        collect(vocab_freq),\n",
    "        by = tuple -> last(tuple),\n",
    "        rev = true,\n",
    "    )\n",
    "\n",
    "    if length(vocab_freq) > vocabsize - 2 # eos and unk ones\n",
    "        vocab_freq = vocab_freq[1:vocabsize-2] # trim to fit the size\n",
    "    end\n",
    "\n",
    "#vocab_freq = reverse(vocab_freq)\n",
    "\n",
    "    while true\n",
    "        length(vocab_freq) == 0 && break\n",
    "        word, freq = vocab_freq[end]\n",
    "        freq >= mincount && break # since it is already ordered\n",
    "        vocab_freq = vocab_freq[1:(end-1)]\n",
    "    end\n",
    "#pushfirst!(vocab_freq,unk=>1,eos=>1) # freq does not matter, just adding the\n",
    "    for i = 1:length(vocab_freq)\n",
    "        word, freq = vocab_freq[i]\n",
    "        ind = (get!(w2i, word, 1 + length(w2i)))\n",
    "        (length(i2w) < ind) && push!(i2w, word)\n",
    "    end\n",
    "\n",
    "    return Vocab(w2i, i2w, 2, 1, tokenizer)\n",
    "end\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "word2ind(dict, x) = get(dict, x, 2) # unk is 2\n",
    "\n",
    "#Implementing the iterate function\n",
    "function Base.iterate(r::TextReader, s = nothing)\n",
    "    if s == nothing\n",
    "        state = open(r.file)\n",
    "        Base.iterate(r, state)\n",
    "    else\n",
    "        if eof(s) == true\n",
    "            close(s)\n",
    "            return nothing\n",
    "        else\n",
    "            line = readline(s)\n",
    "            line = strip(lowercase(line))\n",
    "            sent = r.vocab.tokenizer(line, [' '], keepempty = false)\n",
    "            sent_ind = Int[]\n",
    "            for word in sent\n",
    "                ind = word2ind(r.vocab.w2i, word)\n",
    "                push!(sent_ind, ind)\n",
    "            end\n",
    "            return (sent_ind, s)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "#Embed\n",
    "struct Embed\n",
    "    w\n",
    "end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    Embed(param(embedsize, vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    l.w[:, x]\n",
    "end\n",
    "\n",
    "#Linear\n",
    "struct Linear\n",
    "    w\n",
    "    b\n",
    "end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    Linear(param(outputsize, inputsize), param0(outputsize))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    l.w * mat(x, dims = 1) .+ l.b\n",
    "end\n",
    "\n",
    "# Mask!\n",
    "function mask!(a, pad)\n",
    "    matr = a\n",
    "    for j = 1:size(matr)[1]\n",
    "        i = 0\n",
    "        while (i < length(matr[j, :]) - 1)\n",
    "            if matr[j, length(matr[j, :])-i-1] != pad\n",
    "                break\n",
    "\n",
    "            elseif matr[j, length(matr[j, :])-i] == pad\n",
    "                matr[j, length(matr[j, :])-i] = 0\n",
    "            end\n",
    "            i += 1\n",
    "        end\n",
    "    end\n",
    "    return matr\n",
    "end\n",
    "\n",
    "struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(\n",
    "    src::TextReader,\n",
    "    tgt::TextReader;\n",
    "    batchmaker = arraybatch,\n",
    "    batchsize = BATCHSIZE,\n",
    "    maxlength = typemax(Int),\n",
    "    batchmajor = false,\n",
    "    bucketwidth = 10,\n",
    "    numbuckets = min(BATCHSIZE, maxlength ÷ bucketwidth),\n",
    ")\n",
    "    buckets = [[] for i = 1:numbuckets] # buckets[i] is an array of sentence pairs with similar length\n",
    "    MTData(\n",
    "        src,\n",
    "        tgt,\n",
    "        batchsize,\n",
    "        maxlength,\n",
    "        batchmajor,\n",
    "        bucketwidth,\n",
    "        buckets,\n",
    "        batchmaker,\n",
    "    )\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::MTData, state = nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets\n",
    "            empty!(b)\n",
    "        end\n",
    "        state_src, state_tgt = nothing, nothing\n",
    "    else\n",
    "        state_src, state_tgt = state\n",
    "    end\n",
    "    bucket, ibucket = nothing, nothing\n",
    "\n",
    "\n",
    "    while true\n",
    "        iter_src = (state_src === nothing ? iterate(d.src) :\n",
    "                    iterate(d.src, state_src))\n",
    "        iter_tgt = (state_tgt === nothing ? iterate(d.tgt) :\n",
    "                    iterate(d.tgt, state_tgt))\n",
    "\n",
    "        if iter_src === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent_src, state_src = iter_src\n",
    "            sent_tgt, state_tgt = iter_tgt\n",
    "            if length(sent_src) > d.maxlength || length(sent_src) == 0\n",
    "                continue\n",
    "            end\n",
    "            ibucket = min(\n",
    "                1 + (length(sent_src) - 1) ÷ d.bucketwidth,\n",
    "                length(d.buckets),\n",
    "            )\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, (sent_src, sent_tgt))\n",
    "            if length(bucket) === d.batchsize\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    batch = d.batchmaker(d, bucket)\n",
    "\n",
    "    empty!(bucket)\n",
    "    return batch, (state_src, state_tgt)\n",
    "end\n",
    "\n",
    "\n",
    "function arraybatch(d::MTData, bucket)\n",
    "    bucketx = map(x -> x[1], bucket)\n",
    "    buckety = map(x -> x[2], bucket)\n",
    "    batch_x = fill(d.src.vocab.eos, length(bucketx), maximum(length.(bucketx)))\n",
    "    for i = 1:length(bucket)\n",
    "        batch_x[i, end-length(bucketx[i])+1:end] = bucketx[i]\n",
    "    end\n",
    "    batch_y = fill(\n",
    "        d.tgt.vocab.eos,\n",
    "        length(buckety),\n",
    "        maximum(length.(buckety)) + 2,\n",
    "    )\n",
    "    for i = 1:length(bucket)\n",
    "        batch_y[i, 2:length(buckety[i])+1] = buckety[i]\n",
    "    end\n",
    "\n",
    "    return (batch_x, batch_y)\n",
    "end\n",
    "\n",
    "function loss(model, data; average = true)\n",
    "    total_loss = 0\n",
    "    total_word = 0\n",
    "\n",
    "    for (x, y) in collect(data)\n",
    "        curr_loss, curr_word = model(x, y; average = false)\n",
    "        total_loss += curr_loss\n",
    "        total_word += curr_word\n",
    "    end\n",
    "\n",
    "    average && return total_loss / total_word\n",
    "    return (total_loss, total_word)\n",
    "\n",
    "end\n",
    "function int2str(y, vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w -> !isequal(w, vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1 + length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end\n",
    "function bleu(s2s, d::MTData)\n",
    "    d = MTData(d.src, d.tgt, batchsize = 1)\n",
    "    reffile = d.tgt.file\n",
    "    hypfile, hyp = mktemp()\n",
    "    for (x, y) in progress(collect(d))\n",
    "        g = s2s(x)\n",
    "        for i = 1:size(y, 1)\n",
    "            println(hyp, int2str(g[i, :], d.tgt.vocab))\n",
    "        end\n",
    "    end\n",
    "    close(hyp)\n",
    "    isfile(\"multi-bleu.perl\") || download(\n",
    "        \"https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\",\n",
    "        \"multi-bleu.perl\",\n",
    "    )\n",
    "    run(pipeline(`cat $hypfile`, `perl multi-bleu.perl $reffile`))\n",
    "    return hypfile\n",
    "end\n",
    "\n",
    "## Pre Assigment Part Completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## S2S: Sequence to sequence model with attention\n",
    "#\n",
    "# In this project we will define, train and evaluate a sequence to sequence encoder-decoder\n",
    "# model with attention for Turkish-English machine translation. The model has two extra\n",
    "# fields compared to `S2S_v1`: the `memory` layer computes keys and values from the encoder,\n",
    "# the `attention` layer computes the attention vector for the decoder.\n",
    "\n",
    "struct Memory\n",
    "    w\n",
    "end\n",
    "\n",
    "struct Attention\n",
    "    wquery\n",
    "    wattn\n",
    "    scale\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading reference model\n",
      "└ @ Main In[5]:21\n",
      "┌ Info: Reading data\n",
      "└ @ Main In[5]:40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MTData(TextReader(\"datasets/tr_to_en/tr.test\", Vocab(Dict(\"dev\" => 1277,\"komuta\" => 13566,\"ellisi\" => 25239,\"adresini\" => 22820,\"yüzeyi\" => 4051,\"paris'te\" => 9494,\"kafamdaki\" => 18790,\"yüzeyinde\" => 5042,\"geçerlidir\" => 6612,\"kökten\" => 7774…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"seçmemiz\", \"destekleyip\", \"karşılaştırılabilir\", \"ördeğin\", \"gününüzü\", \"bağışçı\", \"istismara\", \"yaşça\", \"tedci\", \"fakültesi'nde\"], 2, 1, split)), TextReader(\"datasets/tr_to_en/en.test\", Vocab(Dict(\"middle-income\" => 13398,\"photosynthesis\" => 7689,\"polarizing\" => 17881,\"henry\" => 4248,\"abducted\" => 15691,\"rises\" => 6225,\"hampshire\" => 13888,\"whiz\" => 16835,\"cost-benefit\" => 13137,\"progression\" => 5549…), [\"<s>\", \"<unk>\", \",\", \".\", \"the\", \"and\", \"to\", \"of\", \"a\", \"that\"  …  \"archaea\", \"handshake\", \"brit\", \"wiper\", \"heroines\", \"coca\", \"exceptionally\", \"gallbladder\", \"autopsies\", \"linguistics\"], 2, 1, split)), 32, 9223372036854775807, false, 10, Array{Any,1}[[], [], [], [], [], [], [], [], [], []  …  [], [], [], [], [], [], [], [], [], []], arraybatch)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct S2S\n",
    "    srcembed::Embed       # encinput(B,Tx) -> srcembed(Ex,B,Tx)\n",
    "    encoder::RNN          # srcembed(Ex,B,Tx) -> enccell(Dx*H,B,Tx)\n",
    "    memory::Memory        # enccell(Dx*H,B,Tx) -> keys(H,Tx,B), vals(Dx*H,Tx,B)\n",
    "    tgtembed::Embed       # decinput(B,Ty) -> tgtembed(Ey,B,Ty)\n",
    "    decoder::RNN          # tgtembed(Ey,B,Ty) . attnvec(H,B,Ty)[t-1] = (Ey+H,B,Ty) -> deccell(H,B,Ty)\n",
    "    attention::Attention  # deccell(H,B,Ty), keys(H,Tx,B), vals(Dx*H,Tx,B) -> attnvec(H,B,Ty)\n",
    "    projection::Linear    # attnvec(H,B,Ty) -> proj(Vy,B,Ty)\n",
    "    dropout::Real         # dropout probability\n",
    "    srcvocab::Vocab       # source language vocabulary\n",
    "    tgtvocab::Vocab       # target language vocabulary\n",
    "end\n",
    "\n",
    "\n",
    "# ## Load pretrained model and data\n",
    "#\n",
    "# We will load a pretrained model (16.20 bleu) for code testing.  The data should be loaded\n",
    "# with the vocabulary from the pretrained model for word id consistency.\n",
    "\n",
    "if !isdefined(Main, :pretrained) || pretrained === nothing\n",
    "    @info \"Loading reference model\"\n",
    "    isfile(\"s2smodel.jld2\") || download(\n",
    "        \"http://people.csail.mit.edu/deniz/comp542/s2smodel.jld2\",\n",
    "        \"s2smodel.jld2\",\n",
    "    )\n",
    "    pretrained = Knet.load(\"s2smodel.jld2\", \"model\")\n",
    "end\n",
    "datadir = \"datasets/tr_to_en\"\n",
    "if !isdir(datadir)\n",
    "    @info \"Downloading data\"\n",
    "    download(\n",
    "        \"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\",\n",
    "        \"qi18naacl-dataset.tar.gz\",\n",
    "    )\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    MAXLENGTH = 50\n",
    "    @info \"Reading data\"\n",
    "    tr_vocab = pretrained.srcvocab # Vocab(\"$datadir/tr.train\", mincount=5)\n",
    "    en_vocab = pretrained.tgtvocab # Vocab(\"$datadir/en.train\", mincount=5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    dtrn = MTData(\n",
    "        tr_train,\n",
    "        en_train,\n",
    "        batchsize = BATCHSIZE,\n",
    "        maxlength = MAXLENGTH,\n",
    "    )\n",
    "    ddev = MTData(tr_dev, en_dev, batchsize = BATCHSIZE)\n",
    "    dtst = MTData(tr_test, en_test, batchsize = BATCHSIZE)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:           | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing S2S constructor | \u001b[32m  16  \u001b[39m\u001b[36m   16\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing S2S constructor\", Any[], 16, false)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 1. Model constructor\n",
    "#\n",
    "# The `S2S` constructor takes the following arguments:\n",
    "# * `hidden`: size of the hidden vectors for both the encoder and the decoder\n",
    "# * `srcembsz`, `tgtembsz`: size of the source/target language embedding vectors\n",
    "# * `srcvocab`, `tgtvocab`: the source/target language vocabulary\n",
    "# * `layers=1`: number of layers\n",
    "# * `bidirectional=false`: whether the encoder is bidirectional\n",
    "# * `dropout=0`: dropout probability\n",
    "#\n",
    "# Hints:\n",
    "# * You can find the vocabulary size with `length(vocab.i2w)`.\n",
    "# * If the encoder is bidirectional `layers` must be even and the encoder should have `layers÷2` layers.\n",
    "# * The decoder will use \"input feeding\", i.e. it will concatenate its previous output to its input. Therefore the input size for the decoder should be `tgtembsz+hidden`.\n",
    "# * Only `numLayers`, `dropout`, and `bidirectional` keyword arguments should be used for RNNs, leave everything else default.\n",
    "# * The memory parameter `w` is used to convert encoder states to keys. If the encoder is bidirectional initialize it to a `(hidden,2*hidden)` parameter, otherwise set it to the constant 1.\n",
    "# * The attention parameter `wquery` is used to transform the query, set it to the constant 1 for this project.\n",
    "# * The attention parameter `scale` is used to scale the attention scores before softmax, set it to a parameter of size 1.\n",
    "# * The attention parameter `wattn` is used to transform the concatenation of the decoder output and the context vector to the attention vector. It should be a parameter of size `(hidden,2*hidden)` if unidirectional, `(hidden,3*hidden)` if bidirectional.\n",
    "\n",
    "function S2S(hidden::Int, srcembsz::Int, tgtembsz::Int, srcvocab::Vocab, tgtvocab::Vocab;\n",
    "             layers=1, bidirectional=false, dropout=0)\n",
    "    ## Your code here\n",
    "    layerMultiplier = bidirectional ? 2 : 1\n",
    "\n",
    "    if bidirectional\n",
    "        wattnsize = (hidden, 3 * hidden)\n",
    "        mem = Memory(param(hidden, 2 * hidden))\n",
    "    else\n",
    "        wattnsize = (hidden, 2 * hidden)\n",
    "        mem = Memory(1)\n",
    "    end\n",
    "\n",
    "    S2S(\n",
    "        Embed(length(srcvocab.i2w), srcembsz),\n",
    "        RNN(\n",
    "            srcembsz,\n",
    "            hidden,\n",
    "            numLayers = (1 / layerMultiplier) * layers,\n",
    "            bidirectional = bidirectional,\n",
    "            dropout = dropout,\n",
    "        ),\n",
    "        mem,\n",
    "        Embed(length(tgtvocab.i2w), tgtembsz),\n",
    "        RNN(tgtembsz + hidden, hidden, numLayers = layers, dropout = dropout),\n",
    "        Attention(1, param(wattnsize[1], wattnsize[2]), param(1)),\n",
    "        Linear(hidden, length(tgtvocab.i2w)),\n",
    "        dropout,\n",
    "        srcvocab,\n",
    "        tgtvocab,\n",
    "    )\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "@testset \"Testing S2S constructor\" begin\n",
    "    H, Ex, Ey, Vx, Vy, L, Dx, Pdrop = 8,\n",
    "        9,\n",
    "        10,\n",
    "        length(dtrn.src.vocab.i2w),\n",
    "        length(dtrn.tgt.vocab.i2w),\n",
    "        2,\n",
    "        2,\n",
    "        0.2\n",
    "    m = S2S(\n",
    "        H,\n",
    "        Ex,\n",
    "        Ey,\n",
    "        dtrn.src.vocab,\n",
    "        dtrn.tgt.vocab;\n",
    "        layers = L,\n",
    "        bidirectional = (Dx == 2),\n",
    "        dropout = Pdrop,\n",
    "    )\n",
    "    @test size(m.srcembed.w) == (Ex, Vx)\n",
    "    @test size(m.tgtembed.w) == (Ey, Vy)\n",
    "    @test m.encoder.inputSize == Ex\n",
    "    @test m.decoder.inputSize == Ey + H\n",
    "    @test m.encoder.hiddenSize == m.decoder.hiddenSize == H\n",
    "    @test m.encoder.direction == Dx - 1\n",
    "    @test m.encoder.numLayers == (Dx == 2 ? L ÷ 2 : L)\n",
    "    @test m.decoder.numLayers == L\n",
    "    @test m.encoder.dropout == m.decoder.dropout == Pdrop\n",
    "    @test size(m.projection.w) == (Vy, H)\n",
    "    @test size(m.memory.w) == (Dx == 2 ? (H, 2H) : ())\n",
    "    @test m.attention.wquery == 1\n",
    "    @test size(m.attention.wattn) == (Dx == 2 ? (H, 3H) : (H, 2H))\n",
    "    @test size(m.attention.scale) == (1,)\n",
    "    @test m.srcvocab === dtrn.src.vocab\n",
    "    @test m.tgtvocab === dtrn.tgt.vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...\n",
      "└ @ CUDAnative /home/ec2-user/.julia/packages/CUDAnative/RhbZ0/src/compiler/rtlib.jl:188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:  | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing memory | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing memory\", Any[], 2, false)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 2. Memory\n",
    "#\n",
    "# The memory layer turns the output of the encoder to a pair of tensors that will be used as\n",
    "# keys and values for the attention mechanism. Remember that the encoder RNN output has size\n",
    "# `(H*D,B,Tx)` where `H` is the hidden size, `D` is 1 for unidirectional, 2 for\n",
    "# bidirectional, `B` is the batchsize, and `Tx` is the sequence length. It will be\n",
    "# convenient to store these values in batch major form for the attention mechanism, so\n",
    "# *values* in memory will be a permuted copy of the encoder output with size `(H*D,Tx,B)`\n",
    "# (see `@doc permutedims`). The *keys* in the memory need to have the same first dimension\n",
    "# as the *queries* (i.e. the decoder hidden states). So *values* will be transformed into\n",
    "# *keys* of size `(H,B,Tx)` with `keys = m.w * values` where `m::Memory` is the memory\n",
    "# layer. Note that you will have to do some reshaping to 2-D and back to 3-D for matrix\n",
    "# multiplications. Also note that `m.w` may be a scalar such as `1` e.g. when `D=1` and we\n",
    "# want keys and values to be identical.\n",
    "\n",
    "\n",
    "function (m::Memory)(x)\n",
    "    ## Your code here\n",
    "    H, B, Tx = size(x)\n",
    "    val = copy(x)\n",
    "    v = permutedims(val, (1, 3, 2))\n",
    "    k = mmul(m.w, v)\n",
    "    return k, v\n",
    "end\n",
    "\n",
    "# You can use the following helper function for scaling and linear transformations of 3-D tensors:\n",
    "mmul(w, x) = (w == 1 ? x :\n",
    " w == 0 ? 0 :\n",
    " reshape(w * reshape(x, size(x, 1), :), (:, size(x)[2:end]...)))\n",
    "\n",
    "#-\n",
    "@testset \"Testing memory\" begin\n",
    "    H, D, B, Tx = pretrained.encoder.hiddenSize,\n",
    "        pretrained.encoder.direction + 1,\n",
    "        4,\n",
    "        5\n",
    "    x = KnetArray(randn(Float32, H * D, B, Tx)) #!! GPU\n",
    "    #x = Array(randn(Float32, H * D, B, Tx))\n",
    "    k, v = pretrained.memory(x)\n",
    "    @test v == permutedims(x, (1, 3, 2))\n",
    "    @test k == mmul(pretrained.memory.w, v)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 3. Encoder\n",
    "#\n",
    "# `encode()` takes a model `s` and a source language minibatch `src`. It passes the input\n",
    "# through `s.srcembed` and `s.encoder` layers with the `s.encoder` RNN hidden states\n",
    "# initialized to `0` in the beginning, and copied to the `s.decoder` RNN at the end. The\n",
    "# steps so far are identical to `S2S_v1` but there is an extra step: The encoder output is\n",
    "# passed to the `s.memory` layer which returns a `(keys,values)` pair. `encode()` returns\n",
    "# this pair to be used later by the attention mechanism.\n",
    "\n",
    "function encode(s::S2S, src)\n",
    "    emb_out_src = s.srcembed(src)\n",
    "\n",
    "    s.encoder.h = 0\n",
    "    s.encoder.c = 0\n",
    "\n",
    "    y_enc = s.encoder(emb_out_src)\n",
    "\n",
    "    s.decoder.h = s.encoder.h\n",
    "    s.decoder.c = s.encoder.c\n",
    "       \n",
    "    return s.memory(y_enc)\n",
    "end\n",
    "\n",
    "#-\n",
    "# @testset \"Testing encoder\" begin\n",
    "#     src1, tgt1 = first(dtrn)\n",
    "#     key1, val1 = encode(pretrained, src1)\n",
    "#     H, D, B, Tx = pretrained.encoder.hiddenSize,\n",
    "#         pretrained.encoder.direction + 1,\n",
    "#         size(src1, 1),\n",
    "#         size(src1, 2)\n",
    "#     @test size(key1) == (H, Tx, B)\n",
    "#     @test size(val1) == (H * D, Tx, B)\n",
    "#     @test (pretrained.decoder.h, pretrained.decoder.c) === (\n",
    "#         pretrained.encoder.h,\n",
    "#         pretrained.encoder.c,\n",
    "#     )\n",
    "#     @test norm(key1) ≈ 1214.4755f0\n",
    "#     @test norm(val1) ≈ 191.10411f0\n",
    "#     @test norm(pretrained.decoder.h) ≈ 48.536964f0\n",
    "#     @test norm(pretrained.decoder.c) ≈ 391.69028f0\n",
    "# end\n",
    "\n",
    "# Tests were commented out since results were changed as a result of manipulating batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Part 4. Attention\n",
    "#\n",
    "# The attention layer takes `cell`: the decoder output, and `mem`: a pair of (keys,vals)\n",
    "# from the encoder, and computes and returns the attention vector. First `a.wquery` is used\n",
    "# to linearly transform the cell to the query tensor. The query tensor is reshaped and/or\n",
    "# permuted as appropriate and multiplied with the keys tensor to compute the attention\n",
    "# scores. Please see `@doc bmm` for the batched matrix multiply operation used for this\n",
    "# step. The attention scores are scaled using `a.scale` and normalized along the time\n",
    "# dimension using `softmax`. After the appropriate reshape and/or permutation, the scores\n",
    "# are multiplied with the `vals` tensor (using `bmm` again) to compute the context\n",
    "# tensor. After the appropriate reshape and/or permutation the context vector is\n",
    "# concatenated with the cell and linearly transformed to the attention vector using\n",
    "# `a.wattn`. Please see the paper and code examples for details.\n",
    "#\n",
    "# Note: the paper mentions a final `tanh` transform, however the final version of the\n",
    "# reference code does not use `tanh` and gets better results. Therefore we will skip `tanh`.\n",
    "\n",
    "#deccell(H,B,Ty), keys(H,Tx,B), vals(Dx*H,Tx,B) -> attnvec(H,B,Ty)\n",
    "function (a::Attention)(cell, mem)\n",
    "    H,B,Ty = size(cell)\n",
    "\n",
    "    qtensor = cell*a.wquery\n",
    "\n",
    "    qtensor = permutedims(qtensor, (3,1,2))\n",
    "\n",
    "    scores = bmm(qtensor,mem[1]) # Multiply with keys\n",
    "\n",
    "    scores=(a.scale[1])*scores\n",
    "    scores = softmax(scores,dims=2)\n",
    "\n",
    "    v= permutedims(mem[2],(2,1,3))\n",
    "    context = bmm(scores,v)\n",
    "\n",
    "\n",
    "    context = permutedims(context,(2,3,1))\n",
    "    context = cat(cell,context;dims=1)\n",
    "\n",
    "    context = reshape(context,(size(a.wattn)[2],:))\n",
    "    context = a.wattn*context\n",
    "    context = reshape(context,(H,B,Ty))\n",
    "end\n",
    "\n",
    "#-\n",
    "# @testset \"Testing attention\" begin\n",
    "#     src1, tgt1 = first(dtrn)\n",
    "#     key1, val1 = encode(pretrained, src1)\n",
    "#     H, B = pretrained.encoder.hiddenSize, size(src1, 1)\n",
    "#     Knet.seed!(1)\n",
    "#     x = KnetArray(randn(Float32, H, B, 5)) #!! GPU\n",
    "#     # x = Array(randn(Float32, H, B, 5))\n",
    "#     y = pretrained.attention(x, (key1, val1))\n",
    "#     @test size(y) == size(x)\n",
    "#     @test norm(y) ≈ 808.381f0\n",
    "# end\n",
    "\n",
    "# Tests were commented out since results were changed as a result of manipulating batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decode (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 5. Decoder\n",
    "#\n",
    "# `decode()` takes a model `s`, a target language minibatch `tgt`, the memory from the\n",
    "# encoder `mem` and the decoder output from the previous time step `prev`. After the input\n",
    "# is passed through the embedding layer, it is concatenated with `prev` (this is called\n",
    "# input feeding). The resulting tensor is passed through `s.decoder`. Finally the\n",
    "# `s.attention` layer takes the decoder output and the encoder memory to compute the\n",
    "# \"attention vector\" which is returned by `decode()`.\n",
    "\n",
    "function decode(s::S2S, tgt, mem, prev)\n",
    "    emb_out_tgt = s.tgtembed(tgt)\n",
    "\n",
    "    inputfeeding = cat(emb_out_tgt,prev;dims=1)\n",
    "    y_dec = s.decoder(inputfeeding)\n",
    "\n",
    "    attentionout = s.attention(y_dec,mem)\n",
    "end\n",
    "\n",
    "#-\n",
    "# @testset \"Testing decoder\" begin\n",
    "#     src1, tgt1 = first(dtrn)\n",
    "#     key1, val1 = encode(pretrained, src1)\n",
    "#     H, B = pretrained.encoder.hiddenSize, size(src1, 1)\n",
    "#     Knet.seed!(1)\n",
    "#     cell = randn!(similar(key1, size(key1, 1), size(key1, 3), 1))\n",
    "#     cell = decode(pretrained, tgt1[:, 1:1], (key1, val1), cell)\n",
    "#     @test size(cell) == (H, B, 1)\n",
    "#     @test norm(cell) ≈ 131.21631f0\n",
    "# end\n",
    "\n",
    "# Tests were commented out since results were changed as a result of manipulating batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Part 6. Loss\n",
    "#\n",
    "# The loss function takes source language minibatch `src`, and a target language minibatch\n",
    "# `tgt` and returns `sumloss/numwords` if `average=true` or `(sumloss,numwords)` if\n",
    "# `average=false` where `sumloss` is the total negative log likelihood loss and `numwords` is\n",
    "# the number of words predicted (including a final eos for each sentence). The source is first\n",
    "# encoded using `encode` yielding a `(keys,vals)` pair (memory). Then the decoder is called to\n",
    "# predict each word of `tgt` given the previous word, `(keys,vals)` pair, and the previous\n",
    "# decoder output. The previous decoder output is initialized with zeros for the first\n",
    "# step. The output of the decoder at each step is passed through the projection layer giving\n",
    "# word scores. Losses can be computed from word scores and masked/shifted `tgt`.\n",
    "\n",
    "function (s::S2S)(src, tgt; average = true)\n",
    "    ## Your code here\n",
    "    B,Ty = size(tgt)\n",
    "    Ty -=1\n",
    "\n",
    "    project = s.projection\n",
    "\n",
    "    mem = encode(s,src)\n",
    "    \n",
    "    prev = KnetArray(zeros(Float32,s.decoder.hiddenSize,B))\n",
    "\n",
    "    verify = deepcopy(tgt[:,2:end])\n",
    "    mask!(verify, s.tgtvocab.eos)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_word = 0\n",
    "        \n",
    "    for word_order in 1:Ty\n",
    "        step_wise_tgt = reshape(tgt[:,word_order],:,1)                \n",
    "        dec_state = decode(s,step_wise_tgt, mem, prev)\n",
    "        pred = project(reshape(dec_state, :, B))\n",
    "        \n",
    "        Δloss,Δword = nll(pred,verify[:,word_order];average=false)\n",
    "        total_loss += Δloss\n",
    "        total_word += Δword\n",
    "        \n",
    "        prev = dec_state\n",
    "    end\n",
    "\n",
    "    average && return total_loss*1.0/total_word\n",
    "    return total_loss,total_word\n",
    "end\n",
    "\n",
    "#-\n",
    "# @testset \"Testing loss\" begin\n",
    "#     src1, tgt1 = first(dtrn)\n",
    "#     @test pretrained(src1, tgt1) ≈ 1.4666592f0\n",
    "#     #@test pretrained(src1, tgt1, average = false) .≈ (1949.1901f0, 1329)\n",
    "# end\n",
    "\n",
    "# Tests were commented out since results were changed as a result of manipulating batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Part 7. Greedy translator\n",
    "#\n",
    "# An `S2S` object can be called with a single argument (source language minibatch `src`, with\n",
    "# size `B,Tx`) to generate translations (target language minibatch with size `B,Ty`). The\n",
    "# keyword argument `stopfactor` determines how much longer the output can be compared to the\n",
    "# input. Similar to the loss function, the source minibatch is encoded yield a `(keys,vals)`\n",
    "# pair (memory). We generate the output one time step at a time by calling the decoder with\n",
    "# the last output, the memory, and the last decoder state. The last output is initialized to\n",
    "# an array of `eos` tokens and the last decoder state is initialized to an array of\n",
    "# zeros. After computing the scores for the next word using the projection layer, the highest\n",
    "# scoring words are selected and appended to the output. The generation stops when all outputs\n",
    "# in the batch have generated `eos` or when the length of the output is `stopfactor` times the\n",
    "# input.\n",
    "\n",
    "function (s::S2S)(src; stopfactor = 3)\n",
    "    # Preperation for initial step\n",
    "    B,Tx = size(src)\n",
    "    max_step = stopfactor * Tx\n",
    "    tgt_eos = s.tgtvocab.eos\n",
    "\n",
    "    tgt = fill(tgt_eos, (B, 1))\n",
    "    output = Array{Int64}(undef, B, max_step)\n",
    "    prev = KnetArray(zeros(Float32,s.decoder.hiddenSize,B))\n",
    "\n",
    "    mem = encode(s,src)\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    eos_check = fill(1, (B, 1))\n",
    "    while step < max_step\n",
    "        step += 1\n",
    "\n",
    "        dec_state = decode(s,tgt,mem,prev)\n",
    "        pred = s.projection(reshape(dec_state, :, B))\n",
    "        prev = dec_state\n",
    "\n",
    "        eos_num = 0\n",
    "        for i = 1:B\n",
    "            index = findmax(pred[:,i])[2]\n",
    "            tgt[i] = index\n",
    "            if index == tgt_eos\n",
    "                eos_check[i] = 0\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Check here, whether it is a mandatory approach or\n",
    "        # anomally as a result of the implementation\n",
    "        output[:,step] = tgt.^eos_check\n",
    "        sum(eos_check) == 0 && break # all produced eos\n",
    "    end\n",
    "\n",
    "    return output[:, 1:step]\n",
    "end\n",
    "\n",
    "#-\n",
    "# @testset \"Testing translator\" begin\n",
    "#     src1, tgt1 = first(dtrn)\n",
    "#     tgt2 = pretrained(src1)\n",
    "#     @test size(tgt2) == (64, 41)\n",
    "#     @test tgt2[1:3, 1:3] == [14 25 10647; 37 25 1426; 27 5 349]\n",
    "# end\n",
    "\n",
    "# Tests were commented out since results were changed as a result of manipulating batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bidirectional, layers, hidden, srcembed, tgtembed, dropout, epochs, iters, bleu, save) = (true, 2, 512, 512, 512, 0.2, 10, 0, true, true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "┣                    ┫ [0.00%, 1/56220, 00:21/331:37:57, 21.24s/i] (dev = 9.805156f0, tst = (9.802532f0,), mem = 1.6130495f10)\n",
      "┣                    ┫ [0.36%, 205/56220, 01:29/06:46:00, 3.02i/s] (dev = 5.5788116f0, tst = (5.62553f0,), mem = 1.611702f10)\n",
      "┣▏                   ┫ [0.67%, 374/56220, 02:36/06:31:58, 2.50i/s] (dev = 5.305244f0, tst = (5.2683425f0,), mem = 1.6106878f10)\n",
      "┣▏                   ┫ [0.92%, 517/56220, 03:45/06:48:11, 2.08i/s] (dev = 5.0779047f0, tst = (5.0801716f0,), mem = 1.6107114f10)\n",
      "┣▏                   ┫ [1.22%, 686/56220, 04:53/06:40:33, 2.48i/s] (dev = 4.9481955f0, tst = (4.978831f0,), mem = 1.6113822f10)\n",
      "┣▎                   ┫ [1.52%, 856/56220, 06:02/06:36:27, 2.47i/s] (dev = 4.865971f0, tst = (4.900863f0,), mem = 1.6114912f10)\n",
      "┣▎                   ┫ [1.80%, 1011/56220, 07:22/06:49:50, 1.94i/s] (dev = 4.7848115f0, tst = (4.843001f0,), mem = 1.6128373f10)\n",
      "┣▍                   ┫ [2.09%, 1175/56220, 08:31/06:47:53, 2.37i/s] (dev = 4.7266383f0, tst = (4.7943034f0,), mem = 1.6103099f10)\n",
      "┣▍                   ┫ [2.37%, 1334/56220, 09:44/06:50:27, 2.18i/s] (dev = 4.6574674f0, tst = (4.667085f0,), mem = 1.6047518f10)\n",
      "┣▌                   ┫ [2.65%, 1489/56220, 10:52/06:50:04, 2.30i/s] (dev = 4.5909224f0, tst = (4.6260943f0,), mem = 1.6093765f10)\n",
      "┣▌                   ┫ [2.94%, 1654/56220, 12:00/06:47:56, 2.41i/s] (dev = 4.5205126f0, tst = (4.570972f0,), mem = 1.6101166f10)\n",
      "┣▋                   ┫ [3.24%, 1819/56220, 13:10/06:46:58, 2.36i/s] (dev = 4.4790754f0, tst = (4.5259104f0,), mem = 1.6103947f10)\n",
      "┣▋                   ┫ [3.51%, 1971/56220, 14:19/06:48:12, 2.22i/s] (dev = 4.432054f0, tst = (4.4848022f0,), mem = 1.6116485f10)\n",
      "┣▊                   ┫ [3.83%, 2154/56220, 15:27/06:43:20, 2.67i/s] (dev = 4.38435f0, tst = (4.421398f0,), mem = 1.6116001f10)\n",
      "┣▊                   ┫ [4.11%, 2313/56220, 16:34/06:42:49, 2.37i/s] (dev = 4.3394327f0, tst = (4.364574f0,), mem = 1.6105375f10)\n",
      "┣▉                   ┫ [4.41%, 2480/56220, 17:42/06:41:16, 2.47i/s] (dev = 4.275769f0, tst = (4.2917595f0,), mem = 1.6120744f10)\n",
      "┣▉                   ┫ [4.70%, 2640/56220, 18:50/06:41:11, 2.34i/s] (dev = 4.2558265f0, tst = (4.2821665f0,), mem = 1.6117826f10)\n",
      "┣▉                   ┫ [4.99%, 2808/56220, 19:58/06:39:36, 2.50i/s] (dev = 4.2246056f0, tst = (4.253237f0,), mem = 1.6107134f10)\n",
      "┣█                   ┫ [5.30%, 2980/56220, 21:05/06:37:44, 2.55i/s] (dev = 4.157138f0, tst = (4.1822433f0,), mem = 1.6110203f10)\n",
      "┣█                   ┫ [5.59%, 3143/56220, 22:12/06:37:11, 2.42i/s] (dev = 4.114557f0, tst = (4.1534867f0,), mem = 1.6128847f10)\n",
      "┣█▏                  ┫ [5.87%, 3302/56220, 23:21/06:37:38, 2.30i/s] (dev = 4.0511184f0, tst = (4.059096f0,), mem = 1.6117275f10)\n",
      "┣█▏                  ┫ [6.15%, 3458/56220, 24:30/06:38:21, 2.27i/s] (dev = 3.9995768f0, tst = (4.040358f0,), mem = 1.6108947f10)\n",
      "┣█▎                  ┫ [6.42%, 3610/56220, 25:38/06:39:04, 2.26i/s] (dev = 3.9686651f0, tst = (4.024059f0,), mem = 1.6097308f10)\n",
      "┣█▎                  ┫ [6.72%, 3779/56220, 26:45/06:37:54, 2.51i/s] (dev = 3.9236383f0, tst = (3.9424434f0,), mem = 1.6111138f10)\n",
      "┣█▍                  ┫ [7.00%, 3937/56220, 27:52/06:38:03, 2.33i/s] (dev = 3.861893f0, tst = (3.9176261f0,), mem = 1.6113098f10)\n",
      "┣█▍                  ┫ [7.27%, 4087/56220, 29:00/06:38:56, 2.22i/s] (dev = 3.799908f0, tst = (3.8299112f0,), mem = 1.6023907f10)\n",
      "┣█▌                  ┫ [7.58%, 4262/56220, 30:07/06:37:23, 2.59i/s] (dev = 3.77065f0, tst = (3.7893481f0,), mem = 1.6107525f10)\n",
      "┣█▌                  ┫ [7.87%, 4422/56220, 31:21/06:38:37, 2.17i/s] (dev = 3.6909568f0, tst = (3.7213945f0,), mem = 1.6129702f10)\n",
      "┣█▋                  ┫ [8.13%, 4573/56220, 32:29/06:39:17, 2.24i/s] (dev = 3.6535687f0, tst = (3.621326f0,), mem = 1.6091216f10)\n",
      "┣█▋                  ┫ [8.43%, 4737/56220, 33:44/06:40:20, 2.18i/s] (dev = 3.6170416f0, tst = (3.5731895f0,), mem = 1.6089532f10)\n",
      "┣█▋                  ┫ [8.72%, 4902/56220, 34:57/06:40:48, 2.26i/s] (dev = 3.6296015f0, tst = (3.5832515f0,), mem = 1.6069077f10)\n",
      "┣█▊                  ┫ [9.01%, 5064/56220, 36:05/06:40:32, 2.39i/s] (dev = 3.5465617f0, tst = (3.4778993f0,), mem = 1.6112615f10)\n",
      "┣█▊                  ┫ [9.29%, 5222/56220, 37:20/06:41:59, 2.09i/s] (dev = 3.5372026f0, tst = (3.4197454f0,), mem = 1.5993085f10)\n",
      "┣█▉                  ┫ [9.58%, 5386/56220, 38:29/06:41:40, 2.39i/s] (dev = 3.4862123f0, tst = (3.352587f0,), mem = 1.6124072f10)\n",
      "┣█▉                  ┫ [9.90%, 5568/56220, 39:36/06:39:53, 2.70i/s] (dev = 3.4552865f0, tst = (3.3106306f0,), mem = 1.6112135f10)\n",
      "┣██                  ┫ [10.23%, 5754/56220, 40:44/06:37:56, 2.76i/s] (dev = 3.4073071f0, tst = (3.289923f0,), mem = 1.6108631f10)\n",
      "┣██                  ┫ [10.50%, 5905/56220, 41:51/06:38:30, 2.23i/s] (dev = 3.4011006f0, tst = (3.2964134f0,), mem = 1.6008637f10)\n",
      "┣██▏                 ┫ [10.80%, 6072/56220, 42:59/06:37:56, 2.48i/s] (dev = 3.4012058f0, tst = (3.286823f0,), mem = 1.6118692f10)\n",
      "┣██▏                 ┫ [11.06%, 6216/56220, 44:06/06:38:52, 2.14i/s] (dev = 3.388939f0, tst = (3.3131495f0,), mem = 1.6101771f10)\n",
      "┣██▎                 ┫ [11.37%, 6390/56220, 45:13/06:37:50, 2.59i/s] (dev = 3.348964f0, tst = (3.2753572f0,), mem = 1.6025947f10)\n",
      "┣██▎                 ┫ [11.66%, 6557/56220, 46:38/06:39:52, 1.96i/s] (dev = 3.3196645f0, tst = (3.2491157f0,), mem = 1.6083798f10)\n",
      "┣██▍                 ┫ [11.91%, 6694/56220, 47:47/06:41:20, 1.99i/s] (dev = 3.2745907f0, tst = (3.1833007f0,), mem = 1.6112327f10)\n",
      "┣██▍                 ┫ [12.22%, 6870/56220, 48:54/06:40:13, 2.62i/s] (dev = 3.277171f0, tst = (3.1410456f0,), mem = 1.6095798f10)\n",
      "┣██▌                 ┫ [12.50%, 7028/56220, 50:02/06:40:12, 2.35i/s] (dev = 3.2509797f0, tst = (3.125796f0,), mem = 1.6103991f10)\n",
      "┣██▌                 ┫ [12.78%, 7184/56220, 51:10/06:40:21, 2.30i/s] (dev = 3.234125f0, tst = (3.0901544f0,), mem = 1.60944f10)\n",
      "┣██▌                 ┫ [13.08%, 7351/56220, 52:16/06:39:46, 2.50i/s] (dev = 3.216749f0, tst = (2.9838054f0,), mem = 1.610041f10)\n",
      "┣██▋                 ┫ [13.40%, 7532/56220, 53:30/06:39:17, 2.47i/s] (dev = 3.197312f0, tst = (2.99798f0,), mem = 1.604098f10)\n",
      "┣██▋                 ┫ [13.68%, 7690/56220, 54:43/06:40:03, 2.15i/s] (dev = 3.1977367f0, tst = (2.8760505f0,), mem = 1.6115869f10)\n",
      "┣██▊                 ┫ [13.96%, 7851/56220, 55:51/06:39:54, 2.39i/s] (dev = 3.161313f0, tst = (2.8959618f0,), mem = 1.608636f10)\n",
      "┣██▊                 ┫ [14.25%, 8014/56220, 56:59/06:39:47, 2.38i/s] (dev = 3.1512303f0, tst = (2.837752f0,), mem = 1.6116829f10)\n",
      "┣██▉                 ┫ [14.55%, 8182/56220, 58:06/06:39:16, 2.50i/s] (dev = 3.1293921f0, tst = (2.870159f0,), mem = 1.6093957f10)\n",
      "┣██▉                 ┫ [14.86%, 8356/56220, 59:13/06:38:26, 2.61i/s] (dev = 3.1140926f0, tst = (2.8294306f0,), mem = 1.6092129f10)\n",
      "┣███                 ┫ [15.20%, 8543/56220, 01:00:22/06:37:13, 2.73i/s] (dev = 3.0865793f0, tst = (2.7820697f0,), mem = 1.6107623f10)\n",
      "┣███                 ┫ [15.47%, 8697/56220, 01:01:36/06:38:09, 2.08i/s] (dev = 3.1021042f0, tst = (2.8144388f0,), mem = 1.6117821f10)\n",
      "┣███▏                ┫ [15.73%, 8846/56220, 01:02:43/06:38:38, 2.20i/s] (dev = 3.0837073f0, tst = (2.8191202f0,), mem = 1.6094223f10)\n",
      "┣███▏                ┫ [16.03%, 9013/56220, 01:03:52/06:38:25, 2.42i/s] (dev = 3.055901f0, tst = (2.7878447f0,), mem = 1.6112247f10)\n",
      "┣███▎                ┫ [16.29%, 9160/56220, 01:05:00/06:38:57, 2.17i/s] (dev = 3.038988f0, tst = (2.7884374f0,), mem = 1.6082518f10)\n",
      "┣███▎                ┫ [16.58%, 9320/56220, 01:06:25/06:40:39, 1.88i/s] (dev = 3.0183809f0, tst = (2.7847652f0,), mem = 1.6089984f10)\n",
      "┣███▎                ┫ [16.87%, 9486/56220, 01:07:33/06:40:22, 2.44i/s] (dev = 3.0245304f0, tst = (2.8059998f0,), mem = 1.6086674f10)\n",
      "┣███▍                ┫ [17.13%, 9633/56220, 01:08:58/06:42:30, 1.73i/s] (dev = 3.007433f0, tst = (2.798848f0,), mem = 1.6070447f10)\n",
      "┣███▍                ┫ [17.43%, 9800/56220, 01:10:05/06:42:05, 2.48i/s] (dev = 3.007503f0, tst = (2.8073258f0,), mem = 1.6103678f10)\n",
      "┣███▌                ┫ [17.73%, 9965/56220, 01:11:13/06:41:49, 2.43i/s] (dev = 2.9887717f0, tst = (2.678741f0,), mem = 1.6097746f10)\n",
      "┣███▌                ┫ [18.01%, 10124/56220, 01:12:22/06:41:53, 2.31i/s] (dev = 2.9682488f0, tst = (2.697358f0,), mem = 1.6108295f10)\n",
      "┣███▋                ┫ [18.27%, 10269/56220, 01:13:31/06:42:27, 2.12i/s] (dev = 2.963263f0, tst = (2.6420133f0,), mem = 1.6111967f10)\n",
      "┣███▋                ┫ [18.55%, 10429/56220, 01:14:46/06:43:05, 2.11i/s] (dev = 2.9526582f0, tst = (2.6004887f0,), mem = 1.6023773f10)\n",
      "┣███▊                ┫ [18.81%, 10577/56220, 01:15:55/06:43:29, 2.17i/s] (dev = 2.934745f0, tst = (2.5942593f0,), mem = 1.6121022f10)\n",
      "┣███▊                ┫ [19.12%, 10752/56220, 01:17:11/06:43:35, 2.29i/s] (dev = 2.9427238f0, tst = (2.6297302f0,), mem = 1.6090729f10)\n",
      "┣███▉                ┫ [19.42%, 10916/56220, 01:18:18/06:43:16, 2.45i/s] (dev = 2.9257772f0, tst = (2.6349347f0,), mem = 1.6094091f10)\n",
      "┣███▉                ┫ [19.73%, 11094/56220, 01:19:26/06:42:32, 2.62i/s] (dev = 2.9256842f0, tst = (2.6591277f0,), mem = 1.6094633f10)\n",
      "┣███▉                ┫ [19.99%, 11240/56220, 01:20:33/06:42:55, 2.17i/s] (dev = 2.9197412f0, tst = (2.6212063f0,), mem = 1.6078766f10)\n",
      "┣████                ┫ [20.31%, 11421/56220, 01:21:41/06:42:05, 2.68i/s] (dev = 2.9324582f0, tst = (2.5623174f0,), mem = 1.6123623f10)\n",
      "┣████                ┫ [20.60%, 11581/56220, 01:22:49/06:42:00, 2.36i/s] (dev = 2.9245162f0, tst = (2.5123818f0,), mem = 1.6095696f10)\n",
      "┣████▏               ┫ [20.89%, 11745/56220, 01:23:57/06:41:49, 2.41i/s] (dev = 2.9215226f0, tst = (2.4810688f0,), mem = 1.6097313f10)\n",
      "┣████▏               ┫ [21.19%, 11914/56220, 01:25:05/06:41:28, 2.49i/s] (dev = 2.9269187f0, tst = (2.505587f0,), mem = 1.6121632f10)\n",
      "┣████▎               ┫ [21.49%, 12079/56220, 01:26:14/06:41:20, 2.39i/s] (dev = 2.9197018f0, tst = (2.4783168f0,), mem = 1.6097964f10)\n",
      "┣████▎               ┫ [21.74%, 12222/56220, 01:27:21/06:41:46, 2.14i/s] (dev = 2.9171612f0, tst = (2.4760935f0,), mem = 1.6093083f10)\n",
      "┣████▍               ┫ [22.03%, 12388/56220, 01:28:29/06:41:32, 2.44i/s] (dev = 2.927253f0, tst = (2.506765f0,), mem = 1.6124873f10)\n",
      "┣████▍               ┫ [22.31%, 12541/56220, 01:29:37/06:41:44, 2.25i/s] (dev = 2.9096103f0, tst = (2.5064337f0,), mem = 1.6107284f10)\n",
      "┣████▌               ┫ [22.59%, 12698/56220, 01:30:44/06:41:44, 2.33i/s] (dev = 2.9124367f0, tst = (2.5054672f0,), mem = 1.6106178f10)\n",
      "┣████▌               ┫ [22.88%, 12863/56220, 01:31:51/06:41:28, 2.46i/s] (dev = 2.9014225f0, tst = (2.4533658f0,), mem = 1.6102381f10)\n",
      "┣████▋               ┫ [23.15%, 13016/56220, 01:32:59/06:41:37, 2.26i/s] (dev = 2.9029021f0, tst = (2.4726226f0,), mem = 1.6089623f10)\n",
      "┣████▋               ┫ [23.47%, 13195/56220, 01:34:07/06:40:58, 2.65i/s] (dev = 2.9029343f0, tst = (2.4739153f0,), mem = 1.6112537f10)\n",
      "┣████▋               ┫ [23.74%, 13349/56220, 01:35:25/06:41:49, 1.97i/s] (dev = 2.8853261f0, tst = (2.36085f0,), mem = 1.6127101f10)\n",
      "┣████▊               ┫ [24.01%, 13496/56220, 01:36:37/06:42:30, 2.02i/s] (dev = 2.888067f0, tst = (2.3877954f0,), mem = 1.6108076f10)\n",
      "┣████▊               ┫ [24.32%, 13674/56220, 01:37:44/06:41:51, 2.66i/s] (dev = 2.8975284f0, tst = (2.4232986f0,), mem = 1.608526f10)\n",
      "┣████▉               ┫ [24.66%, 13862/56220, 01:38:52/06:40:57, 2.79i/s] (dev = 2.8602746f0, tst = (2.4266303f0,), mem = 1.6107252f10)\n",
      "┣████▉               ┫ [24.93%, 14014/56220, 01:40:00/06:41:10, 2.23i/s] (dev = 2.8683643f0, tst = (2.4386353f0,), mem = 1.6119781f10)\n",
      "┣█████               ┫ [25.23%, 14184/56220, 01:41:07/06:40:48, 2.53i/s] (dev = 2.8566377f0, tst = (2.3826983f0,), mem = 1.6103732f10)\n",
      "┣█████               ┫ [25.53%, 14355/56220, 01:42:14/06:40:25, 2.55i/s] (dev = 2.8478644f0, tst = (2.3862145f0,), mem = 1.6026206f10)\n",
      "┣█████▏              ┫ [25.80%, 14505/56220, 01:43:22/06:40:39, 2.22i/s] (dev = 2.8379776f0, tst = (2.3899028f0,), mem = 1.6092485f10)\n",
      "┣█████▏              ┫ [26.05%, 14648/56220, 01:44:38/06:41:37, 1.87i/s] (dev = 2.8322165f0, tst = (2.3735023f0,), mem = 1.6125981f10)\n",
      "┣█████▎              ┫ [26.36%, 14817/56220, 01:45:48/06:41:28, 2.41i/s] (dev = 2.8353984f0, tst = (2.397077f0,), mem = 1.6121326f10)\n",
      "┣█████▎              ┫ [26.65%, 14981/56220, 01:46:56/06:41:18, 2.43i/s] (dev = 2.8324819f0, tst = (2.413491f0,), mem = 1.6087784f10)\n",
      "┣█████▍              ┫ [26.92%, 15134/56220, 01:48:03/06:41:25, 2.27i/s] (dev = 2.8155642f0, tst = (2.40208f0,), mem = 1.6097145f10)\n",
      "┣█████▍              ┫ [27.20%, 15291/56220, 01:49:12/06:41:29, 2.29i/s] (dev = 2.8092902f0, tst = (2.3794315f0,), mem = 1.6113469f10)\n",
      "┣█████▍              ┫ [27.49%, 15456/56220, 01:50:19/06:41:16, 2.46i/s] (dev = 2.8200366f0, tst = (2.3832698f0,), mem = 1.6107905f10)\n",
      "┣█████▌              ┫ [27.78%, 15617/56220, 01:51:26/06:41:10, 2.39i/s] (dev = 2.8072844f0, tst = (2.3953028f0,), mem = 1.6112554f10)\n",
      "┣█████▌              ┫ [28.10%, 15797/56220, 01:52:35/06:40:39, 2.63i/s] (dev = 2.8241358f0, tst = (2.3759499f0,), mem = 1.611686f10)\n",
      "┣█████▋              ┫ [28.37%, 15948/56220, 01:53:43/06:40:51, 2.22i/s] (dev = 2.7965376f0, tst = (2.3121686f0,), mem = 1.6118617f10)\n",
      "┣█████▋              ┫ [28.63%, 16098/56220, 01:54:50/06:41:02, 2.23i/s] (dev = 2.8087866f0, tst = (2.3303702f0,), mem = 1.6100568f10)\n",
      "┣█████▊              ┫ [28.92%, 16258/56220, 01:55:58/06:41:02, 2.34i/s] (dev = 2.7863524f0, tst = (2.3269403f0,), mem = 1.6102031f10)\n",
      "┣█████▊              ┫ [29.20%, 16415/56220, 01:57:06/06:41:03, 2.33i/s] (dev = 2.781118f0, tst = (2.3069906f0,), mem = 1.6125538f10)\n",
      "┣█████▉              ┫ [29.53%, 16603/56220, 01:58:13/06:40:20, 2.78i/s] (dev = 2.7784371f0, tst = (2.3106027f0,), mem = 1.6114461f10)\n",
      "┣█████▉              ┫ [29.79%, 16749/56220, 01:59:20/06:40:34, 2.19i/s] (dev = 2.7701452f0, tst = (2.2844405f0,), mem = 1.6099232f10)\n",
      "┣██████              ┫ [30.10%, 16922/56220, 02:00:42/06:40:59, 2.12i/s] (dev = 2.818562f0, tst = (2.2705011f0,), mem = 1.6118317f10)\n",
      "┣██████              ┫ [30.39%, 17085/56220, 02:01:51/06:40:59, 2.33i/s] (dev = 2.8163078f0, tst = (2.2704158f0,), mem = 1.6111674f10)\n",
      "┣██████▏             ┫ [30.68%, 17247/56220, 02:02:59/06:40:53, 2.40i/s] (dev = 2.8282518f0, tst = (2.2529097f0,), mem = 1.6112569f10)\n",
      "┣██████▏             ┫ [30.98%, 17416/56220, 02:04:09/06:40:45, 2.42i/s] (dev = 2.8310614f0, tst = (2.2353601f0,), mem = 1.6097297f10)\n",
      "┣██████▎             ┫ [31.25%, 17570/56220, 02:05:17/06:40:54, 2.24i/s] (dev = 2.8089225f0, tst = (2.1991649f0,), mem = 1.6109617f10)\n",
      "┣██████▎             ┫ [31.54%, 17731/56220, 02:06:45/06:41:53, 1.84i/s] (dev = 2.8169556f0, tst = (2.205007f0,), mem = 1.6116254f10)\n",
      "┣██████▎             ┫ [31.82%, 17889/56220, 02:07:53/06:41:53, 2.33i/s] (dev = 2.827007f0, tst = (2.235434f0,), mem = 1.6110963f10)\n",
      "┣██████▍             ┫ [32.10%, 18048/56220, 02:09:00/06:41:52, 2.35i/s] (dev = 2.8393784f0, tst = (2.2684956f0,), mem = 1.6102133f10)\n",
      "┣██████▍             ┫ [32.40%, 18215/56220, 02:10:08/06:41:38, 2.48i/s] (dev = 2.8300254f0, tst = (2.227546f0,), mem = 1.6104434f10)\n",
      "┣██████▌             ┫ [32.67%, 18365/56220, 02:11:35/06:42:49, 1.71i/s] (dev = 2.8214064f0, tst = (2.238517f0,), mem = 1.6105229f10)\n",
      "┣██████▌             ┫ [32.96%, 18528/56220, 02:12:43/06:42:42, 2.40i/s] (dev = 2.8254905f0, tst = (2.1846972f0,), mem = 1.6090995f10)\n",
      "┣██████▋             ┫ [33.24%, 18689/56220, 02:13:51/06:42:38, 2.38i/s] (dev = 2.8151155f0, tst = (2.21009f0,), mem = 1.6108874f10)\n",
      "┣██████▋             ┫ [33.53%, 18852/56220, 02:14:59/06:42:31, 2.40i/s] (dev = 2.8274229f0, tst = (2.2318544f0,), mem = 1.6110153f10)\n",
      "┣██████▊             ┫ [33.83%, 19022/56220, 02:16:15/06:42:41, 2.22i/s] (dev = 2.8114567f0, tst = (2.2421126f0,), mem = 1.611313f10)\n",
      "┣██████▊             ┫ [34.17%, 19210/56220, 02:17:22/06:42:00, 2.82i/s] (dev = 2.8165364f0, tst = (2.271556f0,), mem = 1.6091509f10)\n",
      "┣██████▉             ┫ [34.45%, 19366/56220, 02:18:30/06:42:05, 2.27i/s] (dev = 2.801297f0, tst = (2.2056334f0,), mem = 1.6130129f10)\n",
      "┣██████▉             ┫ [34.72%, 19519/56220, 02:19:39/06:42:13, 2.23i/s] (dev = 2.7928483f0, tst = (2.2203975f0,), mem = 1.6081117f10)\n",
      "┣███████             ┫ [35.03%, 19696/56220, 02:20:46/06:41:48, 2.63i/s] (dev = 2.8203013f0, tst = (2.2042034f0,), mem = 1.6109872f10)\n",
      "┣███████             ┫ [35.32%, 19855/56220, 02:21:55/06:41:50, 2.31i/s] (dev = 2.7996569f0, tst = (2.204436f0,), mem = 1.6129416f10)\n",
      "┣███████             ┫ [35.59%, 20009/56220, 02:23:02/06:41:54, 2.29i/s] (dev = 2.7847955f0, tst = (2.1725368f0,), mem = 1.6116397f10)\n",
      "┣███████▏            ┫ [35.89%, 20177/56220, 02:24:10/06:41:41, 2.49i/s] (dev = 2.7837284f0, tst = (2.1935017f0,), mem = 1.6094819f10)\n",
      "┣███████▏            ┫ [36.21%, 20355/56220, 02:25:18/06:41:19, 2.61i/s] (dev = 2.7852087f0, tst = (2.2027762f0,), mem = 1.6109193f10)\n",
      "┣███████▎            ┫ [36.46%, 20496/56220, 02:26:26/06:41:40, 2.07i/s] (dev = 2.775968f0, tst = (2.1468992f0,), mem = 1.6122482f10)\n",
      "┣███████▎            ┫ [36.75%, 20660/56220, 02:27:33/06:41:32, 2.43i/s] (dev = 2.7682765f0, tst = (2.1591663f0,), mem = 1.6103939f10)\n",
      "┣███████▍            ┫ [37.06%, 20836/56220, 02:28:42/06:41:13, 2.57i/s] (dev = 2.7847068f0, tst = (2.1918461f0,), mem = 1.6102355f10)\n",
      "┣███████▍            ┫ [37.36%, 21002/56220, 02:29:49/06:41:03, 2.46i/s] (dev = 2.7844856f0, tst = (2.107197f0,), mem = 1.6103515f10)\n",
      "┣███████▌            ┫ [37.63%, 21156/56220, 02:30:57/06:41:07, 2.29i/s] (dev = 2.7651203f0, tst = (2.11642f0,), mem = 1.610269f10)\n",
      "┣███████▌            ┫ [37.93%, 21324/56220, 02:32:04/06:40:55, 2.49i/s] (dev = 2.7646987f0, tst = (2.081876f0,), mem = 1.609477f10)\n",
      "┣███████▋            ┫ [38.18%, 21466/56220, 02:33:27/06:41:52, 1.72i/s] (dev = 2.7511783f0, tst = (2.0875907f0,), mem = 1.6121504f10)\n",
      "┣███████▋            ┫ [38.47%, 21626/56220, 02:34:34/06:41:50, 2.37i/s] (dev = 2.7564557f0, tst = (2.0720446f0,), mem = 1.6089202f10)\n",
      "┣███████▊            ┫ [38.77%, 21798/56220, 02:35:41/06:41:31, 2.59i/s] (dev = 2.7451172f0, tst = (2.0795968f0,), mem = 1.6103289f10)\n",
      "┣███████▊            ┫ [39.12%, 21996/56220, 02:36:47/06:40:44, 2.97i/s] (dev = 2.7478607f0, tst = (2.0116096f0,), mem = 1.6090143f10)\n",
      "┣███████▉            ┫ [39.38%, 22140/56220, 02:37:56/06:41:03, 2.08i/s] (dev = 2.741191f0, tst = (2.027113f0,), mem = 1.6108639f10)\n",
      "┣███████▉            ┫ [39.66%, 22295/56220, 02:39:04/06:41:06, 2.30i/s] (dev = 2.75184f0, tst = (2.0391474f0,), mem = 1.6102515f10)\n",
      "┣███████▉            ┫ [39.95%, 22460/56220, 02:40:16/06:41:10, 2.29i/s] (dev = 2.733232f0, tst = (2.0125299f0,), mem = 1.6101992f10)\n",
      "┣████████            ┫ [40.22%, 22610/56220, 02:41:23/06:41:18, 2.22i/s] (dev = 2.781928f0, tst = (2.0100627f0,), mem = 1.6092403f10)\n",
      "┣████████            ┫ [40.51%, 22775/56220, 02:42:31/06:41:11, 2.43i/s] (dev = 2.804911f0, tst = (2.0049987f0,), mem = 1.6089959f10)\n",
      "┣████████▏           ┫ [40.79%, 22930/56220, 02:43:39/06:41:15, 2.28i/s] (dev = 2.7938063f0, tst = (1.9399526f0,), mem = 1.6123631f10)\n",
      "┣████████▏           ┫ [41.07%, 23087/56220, 02:44:48/06:41:19, 2.28i/s] (dev = 2.8273437f0, tst = (1.988971f0,), mem = 1.6108457f10)\n",
      "┣████████▎           ┫ [41.35%, 23246/56220, 02:45:56/06:41:18, 2.35i/s] (dev = 2.8216147f0, tst = (1.9838289f0,), mem = 1.6110318f10)\n",
      "┣████████▎           ┫ [41.63%, 23407/56220, 02:47:04/06:41:16, 2.37i/s] (dev = 2.8263488f0, tst = (1.9968963f0,), mem = 1.6104658f10)\n",
      "┣████████▍           ┫ [41.92%, 23570/56220, 02:48:15/06:41:20, 2.28i/s] (dev = 2.8168736f0, tst = (2.0060399f0,), mem = 1.6101454f10)\n",
      "┣████████▍           ┫ [42.20%, 23727/56220, 02:49:24/06:41:22, 2.30i/s] (dev = 2.8269756f0, tst = (1.9683849f0,), mem = 1.6118368f10)\n",
      "┣████████▍           ┫ [42.48%, 23882/56220, 02:50:32/06:41:28, 2.25i/s] (dev = 2.8297005f0, tst = (1.9899533f0,), mem = 1.6127158f10)\n",
      "┣████████▌           ┫ [42.74%, 24029/56220, 02:51:40/06:41:38, 2.18i/s] (dev = 2.816502f0, tst = (1.9867263f0,), mem = 1.610623f10)\n",
      "┣████████▌           ┫ [43.05%, 24204/56220, 02:52:48/06:41:22, 2.56i/s] (dev = 2.8237414f0, tst = (2.024307f0,), mem = 1.612153f10)\n",
      "┣████████▋           ┫ [43.35%, 24371/56220, 02:53:56/06:41:15, 2.45i/s] (dev = 2.8216267f0, tst = (2.032739f0,), mem = 1.6123333f10)\n",
      "┣████████▋           ┫ [43.64%, 24536/56220, 02:55:03/06:41:05, 2.48i/s] (dev = 2.8174403f0, tst = (2.0524228f0,), mem = 1.6119144f10)\n",
      "┣████████▊           ┫ [43.99%, 24729/56220, 02:56:10/06:40:29, 2.89i/s] (dev = 2.8232627f0, tst = (2.0208879f0,), mem = 1.6115633f10)\n",
      "┣████████▊           ┫ [44.25%, 24880/56220, 02:57:18/06:40:37, 2.22i/s] (dev = 2.8121762f0, tst = (1.9626073f0,), mem = 1.6093174f10)\n",
      "┣████████▉           ┫ [44.54%, 25043/56220, 02:58:26/06:40:34, 2.38i/s] (dev = 2.8199575f0, tst = (1.9816895f0,), mem = 1.611936f10)\n",
      "┣████████▉           ┫ [44.84%, 25210/56220, 02:59:34/06:40:27, 2.46i/s] (dev = 2.8188584f0, tst = (2.0093722f0,), mem = 1.6110337f10)\n",
      "┣█████████           ┫ [45.13%, 25372/56220, 03:00:41/06:40:23, 2.40i/s] (dev = 2.8219736f0, tst = (1.9840808f0,), mem = 1.6109682f10)\n",
      "┣█████████           ┫ [45.43%, 25543/56220, 03:02:01/06:40:37, 2.15i/s] (dev = 2.8055832f0, tst = (2.0059087f0,), mem = 1.6131636f10)\n",
      "┣█████████▏          ┫ [45.72%, 25702/56220, 03:03:10/06:40:38, 2.32i/s] (dev = 2.8020308f0, tst = (1.982491f0,), mem = 1.6094286f10)\n",
      "┣█████████▏          ┫ [46.00%, 25859/56220, 03:04:17/06:40:40, 2.32i/s] (dev = 2.8069408f0, tst = (2.0027368f0,), mem = 1.6119194f10)\n",
      "┣█████████▎          ┫ [46.29%, 26023/56220, 03:05:25/06:40:35, 2.41i/s] (dev = 2.795899f0, tst = (2.0020723f0,), mem = 1.6094707f10)\n",
      "┣█████████▎          ┫ [46.60%, 26199/56220, 03:06:34/06:40:20, 2.58i/s] (dev = 2.7932954f0, tst = (1.9157115f0,), mem = 1.6098409f10)\n",
      "┣█████████▍          ┫ [46.89%, 26361/56220, 03:08:00/06:40:58, 1.87i/s] (dev = 2.7883534f0, tst = (1.9324464f0,), mem = 1.6098215f10)\n",
      "┣█████████▍          ┫ [47.17%, 26520/56220, 03:09:09/06:40:59, 2.32i/s] (dev = 2.7811432f0, tst = (1.9283655f0,), mem = 1.6104596f10)\n",
      "┣█████████▍          ┫ [47.45%, 26679/56220, 03:10:17/06:40:58, 2.35i/s] (dev = 2.787373f0, tst = (1.9438876f0,), mem = 1.6091096f10)\n",
      "┣█████████▌          ┫ [47.74%, 26838/56220, 03:11:26/06:41:00, 2.30i/s] (dev = 2.7800407f0, tst = (1.9710146f0,), mem = 1.6100775f10)\n",
      "┣█████████▌          ┫ [48.00%, 26987/56220, 03:12:34/06:41:11, 2.17i/s] (dev = 2.7679493f0, tst = (1.8563514f0,), mem = 1.6080727f10)\n",
      "┣█████████▋          ┫ [48.30%, 27152/56220, 03:13:42/06:41:04, 2.45i/s] (dev = 2.7729902f0, tst = (1.8381618f0,), mem = 1.6103272f10)\n",
      "┣█████████▋          ┫ [48.58%, 27312/56220, 03:14:49/06:41:00, 2.40i/s] (dev = 2.7694955f0, tst = (1.8641278f0,), mem = 1.611267f10)\n",
      "┣█████████▊          ┫ [48.94%, 27514/56220, 03:15:58/06:40:25, 2.93i/s] (dev = 2.7729554f0, tst = (1.8745161f0,), mem = 1.6099027f10)\n",
      "┣█████████▊          ┫ [49.20%, 27662/56220, 03:17:07/06:40:37, 2.14i/s] (dev = 2.7650652f0, tst = (1.8657866f0,), mem = 1.610455f10)\n",
      "┣█████████▉          ┫ [49.50%, 27828/56220, 03:18:14/06:40:30, 2.46i/s] (dev = 2.7818239f0, tst = (1.8705801f0,), mem = 1.61033f10)\n",
      "┣█████████▉          ┫ [49.77%, 27983/56220, 03:19:34/06:40:56, 1.95i/s] (dev = 2.7616973f0, tst = (1.8396631f0,), mem = 1.6132206f10)\n",
      "┣██████████          ┫ [50.06%, 28143/56220, 03:20:41/06:40:53, 2.39i/s] (dev = 2.8221285f0, tst = (1.8058072f0,), mem = 1.6093676f10)\n",
      "┣██████████          ┫ [50.34%, 28301/56220, 03:21:48/06:40:53, 2.33i/s] (dev = 2.8295903f0, tst = (1.8107849f0,), mem = 1.6117292f10)\n",
      "┣██████████▏         ┫ [50.65%, 28473/56220, 03:23:06/06:41:01, 2.21i/s] (dev = 2.8564858f0, tst = (1.7936196f0,), mem = 1.6127987f10)\n",
      "┣██████████▏         ┫ [50.90%, 28617/56220, 03:24:14/06:41:14, 2.11i/s] (dev = 2.8439784f0, tst = (1.8090419f0,), mem = 1.6088535f10)\n",
      "┣██████████▏         ┫ [51.17%, 28768/56220, 03:25:23/06:41:23, 2.18i/s] (dev = 2.8453777f0, tst = (1.7861314f0,), mem = 1.6081855f10)\n",
      "┣██████████▎         ┫ [51.46%, 28933/56220, 03:26:44/06:41:42, 2.05i/s] (dev = 2.8474965f0, tst = (1.8099409f0,), mem = 1.6075511f10)\n",
      "┣██████████▎         ┫ [51.74%, 29089/56220, 03:28:00/06:41:59, 2.06i/s] (dev = 2.8392277f0, tst = (1.8285358f0,), mem = 1.6107004f10)\n",
      "┣██████████▍         ┫ [51.99%, 29228/56220, 03:29:08/06:42:17, 2.02i/s] (dev = 2.844065f0, tst = (1.8396232f0,), mem = 1.6107204f10)\n",
      "┣██████████▍         ┫ [52.26%, 29378/56220, 03:30:16/06:42:23, 2.22i/s] (dev = 2.857413f0, tst = (1.7794027f0,), mem = 1.6113471f10)\n",
      "┣██████████▌         ┫ [52.54%, 29539/56220, 03:31:26/06:42:25, 2.29i/s] (dev = 2.8549745f0, tst = (1.7981569f0,), mem = 1.6091781f10)\n",
      "┣██████████▌         ┫ [52.82%, 29693/56220, 03:32:34/06:42:27, 2.29i/s] (dev = 2.854894f0, tst = (1.8093151f0,), mem = 1.607836f10)\n",
      "┣██████████▌         ┫ [53.09%, 29849/56220, 03:33:40/06:42:27, 2.34i/s] (dev = 2.8616047f0, tst = (1.7987188f0,), mem = 1.6098362f10)\n",
      "┣██████████▋         ┫ [53.40%, 30023/56220, 03:34:47/06:42:12, 2.60i/s] (dev = 2.8388925f0, tst = (1.8150777f0,), mem = 1.6106772f10)\n",
      "┣██████████▋         ┫ [53.73%, 30205/56220, 03:35:57/06:41:56, 2.62i/s] (dev = 2.858184f0, tst = (1.8400645f0,), mem = 1.6110864f10)\n",
      "┣██████████▊         ┫ [54.00%, 30356/56220, 03:37:04/06:42:01, 2.25i/s] (dev = 2.8603487f0, tst = (1.8307586f0,), mem = 1.6090439f10)\n",
      "┣██████████▊         ┫ [54.30%, 30528/56220, 03:38:12/06:41:51, 2.52i/s] (dev = 2.8524508f0, tst = (1.7822165f0,), mem = 1.6115983f10)\n",
      "┣██████████▉         ┫ [54.59%, 30692/56220, 03:39:20/06:41:46, 2.42i/s] (dev = 2.8534865f0, tst = (1.7903695f0,), mem = 1.6083334f10)\n",
      "┣██████████▉         ┫ [54.88%, 30854/56220, 03:40:27/06:41:42, 2.40i/s] (dev = 2.8545802f0, tst = (1.7812403f0,), mem = 1.6116645f10)\n",
      "┣███████████         ┫ [55.19%, 31030/56220, 03:41:35/06:41:27, 2.62i/s] (dev = 2.8432364f0, tst = (1.7661189f0,), mem = 1.6101786f10)\n",
      "┣███████████         ┫ [55.46%, 31181/56220, 03:42:43/06:41:33, 2.21i/s] (dev = 2.8504462f0, tst = (1.742058f0,), mem = 1.6117085f10)\n",
      "┣███████████▏        ┫ [55.75%, 31340/56220, 03:43:51/06:41:33, 2.34i/s] (dev = 2.8509066f0, tst = (1.7586429f0,), mem = 1.6108682f10)\n",
      "┣███████████▏        ┫ [56.01%, 31487/56220, 03:44:59/06:41:42, 2.17i/s] (dev = 2.841266f0, tst = (1.7348632f0,), mem = 1.6122131f10)\n",
      "┣███████████▎        ┫ [56.29%, 31649/56220, 03:46:06/06:41:38, 2.41i/s] (dev = 2.8312166f0, tst = (1.7655191f0,), mem = 1.6113361f10)\n",
      "┣███████████▎        ┫ [56.61%, 31825/56220, 03:47:14/06:41:25, 2.59i/s] (dev = 2.8261068f0, tst = (1.7664545f0,), mem = 1.6117055f10)\n",
      "┣███████████▍        ┫ [56.91%, 31996/56220, 03:48:30/06:41:30, 2.25i/s] (dev = 2.8339338f0, tst = (1.7252371f0,), mem = 1.6124762f10)\n",
      "┣███████████▍        ┫ [57.20%, 32159/56220, 03:49:38/06:41:27, 2.39i/s] (dev = 2.819213f0, tst = (1.7383678f0,), mem = 1.6103884f10)\n",
      "┣███████████▍        ┫ [57.49%, 32320/56220, 03:50:46/06:41:25, 2.37i/s] (dev = 2.8242748f0, tst = (1.753188f0,), mem = 1.6099097f10)\n",
      "┣███████████▌        ┫ [57.82%, 32507/56220, 03:51:54/06:41:05, 2.73i/s] (dev = 2.8529136f0, tst = (1.7830143f0,), mem = 1.6107703f10)\n",
      "┣███████████▋        ┫ [58.13%, 32680/56220, 03:53:03/06:40:55, 2.53i/s] (dev = 2.8378801f0, tst = (1.7660419f0,), mem = 1.6100899f10)\n",
      "┣███████████▋        ┫ [58.46%, 32867/56220, 03:54:09/06:40:32, 2.81i/s] (dev = 2.836493f0, tst = (1.7459161f0,), mem = 1.6089962f10)\n",
      "┣███████████▊        ┫ [58.77%, 33038/56220, 03:55:37/06:40:57, 1.94i/s] (dev = 2.8232677f0, tst = (1.7725742f0,), mem = 1.6063657f10)\n",
      "┣███████████▊        ┫ [59.06%, 33203/56220, 03:56:45/06:40:52, 2.44i/s] (dev = 2.8315148f0, tst = (1.7885592f0,), mem = 1.6102579f10)\n",
      "┣███████████▉        ┫ [59.38%, 33382/56220, 03:57:52/06:40:37, 2.66i/s] (dev = 2.821732f0, tst = (1.7722528f0,), mem = 1.6107308f10)\n",
      "┣███████████▉        ┫ [59.67%, 33549/56220, 03:59:00/06:40:31, 2.46i/s] (dev = 2.7999768f0, tst = (1.7880545f0,), mem = 1.6091253f10)\n",
      "┣███████████▉        ┫ [59.95%, 33703/56220, 04:00:07/06:40:33, 2.29i/s] (dev = 2.808905f0, tst = (1.7459854f0,), mem = 1.6096227f10)\n",
      "┣████████████        ┫ [60.24%, 33868/56220, 04:01:15/06:40:28, 2.44i/s] (dev = 2.884953f0, tst = (1.5890927f0,), mem = 1.6096884f10)\n",
      "┣████████████        ┫ [60.53%, 34032/56220, 04:02:33/06:40:42, 2.09i/s] (dev = 2.8980565f0, tst = (1.6100826f0,), mem = 1.6105398f10)\n",
      "┣████████████▏       ┫ [60.82%, 34192/56220, 04:03:41/06:40:40, 2.38i/s] (dev = 2.9083757f0, tst = (1.6280732f0,), mem = 1.6091657f10)\n",
      "┣████████████▏       ┫ [61.10%, 34349/56220, 04:04:47/06:40:39, 2.35i/s] (dev = 2.900957f0, tst = (1.583011f0,), mem = 1.6091411f10)\n",
      "┣████████████▎       ┫ [61.37%, 34505/56220, 04:05:55/06:40:41, 2.31i/s] (dev = 2.9284086f0, tst = (1.54391f0,), mem = 1.6093634f10)\n",
      "┣████████████▎       ┫ [61.66%, 34664/56220, 04:07:03/06:40:41, 2.33i/s] (dev = 2.9336345f0, tst = (1.5687213f0,), mem = 1.6099943f10)\n",
      "┣████████████▍       ┫ [61.95%, 34831/56220, 04:08:13/06:40:39, 2.39i/s] (dev = 2.9133537f0, tst = (1.5201682f0,), mem = 1.611161f10)\n",
      "┣████████████▍       ┫ [62.26%, 35001/56220, 04:09:21/06:40:32, 2.49i/s] (dev = 2.9224498f0, tst = (1.5533034f0,), mem = 1.6120582f10)\n",
      "┣████████████▌       ┫ [62.55%, 35164/56220, 04:10:29/06:40:28, 2.42i/s] (dev = 2.9289262f0, tst = (1.5236177f0,), mem = 1.6096946f10)\n",
      "┣████████████▌       ┫ [62.85%, 35334/56220, 04:11:36/06:40:20, 2.52i/s] (dev = 2.9202163f0, tst = (1.5310682f0,), mem = 1.6097094f10)\n",
      "┣████████████▌       ┫ [63.12%, 35487/56220, 04:12:44/06:40:23, 2.26i/s] (dev = 2.9154289f0, tst = (1.5531588f0,), mem = 1.6092741f10)\n",
      "┣████████████▋       ┫ [63.43%, 35663/56220, 04:13:50/06:40:09, 2.65i/s] (dev = 2.9103045f0, tst = (1.5751458f0,), mem = 1.6106895f10)\n",
      "┣████████████▋       ┫ [63.73%, 35828/56220, 04:14:58/06:40:05, 2.43i/s] (dev = 2.9143102f0, tst = (1.5687543f0,), mem = 1.6094958f10)\n",
      "┣████████████▊       ┫ [64.00%, 35981/56220, 04:16:19/06:40:30, 1.89i/s] (dev = 2.9317608f0, tst = (1.5903504f0,), mem = 1.6117101f10)\n",
      "┣████████████▊       ┫ [64.27%, 36130/56220, 04:17:31/06:40:42, 2.08i/s] (dev = 2.9063902f0, tst = (1.5977267f0,), mem = 1.6105868f10)\n",
      "┣████████████▉       ┫ [64.53%, 36279/56220, 04:18:38/06:40:48, 2.21i/s] (dev = 2.9025176f0, tst = (1.5934261f0,), mem = 1.610245f10)\n",
      "┣████████████▉       ┫ [64.81%, 36435/56220, 04:19:47/06:40:52, 2.25i/s] (dev = 2.90439f0, tst = (1.6190109f0,), mem = 1.6114291f10)\n",
      "┣█████████████       ┫ [65.11%, 36607/56220, 04:20:55/06:40:42, 2.56i/s] (dev = 2.9233465f0, tst = (1.5340106f0,), mem = 1.6082425f10)\n",
      "┣█████████████       ┫ [65.41%, 36771/56220, 04:22:02/06:40:37, 2.44i/s] (dev = 2.9055536f0, tst = (1.5332924f0,), mem = 1.6105975f10)\n",
      "┣█████████████▏      ┫ [65.70%, 36937/56220, 04:23:10/06:40:33, 2.43i/s] (dev = 2.894014f0, tst = (1.5526172f0,), mem = 1.6114066f10)\n",
      "┣█████████████▏      ┫ [65.99%, 37099/56220, 04:24:18/06:40:32, 2.37i/s] (dev = 2.9071524f0, tst = (1.5736086f0,), mem = 1.6099887f10)\n",
      "┣█████████████▎      ┫ [66.29%, 37268/56220, 04:25:27/06:40:26, 2.46i/s] (dev = 2.895094f0, tst = (1.5601057f0,), mem = 1.6117096f10)\n",
      "┣█████████████▎      ┫ [66.58%, 37431/56220, 04:26:48/06:40:44, 2.00i/s] (dev = 2.8848784f0, tst = (1.5813473f0,), mem = 1.6118285f10)\n",
      "┣█████████████▎      ┫ [66.83%, 37571/56220, 04:27:57/06:40:56, 2.06i/s] (dev = 2.882442f0, tst = (1.5918332f0,), mem = 1.6080829f10)\n",
      "┣█████████████▍      ┫ [67.12%, 37735/56220, 04:29:14/06:41:07, 2.12i/s] (dev = 2.881216f0, tst = (1.6109649f0,), mem = 1.612281f10)\n",
      "┣█████████████▍      ┫ [67.40%, 37895/56220, 04:30:23/06:41:08, 2.31i/s] (dev = 2.8892198f0, tst = (1.6316005f0,), mem = 1.6104184f10)\n",
      "┣█████████████▌      ┫ [67.71%, 38065/56220, 04:31:30/06:41:00, 2.53i/s] (dev = 2.8897216f0, tst = (1.6355313f0,), mem = 1.6093466f10)\n",
      "┣█████████████▌      ┫ [68.01%, 38235/56220, 04:32:37/06:40:51, 2.55i/s] (dev = 2.88784f0, tst = (1.5934683f0,), mem = 1.6101612f10)\n",
      "┣█████████████▋      ┫ [68.38%, 38445/56220, 04:33:48/06:40:23, 2.98i/s] (dev = 2.8893862f0, tst = (1.5811503f0,), mem = 1.6105292f10)\n",
      "┣█████████████▋      ┫ [68.65%, 38597/56220, 04:34:55/06:40:27, 2.24i/s] (dev = 2.8809505f0, tst = (1.5879034f0,), mem = 1.6100791f10)\n",
      "┣█████████████▊      ┫ [68.96%, 38768/56220, 04:36:04/06:40:20, 2.50i/s] (dev = 2.8809474f0, tst = (1.5552695f0,), mem = 1.6111234f10)\n",
      "┣█████████████▊      ┫ [69.23%, 38923/56220, 04:37:11/06:40:22, 2.30i/s] (dev = 2.8732517f0, tst = (1.5604719f0,), mem = 1.6112461f10)\n",
      "┣█████████████▉      ┫ [69.52%, 39082/56220, 04:38:25/06:40:30, 2.16i/s] (dev = 2.8690586f0, tst = (1.5874316f0,), mem = 1.6098441f10)\n",
      "┣█████████████▉      ┫ [69.80%, 39243/56220, 04:39:33/06:40:29, 2.36i/s] (dev = 2.8666365f0, tst = (1.5795059f0,), mem = 1.6096905f10)\n",
      "┣██████████████      ┫ [70.09%, 39405/56220, 04:40:44/06:40:32, 2.28i/s] (dev = 2.921168f0, tst = (1.5850656f0,), mem = 1.6106952f10)\n",
      "┣██████████████      ┫ [70.39%, 39575/56220, 04:41:59/06:40:36, 2.26i/s] (dev = 2.957995f0, tst = (1.5448382f0,), mem = 1.6119084f10)\n",
      "┣██████████████▏     ┫ [70.67%, 39731/56220, 04:43:09/06:40:40, 2.24i/s] (dev = 2.957477f0, tst = (1.5542545f0,), mem = 1.6085306f10)\n",
      "┣██████████████▏     ┫ [70.96%, 39893/56220, 04:44:16/06:40:37, 2.41i/s] (dev = 2.9725337f0, tst = (1.579987f0,), mem = 1.6087916f10)\n",
      "┣██████████████▏     ┫ [71.24%, 40051/56220, 04:45:24/06:40:37, 2.35i/s] (dev = 2.9643123f0, tst = (1.5763795f0,), mem = 1.6095838f10)\n",
      "┣██████████████▎     ┫ [71.51%, 40203/56220, 04:46:31/06:40:40, 2.26i/s] (dev = 2.9769444f0, tst = (1.5733974f0,), mem = 1.6107121f10)\n",
      "┣██████████████▎     ┫ [71.78%, 40355/56220, 04:47:38/06:40:43, 2.25i/s] (dev = 2.9652088f0, tst = (1.5957444f0,), mem = 1.6094729f10)\n",
      "┣██████████████▍     ┫ [72.03%, 40498/56220, 04:48:46/06:40:52, 2.13i/s] (dev = 2.971294f0, tst = (1.6022118f0,), mem = 1.6104449f10)\n",
      "┣██████████████▍     ┫ [72.32%, 40661/56220, 04:49:53/06:40:49, 2.41i/s] (dev = 2.9877622f0, tst = (1.557363f0,), mem = 1.6106295f10)\n",
      "┣██████████████▌     ┫ [72.60%, 40817/56220, 04:51:01/06:40:51, 2.30i/s] (dev = 2.9579325f0, tst = (1.4341924f0,), mem = 1.6117212f10)\n",
      "┣██████████████▌     ┫ [72.94%, 41005/56220, 04:52:09/06:40:33, 2.79i/s] (dev = 2.9911158f0, tst = (1.4631685f0,), mem = 1.6126018f10)\n",
      "┣██████████████▋     ┫ [73.24%, 41178/56220, 04:53:18/06:40:26, 2.50i/s] (dev = 2.9999704f0, tst = (1.460785f0,), mem = 1.6118341f10)\n",
      "┣██████████████▋     ┫ [73.53%, 41338/56220, 04:54:34/06:40:37, 2.09i/s] (dev = 2.983202f0, tst = (1.4623461f0,), mem = 1.6123146f10)\n",
      "┣██████████████▊     ┫ [73.83%, 41506/56220, 04:55:57/06:40:51, 2.04i/s] (dev = 2.965282f0, tst = (1.4352484f0,), mem = 1.6091156f10)\n",
      "┣██████████████▊     ┫ [74.12%, 41672/56220, 04:57:04/06:40:47, 2.44i/s] (dev = 2.972431f0, tst = (1.425852f0,), mem = 1.6088192f10)\n",
      "┣██████████████▉     ┫ [74.43%, 41844/56220, 04:58:12/06:40:39, 2.54i/s] (dev = 2.9705603f0, tst = (1.4537593f0,), mem = 1.6083278f10)\n",
      "┣██████████████▉     ┫ [74.70%, 41998/56220, 04:59:19/06:40:41, 2.30i/s] (dev = 2.9620175f0, tst = (1.46815f0,), mem = 1.609343f10)\n",
      "┣██████████████▉     ┫ [74.98%, 42152/56220, 05:00:28/06:40:44, 2.26i/s] (dev = 3.0000556f0, tst = (1.4480621f0,), mem = 1.6108763f10)\n",
      "┣███████████████     ┫ [75.26%, 42309/56220, 05:01:35/06:40:45, 2.33i/s] (dev = 2.9606838f0, tst = (1.4249337f0,), mem = 1.6095306f10)\n",
      "┣███████████████     ┫ [75.56%, 42479/56220, 05:02:43/06:40:38, 2.50i/s] (dev = 2.9819877f0, tst = (1.4407023f0,), mem = 1.6094714f10)\n",
      "┣███████████████▏    ┫ [75.84%, 42635/56220, 05:03:52/06:40:42, 2.26i/s] (dev = 2.9702165f0, tst = (1.453639f0,), mem = 1.6114251f10)\n",
      "┣███████████████▏    ┫ [76.13%, 42803/56220, 05:05:00/06:40:37, 2.46i/s] (dev = 2.9612978f0, tst = (1.4638629f0,), mem = 1.6079693f10)\n",
      "┣███████████████▎    ┫ [76.43%, 42968/56220, 05:06:08/06:40:33, 2.43i/s] (dev = 2.9705133f0, tst = (1.4686238f0,), mem = 1.6110866f10)\n",
      "┣███████████████▎    ┫ [76.69%, 43114/56220, 05:07:16/06:40:40, 2.16i/s] (dev = 2.9475338f0, tst = (1.4782183f0,), mem = 1.609266f10)\n",
      "┣███████████████▍    ┫ [77.00%, 43292/56220, 05:08:25/06:40:31, 2.57i/s] (dev = 2.969418f0, tst = (1.4648618f0,), mem = 1.6104156f10)\n",
      "┣███████████████▍    ┫ [77.31%, 43461/56220, 05:09:47/06:40:43, 2.07i/s] (dev = 2.9513154f0, tst = (1.4741895f0,), mem = 1.6104903f10)\n",
      "┣███████████████▌    ┫ [77.60%, 43624/56220, 05:10:55/06:40:41, 2.39i/s] (dev = 2.9456336f0, tst = (1.48275f0,), mem = 1.6105089f10)\n",
      "┣███████████████▌    ┫ [77.94%, 43816/56220, 05:12:02/06:40:23, 2.84i/s] (dev = 2.9544f0, tst = (1.5037256f0,), mem = 1.6107997f10)\n",
      "┣███████████████▋    ┫ [78.22%, 43978/56220, 05:13:11/06:40:21, 2.38i/s] (dev = 2.9497283f0, tst = (1.4413273f0,), mem = 1.6103444f10)\n",
      "┣███████████████▋    ┫ [78.51%, 44138/56220, 05:14:18/06:40:20, 2.37i/s] (dev = 2.9619455f0, tst = (1.4555247f0,), mem = 1.6099539f10)\n",
      "┣███████████████▊    ┫ [78.77%, 44284/56220, 05:15:26/06:40:27, 2.16i/s] (dev = 2.940608f0, tst = (1.4619373f0,), mem = 1.610348f10)\n",
      "┣███████████████▊    ┫ [79.03%, 44432/56220, 05:16:33/06:40:32, 2.18i/s] (dev = 2.9497247f0, tst = (1.454263f0,), mem = 1.6092166f10)\n",
      "┣███████████████▊    ┫ [79.30%, 44584/56220, 05:17:41/06:40:36, 2.25i/s] (dev = 2.938396f0, tst = (1.4556513f0,), mem = 1.6110734f10)\n",
      "┣███████████████▉    ┫ [79.60%, 44752/56220, 05:18:48/06:40:30, 2.50i/s] (dev = 2.9409533f0, tst = (1.4704968f0,), mem = 1.6096268f10)\n",
      "┣███████████████▉    ┫ [79.91%, 44927/56220, 05:19:56/06:40:21, 2.58i/s] (dev = 2.946396f0, tst = (1.4824848f0,), mem = 1.6123327f10)\n",
      "┣████████████████    ┫ [80.21%, 45096/56220, 05:21:04/06:40:15, 2.50i/s] (dev = 3.0157359f0, tst = (1.4684298f0,), mem = 1.6097568f10)\n",
      "┣████████████████    ┫ [80.49%, 45252/56220, 05:22:12/06:40:18, 2.28i/s] (dev = 3.0220447f0, tst = (1.4642632f0,), mem = 1.6085905f10)\n",
      "┣████████████████▏   ┫ [80.79%, 45420/56220, 05:23:20/06:40:13, 2.47i/s] (dev = 3.036507f0, tst = (1.4606513f0,), mem = 1.6117149f10)\n",
      "┣████████████████▏   ┫ [81.07%, 45578/56220, 05:24:28/06:40:14, 2.31i/s] (dev = 3.0201588f0, tst = (1.474275f0,), mem = 1.6106021f10)\n",
      "┣████████████████▎   ┫ [81.38%, 45751/56220, 05:25:38/06:40:09, 2.48i/s] (dev = 3.0251932f0, tst = (1.4578727f0,), mem = 1.6114052f10)\n",
      "┣████████████████▎   ┫ [81.66%, 45909/56220, 05:26:45/06:40:09, 2.34i/s] (dev = 3.0585115f0, tst = (1.3890898f0,), mem = 1.6107223f10)\n",
      "┣████████████████▍   ┫ [81.97%, 46082/56220, 05:27:53/06:40:01, 2.55i/s] (dev = 3.0478294f0, tst = (1.4147485f0,), mem = 1.6115412f10)\n",
      "┣████████████████▍   ┫ [82.23%, 46227/56220, 05:29:00/06:40:07, 2.17i/s] (dev = 3.0470572f0, tst = (1.3968375f0,), mem = 1.6086607f10)\n",
      "┣████████████████▌   ┫ [82.52%, 46391/56220, 05:30:08/06:40:05, 2.40i/s] (dev = 3.0389838f0, tst = (1.3935207f0,), mem = 1.6102353f10)\n",
      "┣████████████████▌   ┫ [82.84%, 46571/56220, 05:31:15/06:39:53, 2.71i/s] (dev = 3.0641713f0, tst = (1.4117903f0,), mem = 1.6097521f10)\n",
      "┣████████████████▋   ┫ [83.13%, 46737/56220, 05:32:23/06:39:50, 2.42i/s] (dev = 3.055209f0, tst = (1.4111049f0,), mem = 1.6108271f10)\n",
      "┣████████████████▋   ┫ [83.40%, 46887/56220, 05:33:30/06:39:53, 2.25i/s] (dev = 3.0536194f0, tst = (1.4266232f0,), mem = 1.6106512f10)\n",
      "┣████████████████▋   ┫ [83.69%, 47052/56220, 05:34:38/06:39:50, 2.43i/s] (dev = 3.0729184f0, tst = (1.4490068f0,), mem = 1.6111531f10)\n",
      "┣████████████████▊   ┫ [83.96%, 47205/56220, 05:35:47/06:39:54, 2.22i/s] (dev = 3.0394464f0, tst = (1.4521788f0,), mem = 1.6095609f10)\n",
      "┣████████████████▊   ┫ [84.24%, 47361/56220, 05:36:54/06:39:55, 2.32i/s] (dev = 3.059073f0, tst = (1.4615579f0,), mem = 1.6089953f10)\n",
      "┣████████████████▉   ┫ [84.55%, 47532/56220, 05:38:10/06:39:58, 2.26i/s] (dev = 3.0565405f0, tst = (1.4425699f0,), mem = 1.6088155f10)\n",
      "┣████████████████▉   ┫ [84.85%, 47704/56220, 05:39:18/06:39:52, 2.52i/s] (dev = 3.0540323f0, tst = (1.4656365f0,), mem = 1.6089953f10)\n",
      "┣█████████████████   ┫ [85.14%, 47864/56220, 05:40:27/06:39:53, 2.33i/s] (dev = 3.0336034f0, tst = (1.4738019f0,), mem = 1.6106393f10)\n",
      "┣█████████████████   ┫ [85.42%, 48022/56220, 05:41:33/06:39:52, 2.37i/s] (dev = 3.0315678f0, tst = (1.4693753f0,), mem = 1.6095649f10)\n",
      "┣█████████████████▏  ┫ [85.70%, 48182/56220, 05:42:41/06:39:51, 2.37i/s] (dev = 3.0508864f0, tst = (1.4809636f0,), mem = 1.6105041f10)\n",
      "┣█████████████████▏  ┫ [86.00%, 48351/56220, 05:43:49/06:39:46, 2.49i/s] (dev = 3.056247f0, tst = (1.4898865f0,), mem = 1.6112711f10)\n",
      "┣█████████████████▎  ┫ [86.29%, 48514/56220, 05:45:07/06:39:56, 2.08i/s] (dev = 3.0418584f0, tst = (1.4539802f0,), mem = 1.612351f10)\n",
      "┣█████████████████▎  ┫ [86.58%, 48674/56220, 05:46:14/06:39:55, 2.37i/s] (dev = 3.015542f0, tst = (1.4209594f0,), mem = 1.6088237f10)\n",
      "┣█████████████████▎  ┫ [86.86%, 48834/56220, 05:47:23/06:39:55, 2.34i/s] (dev = 3.0300338f0, tst = (1.4350796f0,), mem = 1.6080964f10)\n",
      "┣█████████████████▍  ┫ [87.14%, 48990/56220, 05:48:30/06:39:56, 2.32i/s] (dev = 3.023582f0, tst = (1.3996041f0,), mem = 1.6098888f10)\n",
      "┣█████████████████▍  ┫ [87.44%, 49160/56220, 05:49:48/06:40:02, 2.18i/s] (dev = 3.0136533f0, tst = (1.3800611f0,), mem = 1.6087651f10)\n",
      "┣█████████████████▌  ┫ [87.75%, 49335/56220, 05:51:10/06:40:10, 2.15i/s] (dev = 3.0048974f0, tst = (1.4016491f0,), mem = 1.6117415f10)\n",
      "┣█████████████████▌  ┫ [88.00%, 49475/56220, 05:52:17/06:40:18, 2.09i/s] (dev = 3.0045273f0, tst = (1.415909f0,), mem = 1.6077714f10)\n",
      "┣█████████████████▋  ┫ [88.28%, 49633/56220, 05:53:24/06:40:18, 2.33i/s] (dev = 3.0098386f0, tst = (1.4206955f0,), mem = 1.6089548f10)\n",
      "┣█████████████████▋  ┫ [88.55%, 49784/56220, 05:54:31/06:40:21, 2.25i/s] (dev = 3.0035663f0, tst = (1.4185649f0,), mem = 1.6093075f10)\n",
      "┣█████████████████▊  ┫ [88.85%, 49952/56220, 05:55:38/06:40:16, 2.51i/s] (dev = 3.0152433f0, tst = (1.4418296f0,), mem = 1.6087658f10)\n",
      "┣█████████████████▊  ┫ [89.12%, 50101/56220, 05:56:45/06:40:20, 2.22i/s] (dev = 2.9987273f0, tst = (1.3711054f0,), mem = 1.6082454f10)\n",
      "┣█████████████████▉  ┫ [89.39%, 50254/56220, 05:57:53/06:40:22, 2.27i/s] (dev = 3.0074098f0, tst = (1.3605645f0,), mem = 1.6096736f10)\n",
      "┣█████████████████▉  ┫ [89.67%, 50412/56220, 05:59:03/06:40:25, 2.24i/s] (dev = 3.0053883f0, tst = (1.3798667f0,), mem = 1.6104573f10)\n",
      "┣█████████████████▉  ┫ [89.96%, 50578/56220, 06:00:10/06:40:21, 2.49i/s] (dev = 2.9944997f0, tst = (1.3445415f0,), mem = 1.60924f10)\n",
      "┣██████████████████  ┫ [90.26%, 50742/56220, 06:01:23/06:40:24, 2.25i/s] (dev = 3.0878623f0, tst = (1.2994667f0,), mem = 1.6092783f10)\n",
      "┣██████████████████  ┫ [90.53%, 50896/56220, 06:02:31/06:40:27, 2.25i/s] (dev = 3.0962572f0, tst = (1.3060582f0,), mem = 1.6094222f10)\n",
      "┣██████████████████▏ ┫ [90.81%, 51054/56220, 06:03:39/06:40:27, 2.34i/s] (dev = 3.1086245f0, tst = (1.3351815f0,), mem = 1.6099943f10)\n",
      "┣██████████████████▏ ┫ [91.11%, 51221/56220, 06:04:46/06:40:22, 2.48i/s] (dev = 3.1155152f0, tst = (1.3007369f0,), mem = 1.6107821f10)\n",
      "┣██████████████████▎ ┫ [91.41%, 51390/56220, 06:05:54/06:40:18, 2.48i/s] (dev = 3.0951474f0, tst = (1.3159937f0,), mem = 1.6090638f10)\n",
      "┣██████████████████▎ ┫ [91.71%, 51562/56220, 06:07:04/06:40:13, 2.49i/s] (dev = 3.1282532f0, tst = (1.3361664f0,), mem = 1.6093424f10)\n",
      "┣██████████████████▍ ┫ [92.01%, 51726/56220, 06:08:11/06:40:11, 2.42i/s] (dev = 3.1184025f0, tst = (1.3407369f0,), mem = 1.6110222f10)\n",
      "┣██████████████████▍ ┫ [92.31%, 51898/56220, 06:09:18/06:40:04, 2.57i/s] (dev = 3.1429775f0, tst = (1.3688934f0,), mem = 1.6096782f10)\n",
      "┣██████████████████▌ ┫ [92.65%, 52088/56220, 06:10:25/06:39:48, 2.86i/s] (dev = 3.1176202f0, tst = (1.375268f0,), mem = 1.6082349f10)\n",
      "┣██████████████████▌ ┫ [92.94%, 52250/56220, 06:11:33/06:39:46, 2.39i/s] (dev = 3.114068f0, tst = (1.3911861f0,), mem = 1.6098682f10)\n",
      "┣██████████████████▋ ┫ [93.21%, 52405/56220, 06:12:41/06:39:48, 2.28i/s] (dev = 3.103931f0, tst = (1.4025793f0,), mem = 1.6122477f10)\n",
      "┣██████████████████▋ ┫ [93.48%, 52552/56220, 06:13:53/06:39:58, 2.04i/s] (dev = 3.0910156f0, tst = (1.3639771f0,), mem = 1.6109398f10)\n",
      "┣██████████████████▊ ┫ [93.75%, 52708/56220, 06:15:00/06:39:59, 2.32i/s] (dev = 3.1149454f0, tst = (1.3682549f0,), mem = 1.6049615f10)\n",
      "┣██████████████████▊ ┫ [94.03%, 52865/56220, 06:16:07/06:39:59, 2.34i/s] (dev = 3.1178207f0, tst = (1.2938249f0,), mem = 1.6087224f10)\n",
      "┣██████████████████▊ ┫ [94.32%, 53026/56220, 06:17:14/06:39:58, 2.39i/s] (dev = 3.0968263f0, tst = (1.309119f0,), mem = 1.6091369f10)\n",
      "┣██████████████████▉ ┫ [94.61%, 53190/56220, 06:18:28/06:40:02, 2.21i/s] (dev = 3.1020977f0, tst = (1.308845f0,), mem = 1.6106381f10)\n",
      "┣██████████████████▉ ┫ [94.89%, 53345/56220, 06:19:38/06:40:06, 2.23i/s] (dev = 3.0886981f0, tst = (1.3064723f0,), mem = 1.6092926f10)\n",
      "┣███████████████████ ┫ [95.20%, 53524/56220, 06:20:45/06:39:56, 2.66i/s] (dev = 3.0964541f0, tst = (1.3040802f0,), mem = 1.6099268f10)\n",
      "┣███████████████████ ┫ [95.51%, 53696/56220, 06:21:53/06:39:50, 2.53i/s] (dev = 3.077371f0, tst = (1.3114138f0,), mem = 1.6105362f10)\n",
      "┣███████████████████▏┫ [95.81%, 53863/56220, 06:23:02/06:39:48, 2.44i/s] (dev = 3.0897086f0, tst = (1.2874262f0,), mem = 1.6121543f10)\n",
      "┣███████████████████▏┫ [96.07%, 54013/56220, 06:24:09/06:39:51, 2.22i/s] (dev = 3.0966365f0, tst = (1.2880828f0,), mem = 1.6093531f10)\n",
      "┣███████████████████▎┫ [96.37%, 54179/56220, 06:25:18/06:39:49, 2.43i/s] (dev = 3.1062527f0, tst = (1.2718807f0,), mem = 1.6103175f10)\n",
      "┣███████████████████▎┫ [96.66%, 54344/56220, 06:26:28/06:39:48, 2.36i/s] (dev = 3.071433f0, tst = (1.2845091f0,), mem = 1.6077726f10)\n",
      "┣███████████████████▍┫ [96.94%, 54501/56220, 06:27:35/06:39:48, 2.34i/s] (dev = 3.0830734f0, tst = (1.2930577f0,), mem = 1.6098099f10)\n",
      "┣███████████████████▍┫ [97.22%, 54658/56220, 06:28:42/06:39:49, 2.33i/s] (dev = 3.0795865f0, tst = (1.3022832f0,), mem = 1.6096229f10)\n",
      "┣███████████████████▌┫ [97.57%, 54856/56220, 06:29:50/06:39:31, 2.92i/s] (dev = 3.077848f0, tst = (1.3068318f0,), mem = 1.6097426f10)\n",
      "┣███████████████████▌┫ [97.85%, 55009/56220, 06:30:57/06:39:34, 2.26i/s] (dev = 3.0634053f0, tst = (1.3037677f0,), mem = 1.6111119f10)\n",
      "┣███████████████████▋┫ [98.13%, 55167/56220, 06:32:06/06:39:35, 2.32i/s] (dev = 3.0737858f0, tst = (1.310782f0,), mem = 1.6092892f10)\n",
      "┣███████████████████▋┫ [98.41%, 55327/56220, 06:33:21/06:39:42, 2.12i/s] (dev = 3.0696688f0, tst = (1.3269535f0,), mem = 1.6082403f10)\n",
      "┣███████████████████▋┫ [98.70%, 55490/56220, 06:34:28/06:39:39, 2.44i/s] (dev = 3.0665681f0, tst = (1.338053f0,), mem = 1.6093713f10)\n",
      "┣███████████████████▊┫ [98.98%, 55646/56220, 06:35:36/06:39:40, 2.30i/s] (dev = 3.0532377f0, tst = (1.3491865f0,), mem = 1.611394f10)\n",
      "┣███████████████████▊┫ [99.27%, 55808/56220, 06:36:43/06:39:38, 2.41i/s] (dev = 3.059167f0, tst = (1.3425527f0,), mem = 1.6081537f10)\n",
      "┣███████████████████▉┫ [99.56%, 55974/56220, 06:37:52/06:39:37, 2.39i/s] (dev = 3.0738757f0, tst = (1.3192744f0,), mem = 1.6073424f10)\n",
      "┣███████████████████▉┫ [99.87%, 56147/56220, 06:39:03/06:39:34, 2.46i/s] (dev = 3.053389f0, tst = (1.3269145f0,), mem = 1.6101489f10)\n",
      "┣████████████████████┫ [100.00%, 56220/56220, 06:39:44/06:39:44, 2.34i/s] (dev = 3.0395327f0, tst = (1.2866848f0,), mem = 1.6080077f10)\n",
      "┣████████████████████┫ [100.00%, 4045/4045, 01:36/01:36, 42.24i/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 15.77, 47.5/20.9/10.8/5.8 (BP=1.000, ratio=1.036, hyp_len=85441, ref_len=82502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: <unk> gereksinimleri var .\n",
      "REF: they need noise .\n",
      "OUT: they need <unk> needs .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "translate_input (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 8. Training\n",
    "#\n",
    "# `trainmodel` creates, trains and returns an `S2S` model. The arguments are described in\n",
    "# comments.\n",
    "\n",
    "function trainmodel(\n",
    "    trn,                  # Training data\n",
    "    dev,                  # Validation data, used to determine the best model\n",
    "    tst...;               # Zero or more test datasets, their loss will be periodically reported\n",
    "    bidirectional = true, # Whether to use a bidirectional encoder\n",
    "    layers = 2,           # Number of layers (use `layers÷2` for a bidirectional encoder)\n",
    "    hidden = 512,         # Size of the hidden vectors\n",
    "    srcembed = 512,       # Size of the source language embedding vectors\n",
    "    tgtembed = 512,       # Size of the target language embedding vectors\n",
    "    dropout = 0.2,        # Dropout probability\n",
    "    epochs = 0,           # Number of epochs (one of epochs or iters should be nonzero for training)\n",
    "    iters = 0,            # Number of iterations (one of epochs or iters should be nonzero for training)\n",
    "    bleu = false,         # Whether to calculate the BLEU score for the final model\n",
    "    save = false,         # Whether to save the final model\n",
    "    seconds = 60,         # Frequency of progress reporting\n",
    ")\n",
    "    @show bidirectional,\n",
    "        layers,\n",
    "        hidden,\n",
    "        srcembed,\n",
    "        tgtembed,\n",
    "        dropout,\n",
    "        epochs,\n",
    "        iters,\n",
    "        bleu,\n",
    "        save\n",
    "    flush(stdout)\n",
    "    model = S2S(\n",
    "        hidden,\n",
    "        srcembed,\n",
    "        tgtembed,\n",
    "        trn.src.vocab,\n",
    "        trn.tgt.vocab;\n",
    "        layers = layers,\n",
    "        dropout = dropout,\n",
    "        bidirectional = bidirectional,\n",
    "    )\n",
    "\n",
    "    epochs == iters == 0 && return model\n",
    "\n",
    "    (ctrn, cdev, ctst) = collect(trn), collect(dev), collect.(tst)\n",
    "    traindata = (epochs > 0 ?\n",
    "                 collect(flatten(shuffle!(ctrn) for i = 1:epochs)) :\n",
    "                 shuffle!(collect(take(cycle(ctrn), iters))))\n",
    "\n",
    "    bestloss, bestmodel = loss(model, cdev), deepcopy(model)\n",
    "    progress!(adam(model, traindata), seconds = seconds) do y\n",
    "        devloss = loss(model, cdev)\n",
    "        tstloss = map(d -> loss(model, d), ctst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (dev = devloss, tst = tstloss, mem = Float32(CuArrays.usage[]))\n",
    "    end\n",
    "    save && Knet.save(\"attn-$(Int(time_ns())).jld2\", \"model\", bestmodel)\n",
    "    bleu && Main.bleu(bestmodel, dev)\n",
    "    return bestmodel\n",
    "end\n",
    "\n",
    "# Train a model: If your implementation is correct, the first epoch should take about 24\n",
    "# minutes on a v100 and bring the loss from 9.83 to under 4.0. 10 epochs would take about 4\n",
    "# hours on a v100. With other GPUs you may have to use a smaller batch size (if memory is\n",
    "# lower) and longer time (if gpu speed is lower).\n",
    "\n",
    "## Uncomment the appropriate option for training:\n",
    "#model = pretrained  # Use reference model\n",
    "#model = Knet.load(\"attn-1538395466294882.jld2\", \"model\")  # Load pretrained model\n",
    "model = trainmodel(dtrn,ddev,take(dtrn,20); epochs=10, save=true, bleu=true)  # Train model\n",
    "\n",
    "# Code to sample translations from a dataset\n",
    "data1 = MTData(tr_dev, en_dev, batchsize = 1) |> collect;\n",
    "function translate_sample(model, data)\n",
    "    (src, tgt) = rand(data)\n",
    "    out = model(src)\n",
    "    println(\"SRC: \", int2str(src, model.srcvocab))\n",
    "    println(\"REF: \", int2str(tgt, model.tgtvocab))\n",
    "    println(\"OUT: \", int2str(out, model.tgtvocab))\n",
    "end\n",
    "\n",
    "# Generate translations for random instances from the dev set\n",
    "translate_sample(model, data1)\n",
    "\n",
    "# Code to generate translations from user input\n",
    "function translate_input(model)\n",
    "    v = model.srcvocab\n",
    "    src = [get(v.w2i, w, v.unk) for w in v.tokenizer(readline())]'\n",
    "    out = model(src)\n",
    "    println(\"SRC: \", int2str(src, model.srcvocab))\n",
    "    println(\"OUT: \", int2str(out, model.tgtvocab))\n",
    "end\n",
    "\n",
    "# Generate translations for user input\n",
    "## translate_input(model)\n",
    "\n",
    "# ## Competition\n",
    "#\n",
    "# The reference model `pretrained` has 16.2 bleu. By playing with the optimization algorithm\n",
    "# and hyperparameters, using per-sentence loss, and (most importantly) splitting the Turkish\n",
    "# words I was able to push the performance to 21.0 bleu. I will give extra credit to groups\n",
    "# that can exceed 21.0 bleu in this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 4045/4045, 01:35/01:35, 42.52i/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 15.77, 47.5/20.9/10.8/5.8 (BP=1.000, ratio=1.036, hyp_len=85441, ref_len=82502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"/tmp/jl_lJutMd\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main.bleu(model, ddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdin> seçeneklerin yaşam felsefesi\n",
      "SRC: seçeneklerin yaşam felsefesi\n",
      "OUT: the life 's mind is the idea of choice\n"
     ]
    }
   ],
   "source": [
    "translate_input(model)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
