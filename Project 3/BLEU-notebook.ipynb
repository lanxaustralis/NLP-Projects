{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using Knet.train! in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "Package name: AutoGrad Version: 1.2.0\n",
      "Package name: IterTools Version: 1.3.0\n",
      "Package name: StatsBase Version: 0.32.0\n",
      "Package name: Knet Version: 1.3.2\n",
      "Package name: CuArrays Version: 1.5.0\n",
      "Package name: IJulia Version: 1.20.2\n",
      "Package name: Literate Version: 2.2.1\n",
      "Package name: Statistics Version: 0.0.1\n"
     ]
    }
   ],
   "source": [
    "#jl Use `Literate.notebook(juliafile, \".\", execute=false)` to convert to notebook.\n",
    "\n",
    "# # Neural Machine Translation\n",
    "#\n",
    "# **Reference:** Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" In Advances in neural information processing systems, pp. 3104-3112. 2014. ([Paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks), [Sample code](https://github.com/tensorflow/nmt))\n",
    "#import Pkg\n",
    "using Pkg\n",
    "using Knet, Test, Base.Iterators, IterTools, Random # , LinearAlgebra, StatsBase\n",
    "using AutoGrad: @gcheck  # to check gradients, use with Float64\n",
    "Knet.atype() = KnetArray{Float32}  # determines what Knet.param() uses.\n",
    "macro size(z, s) # for debugging\n",
    "    esc(:(@assert (size($z) == $s) string(summary($z), !=, $s))) # for debugging\n",
    "end # for debugging\n",
    "\n",
    "Pkg.add(\"Statistics\")\n",
    "import Statistics\n",
    "using Statistics\n",
    "\n",
    "Pkg.add(\"CuArrays\")\n",
    "Pkg.build(\"CuArrays\")\n",
    "\n",
    "using CuArrays: CuArrays, usage_limit\n",
    "\n",
    "CuArrays.usage_limit[] = 8_000_000_000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "Pkg.update()\n",
    "pkgs = Pkg.installed()\n",
    "\n",
    "for package in keys(pkgs)\n",
    "    if pkgs[package] == nothing\n",
    "        pkgs[package] = VersionNumber(\"0.0.1\")\n",
    "    end\n",
    "    println(\"Package name: \", package, \" Version: \", pkgs[package])\n",
    "end\n",
    "\n",
    "\n",
    "#array_type = KnetArray # For GPU instances\n",
    "#array_type=Array # For CPU instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask! (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "function Vocab(\n",
    "    file::String;\n",
    "    tokenizer = split,\n",
    "    vocabsize = Inf,\n",
    "    mincount = 1,\n",
    "    unk = \"<unk>\",\n",
    "    eos = \"<s>\",\n",
    ")\n",
    "    vocab_freq = Dict{String,Int64}(unk => 1, eos => 1)\n",
    "    w2i = Dict{String,Int64}(unk => 2, eos => 1)\n",
    "    i2w = Vector{String}()\n",
    "\n",
    "    push!(i2w, eos)\n",
    "    push!(i2w, unk)\n",
    "\n",
    "    open(file) do f\n",
    "        for line in eachline(f)\n",
    "            sentence = strip(lowercase(line))\n",
    "            sentence = tokenizer(line, [' '], keepempty = false)\n",
    "\n",
    "            for word in sentence\n",
    "                word == unk && continue\n",
    "                word == eos && continue # They are default ones to be added later\n",
    "                vocab_freq[word] = get!(vocab_freq, word, 0) + 1\n",
    "            end\n",
    "        end\n",
    "        close(f)\n",
    "    end\n",
    "\n",
    "\n",
    "    # End of vanilla implementation of the vocaulary\n",
    "    # From here we must add the mincount and vocabsize properties\n",
    "    # We must change the first two property of the vocab wrt those paramaters\n",
    "    vocab_freq = sort!(\n",
    "        collect(vocab_freq),\n",
    "        by = tuple -> last(tuple),\n",
    "        rev = true,\n",
    "    )\n",
    "\n",
    "    if length(vocab_freq) > vocabsize - 2 # eos and unk ones\n",
    "        vocab_freq = vocab_freq[1:vocabsize-2] # trim to fit the size\n",
    "    end\n",
    "\n",
    "    #vocab_freq = reverse(vocab_freq)\n",
    "\n",
    "    while true\n",
    "        length(vocab_freq) == 0 && break\n",
    "        word, freq = vocab_freq[end]\n",
    "        freq >= mincount && break # since it is already ordered\n",
    "        vocab_freq = vocab_freq[1:(end-1)]\n",
    "    end\n",
    "    #pushfirst!(vocab_freq,unk=>1,eos=>1) # freq does not matter, just adding the\n",
    "    for i = 1:length(vocab_freq)\n",
    "        word, freq = vocab_freq[i]\n",
    "        ind = (get!(w2i, word, 1 + length(w2i)))\n",
    "        (length(i2w) < ind) && push!(i2w, word)\n",
    "    end\n",
    "\n",
    "    return Vocab(w2i, i2w, 2, 1, tokenizer)\n",
    "end\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "word2ind(dict, x) = get(dict, x, 2) # unk is 2\n",
    "\n",
    "#Implementing the iterate function\n",
    "function Base.iterate(r::TextReader, s = nothing)\n",
    "    if s == nothing\n",
    "        state = open(r.file)\n",
    "        Base.iterate(r, state)\n",
    "    else\n",
    "        if eof(s) == true\n",
    "            close(s)\n",
    "            return nothing\n",
    "        else\n",
    "            line = readline(s)\n",
    "            line = strip(lowercase(line))\n",
    "            sent = r.vocab.tokenizer(line, [' '], keepempty = false)\n",
    "            sent_ind = Int[]\n",
    "            for word in sent\n",
    "                ind = word2ind(r.vocab.w2i, word)\n",
    "                push!(sent_ind, ind)\n",
    "            end\n",
    "            return (sent_ind, s)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "#Embed\n",
    "struct Embed\n",
    "    w\n",
    "end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    Embed(param(embedsize, vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    l.w[:, x]\n",
    "end\n",
    "\n",
    "#Linear\n",
    "struct Linear\n",
    "    w\n",
    "    b\n",
    "end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    Linear(\n",
    "        param(outputsize, inputsize),\n",
    "        param0(outputsize),\n",
    "    )\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    l.w * mat(x, dims = 1) .+ l.b\n",
    "end\n",
    "\n",
    "# Mask!\n",
    "function mask!(a, pad)\n",
    "    matr = a\n",
    "    for j = 1:size(matr)[1]\n",
    "        i = 0\n",
    "        while (i < length(matr[j, :]) - 1)\n",
    "            if matr[j, length(matr[j, :])-i-1] != pad\n",
    "                break\n",
    "\n",
    "            elseif matr[j, length(matr[j, :])-i] == pad\n",
    "                matr[j, length(matr[j, :])-i] = 0\n",
    "            end\n",
    "            i += 1\n",
    "        end\n",
    "    end\n",
    "    return matr\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing data\n",
      "└ @ Main In[4]:24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 0. Load data\n",
    "#\n",
    "# We will use the Turkish-English pair from the [TED Talks Dataset](https://github.com/neulab/word-embeddings-for-nmt) for our experiments.\n",
    "\n",
    "datadir = \"datasets/tr_to_en\"\n",
    "\n",
    "if !isdir(datadir)\n",
    "    download(\n",
    "        \"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\",\n",
    "        \"qi18naacl-dataset.tar.gz\",\n",
    "    )\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    tr_vocab = Vocab(\"$datadir/tr.train\", mincount = 5)\n",
    "    en_vocab = Vocab(\"$datadir/en.train\", mincount = 5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    @info \"Testing data\"\n",
    "    @test length(tr_vocab.i2w) == 38126\n",
    "    @test length(first(tr_test)) == 16\n",
    "    @test length(collect(tr_test)) == 5029\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing MTData\n",
      "└ @ Main In[5]:116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 1. Minibatching\n",
    "#\n",
    "# For minibatching we are going to design a new iterator: `MTData`. This iterator is built\n",
    "# on top of two TextReaders `src` and `tgt` that produce parallel sentences for source and\n",
    "# target languages.\n",
    "\n",
    "struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(\n",
    "    src::TextReader,\n",
    "    tgt::TextReader;\n",
    "    batchmaker = arraybatch,\n",
    "    batchsize = BATCH_SIZE,\n",
    "    maxlength = typemax(Int),\n",
    "    batchmajor = false,\n",
    "    bucketwidth = 10,\n",
    "    numbuckets = min(BATCH_SIZE, maxlength ÷ bucketwidth),\n",
    ")\n",
    "    buckets = [[] for i = 1:numbuckets] # buckets[i] is an array of sentence pairs with similar length\n",
    "    MTData(\n",
    "        src,\n",
    "        tgt,\n",
    "        batchsize,\n",
    "        maxlength,\n",
    "        batchmajor,\n",
    "        bucketwidth,\n",
    "        buckets,\n",
    "        batchmaker,\n",
    "    )\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::MTData, state = nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets\n",
    "            empty!(b)\n",
    "        end\n",
    "        state_src, state_tgt = nothing, nothing\n",
    "    else\n",
    "        state_src, state_tgt = state\n",
    "    end\n",
    "    bucket, ibucket = nothing, nothing\n",
    "\n",
    "\n",
    "    while true\n",
    "        iter_src = (state_src === nothing ? iterate(d.src) :\n",
    "                    iterate(d.src, state_src))\n",
    "        iter_tgt = (state_tgt === nothing ? iterate(d.tgt) :\n",
    "                    iterate(d.tgt, state_tgt))\n",
    "\n",
    "        if iter_src === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent_src, state_src = iter_src\n",
    "            sent_tgt, state_tgt = iter_tgt\n",
    "            if length(sent_src) > d.maxlength || length(sent_src) == 0\n",
    "                continue\n",
    "            end\n",
    "            ibucket = min(\n",
    "                1 + (length(sent_src) - 1) ÷ d.bucketwidth,\n",
    "                length(d.buckets),\n",
    "            )\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, (sent_src, sent_tgt))\n",
    "            if length(bucket) === d.batchsize\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    batch = d.batchmaker(d, bucket)\n",
    "\n",
    "    empty!(bucket)\n",
    "    return batch, (state_src, state_tgt)\n",
    "end\n",
    "\n",
    "\n",
    "function arraybatch(d::MTData, bucket)\n",
    "    bucketx = map(x -> x[1], bucket)\n",
    "    buckety = map(x -> x[2], bucket)\n",
    "    batch_x = fill(d.src.vocab.eos, length(bucketx), maximum(length.(bucketx)))\n",
    "    for i = 1:length(bucket)\n",
    "        batch_x[i, end-length(bucketx[i])+1:end] = bucketx[i]\n",
    "    end\n",
    "    batch_y = fill(\n",
    "        d.tgt.vocab.eos,\n",
    "        length(buckety),\n",
    "        maximum(length.(buckety)) + 2,\n",
    "    )\n",
    "    for i = 1:length(bucket)\n",
    "        batch_y[i, 2:length(buckety[i])+1] = buckety[i]\n",
    "    end\n",
    "\n",
    "    return (batch_x, batch_y)\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing MTData\"\n",
    "dtrn = MTData(tr_train, en_train)\n",
    "ddev = MTData(tr_dev, en_dev)\n",
    "dtst = MTData(tr_test, en_test)\n",
    "\n",
    "x, y = first(dtst)\n",
    "\n",
    "# Commented out this since we have changed the batch size\n",
    "# @test length(collect(dtst)) == 48\n",
    "# @test size.((x, y)) == ((128, 10), (128, 24))\n",
    "@test x[1, 1] == tr_vocab.eos\n",
    "@test x[1, end] != tr_vocab.eos\n",
    "@test y[1, 1] == en_vocab.eos\n",
    "@test y[1, 2] != en_vocab.eos\n",
    "@test y[1, end] == en_vocab.eos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing S2S_v1\n",
      "└ @ Main In[6]:105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14478.669f0, 1470)\n"
     ]
    }
   ],
   "source": [
    "# ## Part 2. Sequence to sequence model without attention\n",
    "#\n",
    "# In this part we will define a simple sequence to sequence encoder-decoder model for\n",
    "# machine translation.\n",
    "\n",
    "struct S2S_v1\n",
    "    srcembed::Embed     # source language embedding\n",
    "    encoder::RNN        # encoder RNN (can be bidirectional)\n",
    "    tgtembed::Embed     # target language embedding\n",
    "    decoder::RNN        # decoder RNN\n",
    "    projection::Linear  # converts decoder output to vocab scores\n",
    "    dropout::Real       # dropout probability to prevent overfitting\n",
    "    srcvocab::Vocab     # source language vocabulary\n",
    "    tgtvocab::Vocab     # target language vocabulary\n",
    "end\n",
    "\n",
    "function S2S_v1(\n",
    "    hidden::Int,         # hidden size for both the encoder and decoder RNN\n",
    "    srcembsz::Int,       # embedding size for source language\n",
    "    tgtembsz::Int,       # embedding size for target language\n",
    "    srcvocab::Vocab,     # vocabulary for source language\n",
    "    tgtvocab::Vocab;     # vocabulary for target language\n",
    "    layers = 1,            # number of layers\n",
    "    bidirectional = false, # whether encoder RNN is bidirectional\n",
    "    dropout = 0,\n",
    ")           # dropout probability\n",
    "\n",
    "\n",
    "    layerMultiplier = bidirectional ? 2 : 1\n",
    "\n",
    "    S2S_v1(\n",
    "        Embed(length(srcvocab.i2w), srcembsz),\n",
    "        RNN(\n",
    "            srcembsz,\n",
    "            hidden,\n",
    "            numLayers = layers,\n",
    "            bidirectional = bidirectional,\n",
    "            dropout = dropout\n",
    "        ),\n",
    "        Embed(length(tgtvocab.i2w), tgtembsz),\n",
    "        RNN(\n",
    "            tgtembsz,\n",
    "            hidden,\n",
    "            numLayers = layerMultiplier * layers,\n",
    "            dropout = dropout\n",
    "        ),\n",
    "        Linear(hidden, length(tgtvocab.i2w)),\n",
    "        dropout,\n",
    "        srcvocab,\n",
    "        tgtvocab,\n",
    "    )\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function (s::S2S_v1)(src, tgt; average = true)\n",
    "    #B,Tx = size(src,2)\n",
    "    B, Ty = size(tgt)\n",
    "    Ty -= 1 # Crop one\n",
    "    # Ex, Ey = length(model.srcembed([1])), length(model.tgtembed([1]))\n",
    "\n",
    "    rnn_encoder = s.encoder\n",
    "    rnn_decoder = s.decoder\n",
    "    project = s.projection\n",
    "\n",
    "    # Lx, Ly = rnn_encoder.numLayers, rnn_decoder.numLayers\n",
    "    # Hx, Hy = rnn_encoder.hiddenSize, rnn_decoder.hiddenSize\n",
    "    # Dx = Ly/Lx\n",
    "\n",
    "    emb_out_src = s.srcembed(src)\n",
    "    #@test size(emb_out_src)== (Ex,B,Tx) # Done\n",
    "\n",
    "    # Safe for repetitive usage\n",
    "    rnn_encoder.h = 0\n",
    "    rnn_encoder.c = 0\n",
    "\n",
    "    y_enc = rnn_encoder(emb_out_src)\n",
    "    #@test size(y_enc) == (Hx*Dx,B,Tx)\n",
    "    h_enc = rnn_encoder.h\n",
    "    #@test size(h_enc) == (Hx,B,Lx*Dx)\n",
    "    c_enc = rnn_encoder.c\n",
    "\n",
    "    emb_out_tgt = s.tgtembed(tgt[:, 1:end-1])\n",
    "    #@test size(emb_out_tgt)== (Ey,B,Ty)\n",
    "\n",
    "    rnn_decoder.h = h_enc\n",
    "    rnn_decoder.c = c_enc\n",
    "    y_dec = rnn_decoder(emb_out_tgt)\n",
    "    #@test size(y_dec)==(Hy,B,Ty)\n",
    "\n",
    "    project_inp = reshape(y_dec, :, B * Ty)\n",
    "    project_out = project(project_inp)\n",
    "\n",
    "    #@test size(project_out)==(length(project.b),B*Ty)\n",
    "\n",
    "    verify = deepcopy(tgt)\n",
    "    mask!(verify, s.tgtvocab.eos)\n",
    "\n",
    "    average && return mean(nll(project_out, verify[:, 2:end]))\n",
    "    return nll(project_out, verify[:, 2:end]; average = false)\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing S2S_v1\"\n",
    "Knet.seed!(1)\n",
    "model = S2S_v1(\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    tr_vocab,\n",
    "    en_vocab;\n",
    "    layers = 2,\n",
    "    bidirectional = true,\n",
    "    dropout = 0.2,\n",
    ")\n",
    "(x, y) = first(dtst)\n",
    "## Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "## The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "#@test model(x,y; average=false) == (14097.471f0, 1432)  !!!!!!\n",
    "println(model(x, y; average = false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss\n",
      "└ @ Main In[7]:25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.627923 seconds (1.96 M allocations: 192.643 MiB, 4.00% gc time)\n",
      "(1.0433315f6, 105937)\n"
     ]
    }
   ],
   "source": [
    "# ### Loss for a whole dataset\n",
    "#\n",
    "# Define a `loss(model, data)` which returns a `(Σloss, Nloss)` pair if `average=false` and\n",
    "# a `Σloss/Nloss` average if `average=true` for a whole dataset. Assume that `data` is an\n",
    "# iterator of `(x,y)` pairs such as `MTData` and `model(x,y;average)` is a model like\n",
    "# `S2S_v1` that computes loss on a single `(x,y)` pair.\n",
    "\n",
    "function loss(model, data; average = true)\n",
    "    total_loss = 0\n",
    "    total_word = 0\n",
    "\n",
    "    for (x, y) in collect(data)\n",
    "        curr_loss, curr_word = model(x, y; average = false)\n",
    "        total_loss += curr_loss\n",
    "        total_word += curr_word\n",
    "    end\n",
    "\n",
    "    average && return total_loss / total_word\n",
    "    return (total_loss, total_word)\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing loss\"\n",
    "@time res = loss(model, dtst, average = false)\n",
    "println(res)\n",
    "#@test res == (1.0429117f6, 105937) !!!!!!!!!!\n",
    "## Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "## The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "## Also, because we do not mask src, different batch sizes may lead to slightly different\n",
    "## losses. The test above gives (1.0429178f6, 105937) with batchsize==1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training S2S_v1\n",
      "└ @ Main In[11]:29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S2S_v1(Embed(P(KnetArray{Float32,2}(512,38126))), LSTM(input=512,hidden=512,bidirectional,layers=2,dropout=0.2), Embed(P(KnetArray{Float32,2}(512,18857))), LSTM(input=512,hidden=512,layers=4,dropout=0.2), Linear(P(KnetArray{Float32,2}(18857,512)), P(KnetArray{Float32,1}(18857))), 0.2, Vocab(Dict(\"dev\" => 1277,\"komuta\" => 13566,\"ellisi\" => 25239,\"adresini\" => 22820,\"yüzeyi\" => 4051,\"paris'te\" => 9494,\"kafamdaki\" => 18790,\"yüzeyinde\" => 5042,\"geçerlidir\" => 6612,\"kökten\" => 7774…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"seçmemiz\", \"destekleyip\", \"karşılaştırılabilir\", \"ördeğin\", \"gününüzü\", \"bağışçı\", \"istismara\", \"yaşça\", \"tedci\", \"fakültesi'nde\"], 2, 1, split), Vocab(Dict(\"middle-income\" => 13398,\"photosynthesis\" => 7689,\"polarizing\" => 17881,\"henry\" => 4248,\"abducted\" => 15691,\"rises\" => 6225,\"hampshire\" => 13888,\"whiz\" => 16835,\"cost-benefit\" => 13137,\"progression\" => 5549…), [\"<s>\", \"<unk>\", \",\", \".\", \"the\", \"and\", \"to\", \"of\", \"a\", \"that\"  …  \"archaea\", \"handshake\", \"brit\", \"wiper\", \"heroines\", \"coca\", \"exceptionally\", \"gallbladder\", \"autopsies\", \"linguistics\"], 2, 1, split))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Training SGD_v1\n",
    "#\n",
    "# The following function can be used to train our model. `trn` is the training data, `dev`\n",
    "# is used to determine the best model, `tst...` can be zero or more small test datasets for\n",
    "# loss reporting. It returns the model that does best on `dev`.\n",
    "\n",
    "function train!(model, trn, dev, tst...)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn), steps = 100) do y\n",
    "        losses = [loss(model, d) for d in (dev, tst...)]\n",
    "        if losses[1] < bestloss\n",
    "            bestmodel, bestloss = deepcopy(model), losses[1]\n",
    "        end\n",
    "        return (losses...,)\n",
    "    end\n",
    "    return bestmodel\n",
    "end\n",
    "\n",
    "# You should be able to get under 3.40 dev loss with the following settings in 10\n",
    "# epochs. The training speed on a V100 is about 3 mins/epoch or 40K words/sec, K80 is about\n",
    "# 6 times slower. Using settings closer to the Luong paper (per-sentence loss rather than\n",
    "# per-word loss, SGD with lr=1, gclip=1 instead of Adam), you can get to 3.17 dev loss in\n",
    "# about 25 epochs. Using dropout and shuffling batches before each epoch significantly\n",
    "# improve the dev loss. You can play around with hyperparameters but I doubt results will\n",
    "# get much better without attention. To verify your training, here is the dev loss I\n",
    "# observed at the beginning of each epoch in one training session:\n",
    "# `[9.83, 4.60, 3.98, 3.69, 3.52, 3.41, 3.35, 3.32, 3.30, 3.31, 3.33]`\n",
    "\n",
    "@info \"Training S2S_v1\"\n",
    "epochs = 10\n",
    "ctrn = collect(dtrn)\n",
    "trnx10 = collect(flatten(shuffle!(ctrn) for i = 1:epochs))\n",
    "trn20 = ctrn[1:20]\n",
    "dev38 = collect(ddev)\n",
    "## Uncomment this to train the model (This takes about 30 mins on a V100):\n",
    "#model = train!(model, trnx10, dev38, trn20)\n",
    "## Uncomment this to save the model:\n",
    "#Knet.save(\"s2s_v1.jld2\",\"model\",model)\n",
    "## Uncomment this to load the model:\n",
    "model = Knet.load(\"s2s_v1.jld2\",\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Generating translations\n",
    "#\n",
    "# With a single argument, a `S2S_v1` object should take it as a batch of source sentences\n",
    "# and generate translations for them. After passing `src` through the encoder and copying\n",
    "# its hidden states to the decoder, the decoder is run starting with an initial input of all\n",
    "# `eos` tokens. Highest scoring tokens are appended to the output and used as input for the\n",
    "# subsequent decoder steps.  The decoder should stop generating when all sequences in the\n",
    "# batch have generated `eos` or when `stopfactor * size(src,2)` decoder steps are reached. A\n",
    "# correctly shaped target language batch should be returned.\n",
    "\n",
    "function (s::S2S_v1)(src::Matrix{Int}; stopfactor = 3)\n",
    "    # Preperation for initial step\n",
    "    B,Tx = size(src)\n",
    "    max_step = stopfactor * Tx\n",
    "    tgt_eos = s.tgtvocab.eos\n",
    "\n",
    "    tgt = fill(tgt_eos, (B, 1)) # size as (B,2)\n",
    "    #output = fill(tgt_eos,(B,max_step))\n",
    "    output = Array{Int64}(undef, B, max_step)\n",
    "\n",
    "    rnn_encoder = s.encoder\n",
    "    rnn_decoder = s.decoder\n",
    "    project = s.projection\n",
    "\n",
    "    emb_out_src = s.srcembed(src)\n",
    "\n",
    "    # Safe for repetitive usage\n",
    "    rnn_encoder.h = 0\n",
    "    rnn_encoder.c = 0\n",
    "\n",
    "    y_enc = rnn_encoder(emb_out_src)\n",
    "    rnn_decoder.h = rnn_encoder.h\n",
    "    rnn_decoder.c = rnn_encoder.c\n",
    "\n",
    "    step = 0\n",
    "    #@test Ty == size(tgt,2)\n",
    "\n",
    "    while step < max_step\n",
    "        step += 1\n",
    "        emb_out_tgt = s.tgtembed(tgt)\n",
    "\n",
    "        y_dec = rnn_decoder(emb_out_tgt)\n",
    "\n",
    "        project_inp = reshape(y_dec, :, B)\n",
    "        project_out = project(project_inp)\n",
    "\n",
    "        eos_num = 0\n",
    "        for i = 1:B\n",
    "            # Assigns the position of the highest token\n",
    "            col = project_out[:, i]\n",
    "#             colMax = col[1]\n",
    "#             index = 1\n",
    "#             for j in 1:length(col)\n",
    "#                 if colMax<col[j]\n",
    "#                     colMax, index = col[j],j\n",
    "#                 end\n",
    "#             end\n",
    "            index = findmax(col)[2]\n",
    "            if index == tgt_eos\n",
    "                eos_num += 1\n",
    "            end\n",
    "            tgt[i] = index\n",
    "        end\n",
    "\n",
    "        output[:,step] = tgt\n",
    "        eos_num == B && break # all produced eos\n",
    "\n",
    "\n",
    "    end\n",
    "\n",
    "    return output[:, 1:step]\n",
    "\n",
    "end\n",
    "\n",
    "#-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Generating some translations\n",
      "└ @ Main In[13]:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: `` gördüm ki , çoğu insan benim `` '' gri '' '' olarak bahsetmeye başladığım <unk> düşüyor . yalnız , izin verin , şunu <unk> ifade edeyim -ki bu çok <unk> : ben , hiçbir şekilde , <unk> olmadığını söylemiyorum ; ''\n",
      "REF: `` i found that most people fall on a spectrum of what i have come to refer to as `` '' grey . '' '' let me be clear though — and this is very important — in no way am i saying that preference does n't exist . ''\n",
      "OUT: `` i 've seen that people are very interested in the <unk> , the `` '' <unk> '' '' — `` '' i 'm not going to talk to you . '' '' i do n't know what the word is , `` '' you 're not familiar with the <unk> . '' ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Calculating BLEU\n",
      "└ @ Main In[13]:52\n",
      "┣████████████████████┫ [100.00%, 4045/4045, 01:03/01:03, 64.26i/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 8.35, 37.7/12.1/5.0/2.2 (BP=1.000, ratio=1.065, hyp_len=87851, ref_len=82502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"/tmp/jl_i7THBU\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Utility to convert int arrays to sentence strings\n",
    "function int2str(y, vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w -> !isequal(w, vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1 + length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Generating some translations\"\n",
    "d = MTData(tr_dev, en_dev, batchsize = 1) |> collect\n",
    "(src, tgt) = rand(d)\n",
    "out = model(src)\n",
    "println(\"SRC: \", int2str(src, model.srcvocab))\n",
    "println(\"REF: \", int2str(tgt, model.tgtvocab))\n",
    "println(\"OUT: \", int2str(out, model.tgtvocab))\n",
    "## Here is a sample output:\n",
    "## SRC: çin'e 15 şubat 2006'da ulaştım .\n",
    "## REF: i made it to china on february 15 , 2006 .\n",
    "## OUT: i got to china , china , at the last 15 years .\n",
    "\n",
    "# ### Calculating BLEU\n",
    "#\n",
    "# BLEU is the most commonly used metric to measure translation quality. The following should\n",
    "# take a model and some data, generate translations and calculate BLEU.\n",
    "\n",
    "function bleu(s2s, d::MTData)\n",
    "    d = MTData(d.src, d.tgt, batchsize = 1)\n",
    "    reffile = d.tgt.file\n",
    "    hypfile, hyp = mktemp()\n",
    "    for (x, y) in progress(collect(d))\n",
    "        g = s2s(x)\n",
    "        for i = 1:size(y, 1)\n",
    "            println(hyp, int2str(g[i, :], d.tgt.vocab))\n",
    "        end\n",
    "    end\n",
    "    close(hyp)\n",
    "    isfile(\"multi-bleu.perl\") || download(\n",
    "        \"https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\",\n",
    "        \"multi-bleu.perl\",\n",
    "    )\n",
    "    run(pipeline(`cat $hypfile`, `perl multi-bleu.perl $reffile`))\n",
    "    return hypfile\n",
    "end\n",
    "\n",
    "# Calculating dev BLEU takes about 45 secs on a V100. We get about 8.0 BLEU which is pretty\n",
    "# low. As can be seen from the sample translations a loss of ~3+ (perplexity ~20+) or a BLEU\n",
    "# of ~8 is not sufficient to generate meaningful translations.\n",
    "\n",
    "@info \"Calculating BLEU\"\n",
    "bleu(model, ddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
